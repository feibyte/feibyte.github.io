{"posts":[{"title":"AWS client getaddrinfo EMFILE issue","text":"最近，在我们系统中引入了 AWS Cloud Map 作为我们的服务发现系统。部署几周后没有问题，今天突然抛出错误，日志显示错误 getaddrinfo EMFILE events.ap-southeast-2.amazonaws.com。当然，并非所有请求都触发了此错误，只是在高流量时段才出现了这个错误。 不难排查这是一个 Socket 泄露问题，在我们的系统中算是一个已知问题。解决方法很简单：复用实例 keepAliveAgent。 1const keepAliveAgent = new HttpsAgent({ keepAlive: true, maxSockets: 500 }); 问题根源在我们的代码库中，每个 Lambda 都有一个对应的 createService 负责创出服务实例。问题是：我们是为每次请求生成一个新的实例。之前尝试过切换到单例时，因为一些依赖必须动态创建，最后放弃了。 问题原因很明显：每次运行创建一个新的 HttpsAgent 实例， 而旧的 HttpsAgent 持有的 sockets 并没有被及时释放，很快就导致连接用尽的问题。 在开发测试阶段，请求不多这个没有暴露，因为不会超过 Lambda 限制的连接数 1024 ，问题是在部署几周之后才突然出现的。 当然这个问题应该不仅限于 AWS 客户端，检查了 OpenAPI 生成的客户端服用类似单例： 1const keepAliveAgent = new HttpsAgent({ keepAlive: true, timeout: 15000 }); 这个算是我们整个系统的一个设计问题，短期内不大可能会改动，只能是尽量避免这种错误发现。 教训回过头看下这个问题，可以说是很基本的错误，即便是新手也很容易觉察出问题。这些基本的错误在系统变复杂的时候，也会变得难以发现和不可避免。很多时候一些知识点看起来很简单，简单到没被注意到，往往在触发了一个线上故障的之后，才重视起来。","link":"/2024/05/16/AWS-client-getaddrinfo-EMFILE-issue/"},{"title":"AWS Connect 转接最近通话的客服","text":"需求最近接到一个需求，需要将客户来电转接到最近与客户通话的客服。这个需求很容易理解，客户可能因为各种各样的原因中断通话，再次来电很可能是因为同一个诉求，比如保险索赔，可能需要多次来回沟通。将通话转给同一个客服，客服可以接着继续处理而不用熟悉客户场景，这样做能够提高处理效率。尽管这个需求看起来很基础，但是并没有一个开箱可用的方案。我们的呼叫中心是 Amazon Connect，不过并没有启用 Profile，一些方案也不能采用。 方案解决思路很简单，我们可以添加一个 Lambda 函数来查询最近的通话记录，获取上次通话的队列（queue）和客服（agent）信息，如果查到的客服在线，并且客户想继续上次通话，则将通话转接到上次通话的客服。如果在这个过程中出现任何错误（比如客服不在线或者客户不想继续上次通话），通话将按照正常流程进行。我们需要做的是： Lambda 函数将检索有关客户上次通话的详细信息，包括队列和客服。 如果上次客服在线并且客户想继续通话，新通话将转接到该客服。 如果在此过程中出现任何问题（例如客服不可用或客户偏好），通话将按照正常流程进行。 实现更新 IVR首先，更新 IVR：调用 Lambda 函数检索上次客服和队列，然后将客服设置为工作队列，之后检查客服状态，询问客户是否继续上次通话。IVR 流程如下： Lambda 获取上次通话现在，剩下的工作就是获取上次通话客服。有个问题是 aws client 的 SearchContactsCommand 不支持通过客户电话号码搜索通话，可能是出于隐私考虑吧。一个变通的解决方法是：设置一个自定义的联系人属性（例如 CustomerPhoneNumber）并将其标记为可搜索。然后，使用 SearchContactsCommand 搜索属性搜索联系人。当然在 IVR 设置中设置 CustomerPhoneNumber 为通话电话号码。需要注意的是，添加可搜索键之前创建的联系人仍然不可搜索。此外，我们只需搜索过去 7 天内的通话，太久的通话很可能不太相关。代码片段如下： 1234567891011121314151617181920212223242526272829303132const input: SearchContactsRequest = { InstanceId, TimeRange: { Type: &quot;INITIATION_TIMESTAMP&quot;, StartTime: new Date(Date.now() - 7 * 24 * 3600 * 1000), // Since 7 days ago EndTime: new Date(), }, SearchCriteria: { Channels: [ &quot;VOICE&quot;, ], SearchableContactAttributes: { Criteria: [ { Key: &quot;CustomerPhoneNumber&quot;, Values: [ phoneNumber, ], }, ], MatchType: &quot;MATCH_ALL&quot;, }, }, MaxResults: 10, Sort: { FieldName: &quot;INITIATION_TIMESTAMP&quot;, Order: &quot;DESCENDING&quot;, },};const command = new SearchContactsCommand(input);const response = await client.send(command);const lastContact = (response.Contacts ?? []).filter((contact) =&gt; contact.AgentInfo?.Id)[0] 后续实现和测试这个功能并不复杂，但是我们没有上线这个功能。问题在需求上：一是客服刚好在线的概率不高，二是客服在线的话，直接转接就像插队，需求需要继续讨论。 总结去年参加 AWS Conference 时，一场演讲就是介绍 Amazon Connect 与 AI 的集成，展示了它主动解决客户问题的潜力。比如客户电话刚接通，AI 就开始询问是否是因为某些事情来电等等，看起来很有炫酷。但是，最近要实现的这个本来以为很普通的功能却都找不到一个开箱可用的方案，着实让人有些意外。当然我们也在考虑整合它们的 AI，不过还没那么快。 参考实现 Last Agent and Last Queue Routing on Amazon Connect for Returning Callers可以优化的地方：根据用户的上次通话的满意度，决定是否转接到上次通话的客服。","link":"/2024/05/21/AWS-connect-last-agent-call-routing/"},{"title":"《Designing Data Intensive Applications》读书笔记 - 数据库复制","text":"这一章讲数据库复制（Replication），目标很简单就是保存数据副本在多个机器上，但是实现却没那么容易。首先需要数据复制的几个原因： 数据中心地理上更靠近用户 增强可用性，即便部分服务器节点失败，整个系统依然可用 提高读取的吞吐量 本章讨论三种算法：单个主库（single leader），多个主库（multiple leader），无主库（leaderless） 主从 (Leaders and Followers)存储数据库拷贝的节点称为副本 （replica），当有多个副本时，问题来了：如何确保所有的数据最终在所有的副本上？最常见的方案是主从复制： 一个副本被设计为主库（leader），客户端写入时发送请求到主库或主节点，主节点先将数据存储到本地 在读库写入数据之后，同时发送数据变更到所有的从库，从库相应的更新本地拷贝 客户端读取数据时，可从主库或任意的从库查询 异步复制还是同步复制同步复制可以保证和主库的一致，缺点是如果从库没有响应，主库必须阻塞所有的写入直到同步复制可用。因为这个原因，将所有的从库都设置为同步不太实际，任何一个节点故障都能造成整个系统停止（halt）。实际应用中，如果你启用同步复制的功能，只是意味着只有一个从库是同步的，其他都是异步的。这保证至少有两个节点有最新数据，有时这个配置又称为半同步。更常见的是设置为完全异步这种情况下，如果主库挂掉而且无法恢复，未完成同步的写入数据就会丢失，即便客户端已经确认写入，也不能保证（确认的写入）的持久性。但是完全异步的好处是主库可以继续处理写请求，即便所有的从库更新数据都落后了。 弱的可持久性听起来是一个不好的取舍，尽管如此异步仍被广泛应用，尤其是从库数量很多或者是地理上分布很广的情况。 设置新从库 主库创建快照 复制到从库 从库连接到主库，请求之后的数据变更 一旦完成 caught up 可以继续处理主库发送的数据变更 处理节点故障从节点失败从节点崩溃重启，可以连接主节点，请求连接断开期间所有的数据变更 主节点失败主节点失败稍微复杂，其中一个从节点需要提升为主节点，客户端需要重新配置发送请求到新的主节点，其他从节点需要开始消费来自新主节点的数据变更。自动的 failover 步骤: 确定主节点失败 选举新的主节点 重新配置系统使用新的主节点 可能出错的情况： 如果使用异步复制，新的主节点可能没有收到来旧主节点的所有数据 在与外部存储系统协作时，丢弃写尤其危险。Github 故障，自增 ID 在新主库上导致使用已分配的 ID，但是 Redis 有之前的缓存，导致用户数据泄露。 在一些情况下可能两个节点都认为自己上主库 （split brain） 主库死亡的 timeout 时长；如果太长恢复时间就会比较长，如果太短可能引起不必要的 failover 这些问题：节点失败，网络不可靠，副本一致性取舍，可用性和延迟都是分布式系统中的基本问题。 实现复制日志基于语句的复制在最简单的例子中，主节点将执行的请求语句直接发送到从节点。这种方法存在一些问题： 任何生命语句里面包含有非决定性的函数，比如当前时间，随机数，都可能产生不同的复制数据 如果语句使用自增列，它可能依赖于既有数据，需要保证他们的顺序 如果语句有副作用，可能会引起不一样的副作用 WAL shipping存储引擎已经有的 WAL 日志，将它发送给其他节点比较容易实现。问题是这个日志和存储耦合 逻辑日志复制这种更像是中间路线，与存储引擎引擎解耦 复制延迟问题读自己写（Reading Your own Writes）实现读后写一致性的方案： 对用户可能修改过的数据，从主节点读取 最后更新在1分钟之内的数据，从主节点读取，否则同从节点，同时监控从节点不会落后主节点 1 分钟以上 客户端知道最近写的时间戳，根据这个时间从节点可以知道落后的进度，可以等待从节点 caught up 或交给其它从节点处理 Monotonic read让用户觉得时光倒流的情景，实现单调读的一种方案是确保每次都读从相同的从库读 多个主库复制 （Multi-Leader Replication）多数据中心地理上分布在不同位置时，相比较单leader 性能上更好 能够容忍数据中心出错 可以容忍网络问题 但是需要处理冲突问题 处理写冲突 异步或者同步冲突检测 理论上可以同步，那样就没必要使用多个主库 冲突避免 最简的办法就是避免冲突，确保来自给定用户的请求，由相同的数据中心相同主库处理 一致状态收敛 当数据中心处理写冲突时，写入的顺序在不同节点可能不一致，如果按照时间处理，可能最后导致不一致，所以解决冲突需要一种收敛的方式。比如对于每个写都有一个唯一的 ID，冲突时最高 ID 获胜。这种方法很常见但是很可能会导致数据丢失。 冲突解决逻辑 写入时解决 读取时解决 多主库拓扑结构最基本的就是 all to all，每一个主节点都发送写入到其他的主节点。更严格的拓扑是环形拓扑（MySQL 默认只支持环形） 无主库复制 （leaderless replication）客户端直接发送写请求到多个副本 一个节点故障时写入数据库客户端并行发送请求到所有的服务节点 读修复和反熵 读修复 客户端的读取的时候，从多个节点并行读取时数据时可以检测到旧的响应数据。客服端发现后，将新值写入该副本。适用于读频繁的值 反墒 后台程序去持续的检测副本之间的不同，并且复制丢失数据到副本 读和写的法定人数 （Quorums of reading and writing）一般来说，如果有 n 个节点，每个写必须被 w 个节点确认，读取从 r 个节点查询。只要 w + r &gt; n 我们就能读取到最新的数据。 即便满足上面的条件，仍然有些局限： sloppy quorum 可能存在写的节点和读的节点没有相同节点 如果两个并发写发生，并不清楚哪一个先后 如果发送并发读和写，写可能只反映在某些副本上，并不确定读返回新的还是旧的数据？ 如果写部分成功部分失败，但是不满足上述条件，在成功节点上没有回滚。这意味着即便写入失败，后续的读依然可能会返回之前写入的数据 如果一个有新数据节点挂掉，然后又从一个旧数据节点恢复，可能就会违背了之前的条件 即便所有的都运行正常，也可能会有时序的问题 Sloppy Quorums and Hinted Handoff如果出现网络故障，大部分节点可能都会无法连接客户端，这可能导致不能满足法定人数条件，但是只有少部分节点可用，可用这个时候就需要做些取舍 直接返回错误，因为不满足这个法定人数条件 接受写，在网络恢复之后再将写入发送给这些节点。一个类比就是你把自己锁在房子外面了，然后你可以去邻居家沙发上睡一晚，一旦你找回钥匙，你的邻居要求你回去。 检测并发写之前说过，最新写胜出是以写丢失为代价来解决写冲突的。怎么才能决定两个操作是并发的呢？还是有先后顺序的呢？取决于他们是否知道另一个操作的存在检测先后关系的方案： 维护一个版本号和数据一起写入 读取时返回所有没有被覆盖的数据，客户端必须先读再写 客户端写入时，必须加上先前版本并合并先前版本的数据 服务端接收到写请求，可以覆盖低版本号的数据，但必须保留高版本的数据 最后这一章讲了数据库复制，主从，多主，无主库的实现，以及一些冲突解决的方案。这一章的内容比较多，但是都是比较基础的内容，对于分布式系统的理解有很大的帮助。有时间可以看下分布式的课程了。","link":"/2024/07/16/Designing-data-intensive-applications-note-replication/"},{"title":"Athena in IntelliJ IDE","text":"目前所在公司使用的是 Serviceless 架构，数据库使用 DynamoDB，每天定时任务会导入数据湖，所以平时会经常使用 Athena 查询来排查问题，尤其是最近在调查数据一致性的问题。Athena 本身可以满足日常需求，只是使用多的时候觉得不如 IDE 方便。 问题当前使用的是 IntelliJ IDEA （已经更新到最新版），直接选择 Database 添加 Data Source 选择 AWS Athena 看到的是下面这个界面，授权方式有三种： User &amp; Password，AWS Profile， No auth。 我们使用的是 SSO 应该选择 No auth 之后切换到 Advanced 参数就不知道怎么设置了 谷歌上找到两遍文章 Using AWS Athena from IntelliJ-based 太旧，这个 Configure JetBrains IntelliJ AWS Athena data source using JDBC driver 虽说不太一样倒是给了思路。查看了一下 IDE 自带的驱动版本是 2.x，查看文档参数时发现 Connecting to Amazon Athena with JDBC 版本 3.x 的文档很简单，不如直接使用 3.x。 步骤 下载和添加驱动 Connecting to Amazon Athena with JDBC 创建数据源，配置参数，参见文档 AWS configuration profile credentials 非常简单 测试链接 最后效果 还可以切换到可视化","link":"/2024/04/28/Athena-in-IntelliJ-IDE/"},{"title":"《Designing Data Intensive Applications》读书笔记 - 事务","text":"最近读到的最好的一本技术书，评价非常高，堪称经典。书还未读完，收获很大，值得记录的很多。这一篇博客先讲事务，真心觉得这本书解释的非常清楚，胜过很多的博客。 事务不是所有的的应用都支持事务，有些应用为了高性能或者高可用性就完全舍弃了事务。是否需要事务，首先需要理解事务能够解决的问题以及需要付出的成本。 ACID事务的四个特性：原子性、一致性、隔离性、持久性。 原子性 原子性通常来说事物不可分割，但是无论如何，事务都无法做到像原子一样不可拆分，所以作者认为更合理的名字是 abortability，就是事务在出错时可中止。 一致性 关于数据的某些陈述一直为真，比如转账事务，转账前后的总金额应该是相同的。作者认为一致性的定义取决于应用对不可变性的定义，应该由应用来定义事务的正确性，而不是由数据库来定义。 隔离性 并发执行的事务互相隔离，或者说可顺序执行。并发事务的结果应该与顺序执行的结果一致。但是实际上，顺序级的隔离很少见，因为会导致性能问题。 持久性 事务提交后，数据应该持久化，即使系统崩溃也不会丢失。对于单节点数据库，意味着数据写入磁盘，对于分布式数据库，意味着数据写入多个节点。这儿要强调的是完全的持久性是不存在的。 单个对象写也涉及到原子性和隔离性，但是事务一般理解为对多个对象的一组操作。 事务的关键特性是出错时可以中止并且可以进行重试。但是一些常见的 ORM 框架并不会进行重试，而是直接抛出异常。 This is a shame, because the whole point of aborts is to enable safe retries.重试看起来很容易，但是有些问题： 如果是网络问题，重试可能会导致重复操作。 如果是过载问题，重试只会加重负载。 只有暂时性的问题才能重试，如果是永久性的问题，重试是没有意义的。 如果事务中有副作用，比如事务失败发邮件，重试可能会导致重复发送邮件。 If the client process fails while retrying, any data it was trying to write to the database is lost 没 Get 到这句话的意思。 弱隔离级别序列化隔离级别是最高的，但是实际上很少使用，因为性能问题。大多数数据库使用的是弱隔离级别。 读提交（Read Commited）最基本的事务隔离级别，确保： 没有脏读 没有脏写，注意并不包括两个 Counter 自增的问题 防止脏写最常见的方法是使用行级锁，事务修改一行时必须先获取到锁，其他事务修改同一行时会被阻塞。防止脏读使用锁就不现实了，因为一个耗时的写事务会导致读事务长时间等待。 Snapshot Isolation or Repeatable Read (可重复读)读提交包括了所有事务要求的特性，可以中止，阻止读取未提交数据，脏写。但是仍有其它并发问题，比如不可重复读，事务前后读取结果不一样，虽然最终数据一致，但是用户可能困惑，在备份数据时可能出现不一致问题。书中并没有说明前后两次读是在一个事务中，所举的场景比如：备份，分析查询和数据完整性检查，都是现实中的普通场景。 可重复读的实现一般是使用多版本并发控制（MVCC），简单来说，对于每个事务都对应着一个唯一的增长的事务 ID，每条被写入的数据都有一个事务 ID。书中使用两个字段：created_by 和 delete_by，更新对应的是删除和添加。对应的可见性规则： 每个事务开始时，数据库列出所有未提交的事务以及已经中止的事务，这些事务的写入都被忽略，即便后续提交。 任何中止事务的写入被忽略 任何 transaction ID 大于当前事务 ID 的写入被忽略 所有其它写入都是可见的 MVCC 的索引 一种方法是过滤掉不可见数据，另一种方法是就像 immutable 树，索引树对每个事务都有不同根节点。 Snapshot Isolation 有不同的名字，Oracle 叫做 serializable，PostgreSQL 和 MySQL 是 repeatable read， Lost Updates (更新丢失)一些场景： 自增或者根据当前值更新 集合添加，读取集合之后添加 两个用户同时编辑 wiki 页面，每个用户保存整个页面内容，覆盖当前数据库内容 解决方案： 原子写 UPDATE counters SET value = value + 1 WHERE ... 显式锁 SELECT ... FOR UPDATE 就是很容易遗漏 自动检测 数据库可以执行非常高效的检测（PostgreSQL, Oracle, SQL Server）， MySQL 没有 Write Skew and Phantoms书中例子，两个 on-call 医生同时请假，系统约束必须有一个医生在岗。在事务中，先查询在岗人数，如果人数大于 2，取消 one on-call。 如果发生竞争条件，有可能两个人都请假，导致系统没有医生在岗。这个可以说是 更新丢失的一个泛化，先查询再更新，更新之后不再满足约束。还有一个订会议室的例子，先查询给定会议室在时间区间内是否有预定，如果没有，预定会议室。 这儿的主要问题是没有一个合适的对象可以加锁， Materializing conflicts 方案，维护会议室和 time slots (时间段) 的表，这样就可以给查询到的行加锁。这种方案容易出错，通常又不够优雅，不如直接考虑序列化。 序列化真正的序列化真正的序列化变得可行因为 RAM 够大， OLTP 很少部分数据进行读写而且读写一般很快。 两段锁 (Two-Phase Locking)在事务开始时，获取所有需要的锁，直到事务结束才释放锁。死锁更容易发生，毕竟事务开始时就获取所有锁，一直持有到事务结束。 Serializable Snapshot Isolation (SSI)SSI 是基于 Snapshot Isolation 的，增加算法检测冲突。检测 Query 结果变化 检测对过时 MVCC 对象版本的数据读 检测读后写 工程细节影响算法实际表现。事务读写数据的粒度，如果跟踪很细，识别需要中止的事务就比较精准，反之就比较粗糙，中止事务的可能性就比较大。对读比较多的应用而言，SSI 非常有吸引力 总结读完这部分再看一些关于事务的博客总觉得他们写的不够精准。这本书本身讲的也很清楚，很容易理解，非常值得一读。","link":"/2024/06/10/Designing-data-intensive-applications-note-transaction/"},{"title":"《Designing Data Intensive Applications》读书笔记 - 存储和索引","text":"首先讨论传统关系性数据库使用的存储引擎，分为两类：日志结构存储引擎和页面结构存储引擎。 数据结构Hash 索引首先从最简单的 key-value 数据库开始，只有两个函数的脚本: 12345678#!/bin/bashdb_set () { echo &quot;$1,$2&quot; &gt;&gt; database}db_get () { grep &quot;^$1,&quot; database | sed -e &quot;s/^$1,//&quot; | tail -n 1} 数据写入以追加的方式添加到文件结尾，数据读取从文件结尾开始匹配目标 key，修改数据也是以追加一条新记录的形式，删除数据需要一个特殊的 tombstone 标志。数据的检索存在问题，每次都需要扫描所有数据，时间复杂度为 O(n)。 很容易想到，在内存中保存一份索引，记录 key 对应的文件记录偏移。 现在的一个问题是如何避免磁盘写满，一个方案是将记录文件进行分片，当文件达到一定大小时就开始写入新文件。在写入新的分片文件时，旧的分片文件可以进行压缩，合并。查找过程，需要先查找新分片的索引，再查找次新分片的索引。 这个实现看上去太过简单，实际上非常可行。数据的写入非常高效，因为磁盘顺序写没有寻道时间, Bitcask 就是这种实现方式。适用于每个值频繁更新的情况，比如视频播放计数。 一些实现细节 文件格式 CSV 并不是最合适的格式。更快更简单的二进制格式：首先编码字符串长度，然后是字符串本身。 删除记录 追加一个特殊的删除记录，标记为删除（tombstone） 合并分片的时候，丢弃给定 key tombstone 之前的记录。 故障恢复 数据库重启时，内存中的哈希表会丢失，通过文件重新建立索引太过耗时，Bitcask 存储哈希表的快照，直接从快照中恢复。 部分写记录 写入过程中崩溃，校验和 并发控制 一般实现是只有一个写线程，读线程可以并发。文件分片不会改变，所以支持多线程并发读 优点只追加的日志看起来浪费，好处是： 追加和分片合并是顺序写，磁盘顺序写比随机写快很多。 并发和故障恢复更简单，不需要考虑并发写入的问题。 合并旧分片可以避免文件碎片化，提高读取性能。 局限 索引需要能够整个存入内存，理论上可以放在上磁盘上，操作系统可以交换磁盘，实际上效率也非常低（随机访问时交换频率很高） 范围查找效率依然很低。 SSTable 和 LSM-Tree接下来做些小的改变，保持键值对有序，按 key 排序，我们将这个格式称为 SSTable（Sorted String Table）。咋看上去这可能会破坏我们的顺序写，不过可以带来几个好处： 文件进行分片，合并压缩过程非常高效，归并排序的合并过程，时间复杂度是 O(n) 哈希文件不需要保持所有的 key，现在可以维护一个稀疏索引。 范围查询时，整块读取，可以减少磁盘带宽的使用 初始有序文件的创建在写入文件之前，可以将记录保存在内存中，使用红黑树维护顺序，超过一定大小之后写入文件。为了应对故障，同时需要维护一个 WAL 日志文件 未命中查询LSM-tree 在查找不存在的 key 时性能比较慢，一个办法是 Bloom Filter，这样可以快速判断 key 是否存在。 B-Trees B-Tree 可以说是广泛使用的一种索引结构，和 LSM-Tree 唯一的相同之处就是保持记录在文件中有序。最突出的不同是，每个 key 在文件中只有一个记录，修改是直接操作磁盘。管理文件是以页或块（对应操作系统中的页）为单元，将索引结构以树的形式组织。 优化 不去覆写页和维护 WAL，而是写入一个新的页，然后更新父节点的引用 不存储整个 key, 只保存缩写，只需要用来确定 key 的范围即可，这样子一个页内可以存放更多的key, 整个树的高度更低 一般来说，页可以在磁盘的任意位置，很多 B 树的实现让叶子节点在磁盘上连续。但是，随着树的增长，这个顺序很难维持 额外的指针，叶子节点之间的指针，可以加速范围查询 B 树变种 fractal trees 借鉴了 LSM-tree 的思想减少磁盘寻道 比较根据经验，LSM-Tree 写入速度更快，B-Tree 读取速度更快 LSM-Tree 优点： 顺序写磁盘，磁盘吞吐量高 更好的压缩，避免磁盘碎片化 LSM-Tree 缺点： 压缩操作可能会影响正在进行的读和写，磁盘带宽必须和压缩写共享 不合理的配置，如果压缩合并跟不上写入的速度，可能导致磁盘耗尽 B-Tree 优点： 一个 key 只在一个地方，容易实现并发控制 B-Tree 缺点： 块大小固定，磁盘碎片化 至少写入两次磁盘，写放大比 LSM-Tree 显著 其它索引结构 聚簇索引 存储值和索引在一起，不需要再次查找 MySQL 主键索引就是聚簇索引，二级索引引用主键 多列索引，比如按经纬度查询 全文搜索，模糊索引 所有内容在内存 事务处理或分析数据库除了用于 OLTP 也越来越多的用于 OLAP 一般来说，数据分析需要扫描很多记录，同时只关心少数几列，往往会进行聚合计算。 数据仓库单独的数据库运行分析查询，不会影响 OLTP 操作。数据是从 OLTP 数据提取，转化为分析友好的模式，清洗加载到数据仓库（ETL）。使用单独的数据库最大的好处是可以为分析访问模式优化数据仓库。 区别数据仓库的数据模型通常是关系性，因为 SQL 非常适于分析查询。表面上看数据仓库和 关系性 OLTP 数据库比较相似，但是系统内部非常不同，因为各自为不同的查询模式优化。 星形模式不像事务处理会有多样的数据模型，分析处理模型较为一致，star schema.模式中心称为 fact-table 每行通常是一个单独的事件，比如网页浏览或者点击，包含其它纬度的数据说明何人何事何时何地以及为何。星形模式是因为 fact-table 在中心和其它纬度的表相关联，形状像星形。 列式存储通常 fact-table 非常大量的数据，经常超过 100 多列，查询时一般只会涉及到 4-5 列。为了支持高效查询，通常选择列式存储。 压缩列每列的数据经常重复，可以非常高效的进行压缩，基于列中数据，可以选择不同的压缩方式。一种相当有效方式是：位图编码例如，销售记录可能有上亿行，而商品只有 n 种，我们就可以选择将其转换为 n 万个不同的位图。如果 n 很小 (200) , 每行可以用 1 个位表示，如果 n 很大（比较稀疏），可以使用游程编码方法（run-length encoded）位图索引非常适合常见的查询，比如下面的查询只需要进行对应的位操作即可。WHERE product_sk = 31 AND store_sk = 3:WHERE product_sk IN (30, 68, 69): 列式存储可以更高效的利用 CPU 周期，列压缩允许更多的行放入 CPU 一级缓存。 排序不同的查询受益于不同的排序，存储冗余数据以不同的排序方式 写入就地更新的方式对于压缩的列不大可能，不过可以借鉴 LSM-Tree 的思路，所有的写先存入内存，等到足够多时再写入磁盘。查询时合并两个结果。 物化视图（materialized view ）多个查询使用相同的聚合，直接缓存起来更为高效。数据变更时，视图需要重新更新 （OLAP 很少更新）data cubes 比如，产生销售数据，聚合数据按时间和产品，之后方便查询。 总结数据存储和索引是数据库的核心，不同的查询模式需要不同的存储和索引方式。OLTP 和 OLAP 有不同的查询模式，需要不同的存储方式。","link":"/2024/07/07/Designing-data-intensive-applications-note-storage/"},{"title":"《Designing Data Intensive Applications》读书笔记 - 分区","text":"上一篇讲数据库复制，对非常大的数据库，非常高的数据吞吐量，往往需要同时采用分区。每条数据记录只属于一个分区，但是它可能存储在多个不同的副本节点上。 一个节点可以存储多个分区，对于主从复制来说，同时使用分区看起来像下图，每个分区的主库分配给一个节点，其它从库分配给其它节点。 Key-Value 数据分区分区的目标是尽量将数据分散存储在各个节点上，查询负载均匀分布在各个节点上。如果分区不均匀，一些分区有更多数据承载更多的查询，我们称之为倾斜。负载比例比较高的分区称为热点。最简单的避免热点方法是使用随机分区，缺点是读取时不知道数据在哪个节点，需要并行查询所有节点。 Key Range 分区这种方式是分配一个连续范围内的键给一个分区。就像百科全书，字母范围 A-B 在第一卷，当然键的范围不必平均分配，因为数据本身也不是均匀分配的，某些键包含的数据可能比其它键多很多。分区的界限可以由管理员手动设置，也可以由系统自动选择。这种分区的好处是，范围扫描非常高效，缺点是某些访问模式可能导致热点。比如选择日期作为 key 的话，最近日期的数据分区很可能成为热点。所以，在选择 Key 时需要谨慎。 Key 的哈希分区因为倾斜和热点的问题，许多分布式存储使用哈希函数决定分区。不幸的是，这种方式不支持高效的范围查询。Cassandra 折中了两种策略，组合键，先按范围分区，再按哈希分区。这种组合索引方式为一对多数据关系提供一种比较优雅的数据模型。 倾斜负载和热点消除哈希函数可以减少热点，但是并不能完全消除。在极端情况下，比如社交媒体上的网红可能有大量的粉丝，这种情况下大量数据的写入可能都集中在一个分区。现在，很多数据系统还不能自动补偿这种高度倾斜的负载，需要应用层来减少这种倾斜。比如，对于热点的键，可以将其分为多个键，增加两位数字的随机数可以分为 100 个键，这样可以将数据分散到多个分区。当然，读取数据的时候需要一些额外的工作，需要读取所有的分区然后合并结果。 另外，需要维护和追踪哪些键被分割，这样可以在查询时合并结果。 分区和二级索引本地二级索引这种索引就是在同一个分区内，维护一个二级索引，也就是说每个分区都有自己的二级索引。但是，读取本地二级索引的时候，需要在所有的分区上进行查询，有时称为 scatter/gather，即便查询分区是并发的，查询时间也受累于最慢的分区。 全局二级索引与二本地索引对应，全局二级索引是在所有的分区上维护一个单独索引，当然不止存储在一个节点上，同时也有会自己的分区。全局索引的好处是读取更快，不需要在所有的分区上进行查询，但是写入时会更慢。实际中，更新全局索引通常是异步的。DynamoDB 的全局索引只支持最终一致性，而本地二级索引可以支持强一致性读。 分区再平衡不管怎样，随着数据库的增长和机器变化，将数据和请求转移到的不同节点是必要的，这个过程称为分区再平衡。基本需求： 再平衡之后，负载应该相对均匀 再平衡过程中，应该保持服务可用 不移动不必要的数据 再平衡的策略为什么不能再哈希？因为再哈希会导致大量的数据移动，而且会导致热点。 固定分区数目相对简单的方案：创建比较多的分区，分配多个分区给一个节点。比如一个节点开始有 10 个分区，当一个新节点加入时，可以从其它节点偷走一个分区。移动数据时，将整个分区的数据进行移动。原则上，分区可以分割合并，但是固定分区运维简单，大多数固定分区数据库不提供分区分割合并的功能。通常分区数目在数据库创建时就固定了，分区的数目要足够大，相对于将来增长的节点数量。当然分区数据过多会增加固定开销。所以选择一个合适的分区数目是很重要的。 动态分区数目固定数目分区需要一个合适的固定边界，如果边界配置不合适，可能会导致数据很不均匀。动态分区在分区超过一个阈值时，自动分裂，当分区数据过少时，自动合并。一个优点是，分区数目适配总的数据量。数据量小的时候，分区比较少，固定开销就小。 节点比例分区第三种方案让分区数据量和节点数目成比例，当节点数目增加时，分区数目也增加。 自动还是手动全自再平衡很方便，但是有些不可预测。再平衡是一个很昂贵的操作，需要重新路由请求，移动大量数据。如果操作不同，这个过程可能导致网络过载，节点负载过高。 请求路由分区完成之后，留下一个问题，客户端请求时如何知道数据在哪个分区。这个普遍的问题又叫服务发现。有几种方法： 允许客户端连接任意节点 如果那个节点碰巧有数据，就直接返回，如果没有，就转发请求到正确的节点，收到回复时转发回客户端 代理节点，客户端连接到代理节点，代理节点知道数据在哪个节点，然后转发请求到正确的节点 客户端知道分区和分区节点的映射 不管哪种方法，都需要给定组件知道节点上的分区分配。很多分布式系统依赖独立的协调服务像 ZooKeeper.Cassandra 采用不同的方法，采用 gossip 协议，节点之间相互通信，知道数据在哪个节点。请求可以发送到任意节点，节点会转发请求到正确的节点。 总结这一章内容比较简单，有意思的是分区再平衡的问题，还有最后关于请求路由的问题。","link":"/2024/07/19/Designing-data-intensive-applications-note-partition/"},{"title":"《The DynamoDB Book》读书笔记","text":"同事推荐的一本书，只有英文电子版。作者是Alex DeBrie，之前介绍过，是单表设计的推崇者。 这本书前面部分几个章节介绍 DynamoDB 的基本概念，后面部分是一些实际的设计案例。 1. 什么是 DynamoDBDynamoDB 的几个特性使用过的基本都知道：键值或宽列数据模型、无限扩展、HTTP 连接、IAM 鉴权、弹性定价、DynamoDB Streams、无需管理。什么时候使用 DynamoDB: 超大规模应用、Serverless 应用、(大多数 OLTP 应用，缓存，简单数据模型)。我们现在的应用是无服务应用，毫无疑问选择 DynamoDB 。另外我想强调的一点是 DynamoDB 能够满足绝大多数 OLTP 应用需求，当然现实中开发效率上确实不如成熟的关系性数据库。 2. DynamoDB 的概念Table, Item 和 Attribute, Primary Key, Secondary Index 和 item collection（相同 partition key 的item）。Primary Key 有两种类型：Simple Primary Key 和 Composite Primary Key。Simple Primary Key 只有一个属性，Composite Primary Key 有两个属性，分别是 Partition Key 和 Sort Key。很多时候使用 Composite Primary Key 是最好的选择，因为可以支持更多的查询需求。Secondary Index 有两种类型：Local Secondary Index 和 Global Secondary Index。Local Secondary Index 只能使用相同的 Partition Key，Global Secondary Index 可以使用不同的 Partition Key。Global Secondary Index 更常用，因为更灵活。 另外介绍了一些高级概念：DynamoDB Streams, TTL, Partitions, Consistency 还有 DynamoDB Limits （Item size 400K, result 1MB, Single Partition 3000 RCU or 1000 WCU），实际中很少会触及这个限制，遇到时都是非常极端的场景，比如 Item size 400K，我们遇到过一次，原因是将 DeviceId 存在 User item 中，每次 E2E 测试都会新增一个 DeviceId，突然一天测试开始失败因为 User item 超过 400K。 3. DynamoDB API作者将 DynamoDB 的 API 分为三类：基于 Item 的操作、Query 和 Scan。基于 Item 的操作有 GetItem, PutItem, UpdateItem, DeleteItem，对指定的 Item 进行操作增删改查，必须指定 Primary Key，修改数据只能修改一个 Item。即便能够使用 PartiQL 语法像 SQL 一样查询或更改，也不能批量修改数据。Query 是根据 Partition Key 获取多个 Items 甚至不同实体类型，一再强调的是同一个 Partition Key 下的 Sort Key 是有序的，可以使用 Sort Key 进行范围查询，性能接近 O(lgn)。Scan 是全表扫描，更新索引的时候会用到。 下面是一个 Query API 请求的例子： 12345678910111213items = client.query( TableName = 'MoviesAndActors', KeyConditionExpression = '#actor = :actor AND #movie BETWEEN :a AND :m', ExpressionAttributeNames = { '#actor': 'Actor', '#movie': 'Movie' }, ExpressionAttributeValues = { ':actor': { 'S': 'Tom Hanks' }, ':a': { 'S': 'A' }, ':m': { 'S': 'M' }, }) 刚接触的时候，DynamoDB 的 API 确实有点难用，就不能设计简单点吗？书中解释说，之所以这么啰嗦是因为这样设计基本不需要花时间解析请求，也就是说一切为了性能。另外，书中不建议使用 ODM 主要考虑是后面会提到的单表设计。 DynamoDB API 中的表达式 Key Condition Expression 每个 Query 请求都会用到的，用于指定 Partition Key 和 Sort Key 的条件。 Filter Expression 用于过滤结果，只返回符合条件的 Item。只是在返回结果之前过滤，不会减少读取的数据量（读数据请求依然受 1MB 限制）。 Projection Expression 用于减少网络传输，只返回指定的属性。 Condition Expression 用于 PutItem, UpdateItem, DeleteItem，用于指定操作的条件。 Update Expression 用于 UpdateItem，用于指定更新的属性，比如自增，从集合中删除 4. DynamoDB 数据建模本书首先解释了与关系数据库的区别：Joins 在超大规模数据上的限制， Normalization 带来的益处以及为什么 DynamoDB 不需要 Normalization。主要观点是存储成本很低以及数据完整性是应用层面的问题而非数据库层面的问题。建模的步骤： 理解业务需求 ERD 设计 列出所有的访问模式 选择 Primary Key 为其它访问模式添加 Secondary Index 作者比较推崇单表设计，之前的文章也讨论过单表设计的优势与考量，这里就不再赘述。 建模实现中的一些技巧： 将索引属性与应用属性分开 不同索引属性不复用属性名 添加 Type 属性 写脚本帮助调试 使用短的属性名 在我们的项目中数据存储大概是这样子的： 1234567891011{ &quot;pk&quot;: &quot;users#123&quot;, &quot;sk&quot;: &quot;users#123&quot;, &quot;gpk1&quot;: &quot;users#test@example.com&quot;, &quot;gsk1&quot;: &quot;users#2024-06-07T00:00:00Z&quot;, &quot;type&quot;: &quot;users&quot;, &quot;value&quot;: { &quot;name&quot;: &quot;Alice&quot;, &quot;_id&quot;: &quot;123&quot; }} 注意这儿用 pk sk gpk1 gsk1 而非更对应具体索引名是因为同一张表中有很多不同实体，比如同一张表中可能还有 user-audit，它们也在使用这些索引，像下面这样： 123456789{ &quot;pk&quot;: &quot;users#123&quot;, &quot;sk&quot;: &quot;user-audit#2024-06-07T00:00:00Z&quot;, &quot;type&quot;: &quot;user-audit&quot;, &quot;value&quot;: { &quot;name&quot;: &quot;Alice&quot;, &quot;_id&quot;: &quot;2024-06-07T00:00:00Z&quot; }} 建议使用一个简单的库来简化上面的设计，我们使用的是同事写的一个库 dynaglue，项目内也有一些复用代码。像上表结构，配置完成之后就可以开始实现和测试 UserRepoService 了。 5. 常用策略一些常见的模式吧，大多数比较容易，看到例子就能明白。 一对多关系 Denormalization 比如将用户地址直接存储在用户信息中 存储相关实体同一个 Partition Key 比如上面例子中将用户审计信息存储在同一张表中，同一个 Partition Key 下的 Item Collection，一条 Query 请求就可以获取到用户信息和用户审计信息。 使用 Sort Key 存储层级数据比如 #省#市#县 这样的层级数据。 多对多关系 浅复制 比如班级中存储所有学生列表 邻接列表 比如电影角色 规范化 需要多次请求 过滤 用 Partition key 过滤 使用 Sort key 过滤 查询一个范围内的数据其实效率非常高 组合 Sort key 比如 status#timestamp 稀疏索引 举例来说，工单状态，只有 open 的工单才有 open 的索引，但是不能将状态作为 Partition key，因为这样会导致数据分布不均匀，应新建单独的索引使用工单 ID 作为 Partition key。 使用 Filter Expression 这个上面提到过，只是在返回结果之前过滤，不会减少读取的数据量（读数据请求依然受 1MB 限制），可以减少网络传输数据量。 排序 基本排序 确保 Sort key 有序，比如全部大写，时间戳，Sortable ID 像 KSUID 等。上面用户审计的例子是用时间戳作为 Sort key，实际中 KSUID 更合适。 可变属性排序 比如更新时间 因为我们不能直接更新 Primary Key，所以只能将这个属性作为 Secondary Index 的 Sort key。 升序和降序排序 ScanIndexForward Zero Padding 数字排序 比如 001 002 003 这样的排序。 迁移 既有实体添加新属性 添加新实体 添加新实体到既有的 Item Collection 添加新实体到新的 Item Collection 既有实体添加新的访问模式 （新建二级索引） 使用并行扫描 Segment 和 TotalSegments 平时工作中都会涉及，但是除了新建索引，我们需要脚本更新索引，其它基本不需要关心，脚本更新索引的时候可以使用并行扫描。 其它策略 确保值唯一 conditionExpression attribute_not_exists 有序 ID 比如 ISSUE ID, 作为 Project 的一个属性，书中的例子是先自增这个 IssueCount ，然后用 IssueCount 创建 Issue。不完美，但是可以接受。 分页 不像关系数据库可以很容易获取到全部数据条数。 实例书中介绍了 4 个例子，包括一个电商应用，这儿只记录下 Github 的例子因为这个大家比较熟悉。使用上面提到的建模步骤： 理解业务需求 画出 ERD 列出所有的访问模式1234567891011121314151617181920212223242526272829303132333435363738394041**Repo**:- Get / Create Repo- Get / Create / List Issues for Repo- Get / Create / List Pull Requests for Repo- Fork Repo- Get Forks for Repo **Interactions**:- Add comment to Issue- Add comment to Pull Request- Add reaction to Issue / Pull Request / Comment- Star Repo- Get Stargazers for Repo **User management**:- Create User- Create Organization- Add User to Organization- Get Users in Organization- Get Organizations for User**Accounts &amp; Repos**:- Get Repos for User- Get Repos for Organization- Get / Create Repo- Get / Create / List Issues for Repo- Get / Create / List Pull Requests for Repo- Fork Repo- Get Forks for Repo **Interactions**:- Add comment to Issue- Add comment to Pull Request- Add reaction to Issue / Pull Request / Comment- Star Repo- Get Stargazers for Repo **User management**:- Create User- Create Organization- Add User to Organization- Get Users in Organization- Get Organizations for User**Accounts &amp; Repos**:- Get Repos for User- Get Repos for Organization 选择 Primary Key Repo PK: REPO#&lt;Owner&gt;#&lt;RepoName&gt; SK: REPO#&lt;Owner&gt;#&lt;RepoName&gt; 这儿考虑到到约束是给定用户下 Repo 唯一，如果使用随机ID，必须在新建 Repo 时检查 Repo 名是否唯一。 Issue PK: REPO#&lt;Owner&gt;#&lt;RepoName&gt; SK: ISSUE#&lt;ZeroPaddedIssueNumber&gt; Pull Request PK: PR#&lt;Owner&gt;#&lt;RepoName&gt;#&lt;ZeroPaddedPRNumber&gt; SK: PR#&lt;Owner&gt;#&lt;RepoName&gt;#&lt;ZeroPaddedPRNumber&gt; 这儿不理解为什么不能像 Issue 一样使用 REPO#&lt;Owner&gt;#&lt;RepoName&gt; 作为 PK。 IssueComment PK: ISSUECOMMENT#&lt;Owner&gt;#&lt;RepoName&gt;#&lt;IssueNumber&gt; SK: ISSUECOMMENT#&lt;CommentId&gt; PK 相当于 Issue 唯一标识，SK 相当于 Comment 唯一标识。 PRComment PK: PRCOMMENT#&lt;Owner&gt;#&lt;RepoName&gt;#&lt;PRNumber&gt; SK: PRCOMMENT#&lt;CommentId&gt; Reaction PK: &lt;TargetType&gt;REACTION#&lt;Owner&gt;#&lt;RepoName&gt;#&lt;TargetIdentifier&gt;#&lt;UserName&gt; SK: &lt;TargetType&gt;REACTION#&lt;Owner&gt;#&lt;RepoName&gt;#&lt;TargetIdentifier&gt;#&lt;UserName&gt; User PK: ACCOUNT# SK: ACCOUNT# Org PK: ACCOUNT# SK: ACCOUNT# MemberShip PK: ACCOUNT# SK: MEMBERSHIP# 为其它访问模式添加 Secondary IndexGS1REPO GS1PK: REPO#&lt;Owner&gt;#&lt;RepoName&gt; GS1SK: REPO#&lt;Owner&gt;#&lt;RepoNamePull Request GS1PK: PR#&lt;Owner&gt;#&lt;RepoName&gt; GS1SK: PR#&lt;ZeroPaddedPRNumber&gt;GS2REPO GS2PK: REPO#&lt;Owner&gt;#&lt;RepoName&gt; GS2SK: REPO#&lt;Owner&gt;#&lt;RepoName&gt;Fork GS2PK: REPO#&lt;OriginalOwner&gt;#&lt;RepoName&gt; GS2SK: FORK#&lt;Owner&gt;GS3Repo GS3PK: ACCOUNT#&lt;AccountName&gt; GS3SK: #&lt;UpdatedAt&gt;User GS3PK: ACCOUNT#&lt;UserName&gt; GS3SK: ACCOUNT#&lt;UserName&gt;Org GS3PK: ACCOUNT#&lt;OrgName&gt; GS3SK: ACCOUNT#&lt;OrgName&gt; 这个例子可能比较适合单表设计，因为所有实体都是围绕 Repo 展开的，数据实体之间关系更多的是一对多，而不是多对多。如果把上面的设计再简化一点的话，就会显得一目了然更容易理解： REPO PK: REPO#RepoId SK: REPO#RepoId ISSUE PK: REPO#RepoId SK: ISSUE#IssueId 设计时甚至可以使用 ISSUE#IssueId 作为 PK，就是之后还是要建二级索引支持根据 Repo 查询 Issue。 PR PK: REPO#RepoId SK: PR#PRId ISSUE COMMENT PK: ISSUE#IssueId SK: COMMENT#CommentId PR COMMENT PK: PR#PRId SK: COMMENT#CommentId 总结考虑很少人读过这本书，虽说这本书讲的内容很浅，在实际应用中涉及到单表设计还是值得借鉴。","link":"/2024/06/07/The-DynamoDB-Book-note/"},{"title":"DynamoDB 单表设计的优势与考量","text":"大多数开发都有关系数据库设计经验，在初次使用 DynamoDB 设计数据模型的时候，很容易陷入关系数据库的思维陷阱, 不自觉的遵守关系数据库设计的范式， 尝试将数据模型规范化，每个实体或实体关系都有对应的单独的表，通常称之为多表设计。与之对应的是，将所有实体和实体关系都存储在同一张表中，毕竟 DynamoDB 是 Schemaless 的数据库，称之为单表设计。这儿要强调的是，这两种设计只是极端的两点。可能也不是一个合适的命名，因为在实际应用中，单表设计并不意味着只能有一张表。在两个极端之间，单表设计更倾向于将相关实体存入在同一张表中，多表设计则倾向将不同实体类型存入不同的表中。 在官方文档中，单表和多表设计比较时也较为推荐单表设计。本文就来根据实际经验，讨论下实际实践中单表设计的优势。我们自己的项目采用的是单表设计，很大程度上受 《The DynamoDB Book》影响，作者 Alex DeBrie 是单表设计的推崇者。当然，我们项目中已经有十几张表，尽管我们已经尽量将相关实体存入同一张表中。 单表设计优点1. 多个不同类型实体只需要一个请求这个是单表设计的最大卖点，可以在一个请求中获取多个不同类型的实体，减少了请求次数，提高性能并降低成本。《The DynamoDB Book》书中举的例子是用户信息和用户订单信息，如果是多表设计，用户信息表和用户订单表必须分别查询，而单表设计只需要一次查询。在下面表，用户信息和订单信息使用相同的 Partition Key，在同一个 itemCollection 中，可以在一个 Query 请求中同时获取用户信息和订单信息。 实际应用中的问题：上层的应用往往并不关心底层实现，很显然这两个不同的实体，在上层对应这不同的功能方法。更不用说，可能会有至少五六种不同的实体类型在同一个 itemCollection 中，除非刻意设计，否则很难利用到这条优点。在 《The DynamoDB Book》中甚至刻意维护不同实体之间的顺序，以便更高效的查询。在实际应用中，我们根本不会关心同一个 itemCollection 中的实体类型的顺序，总的来说这个优点在实际应用中并不是很明显。 2. 降低账单成本和延时 两条记录总大小不超过 4KB 的单一请求是 0.5 RCU 一致性读两次独立请求总大小不超过 4KB 是 1RCU 一致性读两个独立数据的请求时间平均来说比单独的一次请求要长 账单的优点比较显而易见：减少了请求次数，自然减少了账单成本，通常单个实体记录不会很大，多个实体记录也不一定会有 4KB。延时的优点也很容易理解，单次请求大多数情况下比两次请求要快。 问题是，即便在单表中，在实际应用中，上层服务并不知道下面的实现细节，还是会发出两个请求，这样这个点优点对我们来说一样并不存在。 3. 单表更容易管理 权限维护变少容量管理更容易预测监控更少的 Alarm只需要在一张表上管理密钥 权限维护变少，同时带来的是权限粒度没有那么细，实际当中我们已经觉得我们的权限太细了，每个 Lambda 都配置不同的角色权限。这些优点是实实在在的，单表设计更容易管理，不需要配置更多的配置，也不需要在每个 Lambda 中都添加对应表的访问权限。 4. 流量更顺滑就像股票指数要比单一股票更稳定一样，单表设计的流量更稳定，更能充分利用 预置 功能。 单表设计的缺点缺点不需要解释太多，主要是： 学习曲线陡峭在理解了 DynamoDB 的核心概念之后，单表设计思想不难理解，只是遵循这种设计是有一定的成本的，并不认为学习曲线很陡峭。 同一张表中不同实体类型的数据需求（备份，加密）必须一致 所有的数据变更都会影响到 Streams 使用 GraphQL 更难实现 高级 SDK 比如 DynamoDBMapper 很难处理结果因为不同实体对应不同类 总结总的来说，DynamoDB 的设计比较灵活，很多需求都可以满足，单表和多表之间也没有绝对的界限。即便是单表的推崇者也只是推荐一张表对应一个服务，而不是整个项目只有一张表。个人认为强调单表设计更多的是一种功矫枉过正，尝试让大家摆脱关系数据库的思维惯性。在实际应用中，个人建议是无论如何先需要学习和理解 DynamoDB 的核心概念，然后根据实际需求来权衡单表和多表设计。 相关文章推荐： Single-table vs. multi-table design in Amazon DynamoDB Single table design for DynamoDB: The reality The What, Why, and When of Single-Table Design with DynamoDB","link":"/2024/06/02/DynamoDB%20single%20table%20design/"},{"title":"《从程序员到架构师》笔记","text":"书名有点大，其实书中更偏向于一些架构场景介绍，书中讲解了作者经历过的十几次架构设计，介绍了场景，技术选择以及相关的权衡。书很快就能读完，对我个人来说多少有些收获。 冷热分离书中讲了一个很常见的场景：工单数量越来越多，查询变得很慢，工单的特点就是过了一段时间就会关闭，很少再次访问，所以他们的解决方案也很直接：增加一个归档的状态，将归档的数据存入另外一个数据库，并且增加一种新的查询方式。我们的 Serviceless 系统中为什么没有这种问题？我们使用的是DynomDB 所有的查询都是基于索引，没有分页功能，可以想象根据索引定位指定数据记录，然后根据 SortKey 顺序获取数据，性能上不存在问题。书中还提到了二期实现，将冷数据迁移到 Hbase，主要的考量是查询归档数据方式非常简单，只需根据邮箱或者工单ID。自己没使用过 Hbase 不过它是和 DynomDB 类似的产品，直接根据工单或者邮箱去建立全局索引就可以满足需求。 读写分离与第一个场景相比，查询会很复杂灵活，工单的状态可能会发生变化。数据量非常大，考虑大数据需要借用查询引擎 触发 三种方式 业务代码中修改，日志变更触发 实现 异步更新，使用消息队列，标记数据 存储 ElasticSearch 查询 迁移 分库分表针对海量数据存储的很常见的方案，在我的工作中使用的 NoSQL 没有这种问题。中间件模式 Proxy 模式和 Client 模式个人更倾向于 Proxy 模式，因为可以做到与业务逻辑解解耦。书中选择 Client 模式是想做的灵活可控，不增加新的一层影响运维。选取用户 ID 作为分片主键，必需优先保障查询用户所有订单效率。Hash 取模 每次 2 倍扩容，模数选 2^n 迁移： 日志记录写入消息队列，加入新库新表全部迁移之后做校验，逐步切换 读缓存场景是商品详情页几十条查询，耗时十几秒。分布式缓存 Redis AWS 去年底推出 ElasticCache Serverless 缓存预热 恶意请求缓存击穿 缓存雪崩 大面积过期，随机过期更新缓存先更新再删除 或者 先删除再更新再删除 写缓存举例场景是预约抢购消息队列比较适合这种业务场景，书中设计有些麻烦，使用 Redis 批量写加定时 数据收集场景是收集用户活动，进行数据分析书中架构比较复杂，首先写入本地日志， Logstash 收集写入 Kafka，再选择实时计算框架写入持久层。最主要就是讨论为什么不能直接写入 Kafka 简单来说就是异步数据可能丢失，同步写入必须牺牲性能。 服务发现书中使用的是 zookeeper 没有使用 Dubbo 考虑到其它非 Java 服务 K8s 需要全部改动微服务框架，spring cloud很多不知道的服务 全链路日志这应该只是追踪，使用的是 Sky walking应该是自己部署的开源项目 熔断首先并不是流量大才会导致熔断，目标服务响应时间过长，线程阻塞都会触发熔断。书中给出的例子，比如某个服务接口，调用第三方接口，响应时间过长，占用了太多线程，导致其他服务不可用，比如缓存雪崩，导致数据库服务压力过大，所有的服务都会出现堵塞超时。 线程隔离熔断 Hystrix当前服务与其他服务有强依赖关系，每一个依赖都有一个隔离的线程池在给定时间窗口调用次数超过阈值并且超时比例超过阈值。触发熔断 调用本地 Fallback 限流讲秒杀的时候有提到限流，这一章主要讲限流算法：固定窗口，滑动窗口，漏桶，令牌桶。 漏桶就像名字一样，桶里能放就漏下去，放不下就流出去了。令牌桶比较灵活。分布式还是统一限流，这儿选择分布式是考虑统一时 Redis 压力过大。 RateLimiter 这个场景很有意思，自己还没遇到过。 微服务数据一致性业务场景：下游服务失败上游如何处理实时一致性 TCC最终一致性 MQTCC try confirm cancel 太过麻烦Seata AT 模式自动回滚 数据同步微服务数据依赖问题订单服务依赖商品服务 场景按商品名搜索订单，当然订单表可以扩展商品字段，问题是如何保障一致性问题。Biforst 同步冗余数据库 BFF这个前端就可以负责 后面开发运维场景就没有太深入了。Mock 接口有很简单的方案，支持简单逻辑的比如根据参数返回不同值，这个点没深入研究。一人一套测试环境，我们现在每个任务都会创建一个完全独立的环境，因为 serviceless 很容易做到。这部分作者的情况是一些服务必须服用，所以不能整体重新部署一套环境，解决方案比较切合实际，个人以为这才是实际开发中经常遇到问题的解决方式。 总结这本书对于后端同学来说可能比较简单，对我来说熔断限流是不曾接触过的，其它部分场景多少比较熟悉。","link":"/2024/05/09/%E3%80%8A%E4%BB%8E%E7%A8%8B%E5%BA%8F%E5%91%98%E5%88%B0%E6%9E%B6%E6%9E%84%E5%B8%88%E3%80%8B%E7%AC%94%E8%AE%B0/"},{"title":"工作在澳洲","text":"在对澳洲一切习以为常之前，记下最初感受不同的地方。首先，说下澳洲的互联网状况，在来之前听说过的公司只有 atlassian 一家，后来发现也有不少的小的初创公司。另外就是整体的感受，不像国内能够时不时感受到互联网切切实实的给生活带来变化，随处可见的二维码在澳洲也没有。澳洲节奏很慢，找工作也慢，不比国内很多公司通知面试几场谈下来当天就通知结果。我刚到澳洲时，签证和英语都是一个问题。当时的想法也简单：随便找家 local 公司，练练英语也好。 说说我刚到澳洲时的英语水平，虽说大学英语六级一次过，但是听说真的很水，几乎没有开口说过英语，接到中介电话都听不懂说什么。所以，最开始也只是熟悉下这边的生活，找机会练练英语，有事没事参加下 meetup 。技术类 meetup 一般都提供 🍕 和 🍺。语言类 meetup 大家英语都不好，所以也没什么顾忌，参加的多了，跟人说英语的时候就不会慌。同时你也会发现来自世界各地的人都是一样程度的烂，但是他们很多人更为积极主动，即使说的很烂也很有自信。 开始的一段时间，除了学英语，还在家做饭做家务，十分受气。因为签证只能找短期工作，开始在 seek 上投简历，目标是一份短期合同工作。这边的合同工很普遍，工资相对较高。另外，很多资源都是在中介手上，不过中介还是相对靠谱的。开始的进展很不理想，中介联系之后大多不了了之，到后来才发现，其实合同工对我来说更难，因为英语和签证的原因，对他们来说风险很大，倒是大公司才愿意也有能力承担这些风险。所以，之后就只看全职工作了，LinkedIn 上也比 seek 上更有效率。 之后接到几个面试，开始的时候也不清楚这边都是什么套路，整体慌着学英语，也没时间做很充分的准备，在面试之前就做些编程挑战题目，还有浏览一些面试题。总共也没经历几次面试，比较有含量的就是面试本地一家独角兽企业（事先并不知道，中介通知我面试之后跟我介绍才知道）。中介还是很负责的，给些建议，练练英语。第一次面试就是和 HR 聊一些面试问答题，说实在答的并不好，因为有些记忆性的问题没有准备，还有就是可能当时只说了一个点，面试完了坐在车上想的时候，觉得没有答好。不过反馈说还好，然后通知二面结对编程（之前只在书上看到的概念）。实际就是出题编程，总的来说没什么难度。之后就通知最后一面三小时。了解了这家公司之后，他们也将在中国设立分部，对它还是蛮有兴趣的，于是最后一面花了不少时间准备。最后一面分两轮：第一轮实现一个简单游戏，现场投屏写代码。当时压力很大，加上又对那次机会很重视有点慌。不过第一轮还好。第二轮让实现 Virtual Dom 的 diff 算法，这个与我之前预料的不同，现场不可能好好思考，感觉脑袋完全懵逼。结果就悲剧了。当时觉得很可惜，如果之前不准备这么多，可能面试也不会这么慌，也不会感到那么遗憾。面试完之后接到另一家公司（现东家）电话准备最后一面，不过没过多久就取消面试直接签合同了。因为这家公司说为我做担保，并且支持我上英语课，当时还挺满意的就没有继续找其他机会。 就我仅有的经历来说，澳洲求职还是很可能要与中介打交道的。与国内相比，机会少一些，但是有不少合同工薪酬很不错。面试过程比较可能会要求现场编程，甚至于稍微复杂比如一个小游戏。 再说说这边的工作情况，越是小公司可能越是不尽相同，这家公司人与人之间相处很融洽，你会觉得自己受欢迎。当然这是企业文化的一部分，但是就想这么小的一家公司就如此重视企业文化，想想国内几千人的公司从来没重视过这些东西。办公环境不错，就在 Rock 区，有健身房，很多人觉得最棒的是咖啡机。入职时送一个小 Kindle，每季度会选一本书，当然与管理文化相关。 最开始写了一段时间的 E2E 测试，QA 是质量辅助，测试还是要开发写的，测试已经帮助梳理好了关键 Case，剩下就是用代码实现了。这个国内很少有执行的，有些大公司喊着去测试，结果是测试转开发，开发也不写测试，测试交给外包来做。个人认为就国内情况而言，在野蛮疯长阶段，舍弃部分代码质量是有必要的，但是当客户稳定时，代码质量不容忽视。不止是 bug 带来的隐患，还有人员交替频繁，技术债早晚都要还的。 之后 bootcamp 临时加入其它小组 1~2 个星期，也算是帮助熟悉业务和同事。 敏捷开发毫无疑问是主流，在国内的时候团队也尝试过，结果不尽人意。这个也跟文化和管理方式有关，国内的企业才多久，百度什么时候才提出自己的愿景。敏捷开发对个人素质和能力都有要求，每个人能够充分发挥自己的作用。国内情况很复杂，有自己的特点，人情社会嘛。但是不管怎样，个人觉得必须有一套自己的方法论，拍脑袋分任务未免太初级。 还有一个比较有意思的是，每季度公司会组织一次 Smash it， 两周时间放下手头工作搞创新，做你觉得能创造价值的任何事。后来才知道这个是政府支持的，其它公司也有类似的活动。–《浪潮之巅》讲成功的基因时 3M 成功的秘诀。 管理文化，公司完全信任员工，允许远程办公。推崇 Ownership 每个人都是 Owner，而不是一颗螺丝钉。Ownership 中， 不是 Boss 指挥你做什么，而是员工告知老大自己要做什么。可能因为没有绩效之争，员工之间也比较和谐，Manager 很少管，更多的是怎么支持你的工作。两周组织一次一对一谈话，帮助你 review 或者哪些能够支持你的工作。 前端小组，两周组织一次会议，基本就是一次 Catchup，你可以添加一些话题，比如引入新的工具，新的规范或者技术上的问题都可以在这个会议上讨论。","link":"/2019/08/03/working-in-austrila/"},{"title":"CSAPP 笔记","text":"《深入理解计算机系统》这本书是 CS 的经典书目。翻看之处，思绪总是回到学校，感慨良多，也悔恨当年在学校没有好好学艺，更没有穷根究底的思考。整体来看的话，这本书更像是导论，所讲内容部分属于计算机组成原理，部分是操作系统，还有部分汇编。单从知识点上，这本书讲的并不全面，不过还是有不少收获。 在读此书之前，刷了一遍 CS50 的视频。如果没有 CS 的专业背景，非常推荐看下这个视频。 信息的表示布尔代数和环，抽象代数。大学学过离散数学这门课，里面有部分讲到抽象代数，现在啥都不记得了。 C 语言中的无符号数和有符号数无符号数和有符号数可以互相复制，转换是隐式的。如果运算的两个数分别是有符号和无符号，C 将会隐含地转为无符号数。有些时候的运算结果是违反直接的，比如 -1 &lt; 0u 结果为 0。 书中给了一个下面一个例子，当 length 为了时，直接抛出存储器错误。这是因为 length - 1 得到的是一个无符号数 0xffffffff。 12345678float sum_element(float a[], unsigned length) { int i; float result = 0; for (i = 0; i &lt; length - 1; i++) { result += a[i]; } return result;} 解决的办法就是大多数语言禁用无符号数。 溢出问题有些语言（python）自动处理了这个问题，自动转为大数类型并支持大数的运算。 浮点以 32 位单精度浮点数为例，浮点数的表示形式为[sign + exp + frac]sign 符号位 1位， exp 指数为 8 位，frac小数位 23 位。指数偏移 Bias 为 127 = 2^7 - 1 根据 exp 值不同，有三种情况规格化值：exp 即非全 0 也非 全 1 指数值为 e - Bias 小数值为 1.frac非规格化：exp 全 0 指数值为 1 - Bias特殊值：exp 全 1 无穷以及 NaN 书中的示例 k = 4, n = 3, Bias = 2^3 -1 = 7 描述 位表示 e E f M V 0 0 0000 000 0 -6 0 0 0 最小的非规格化数 0 0000 001 0 -6 1/8 1/8 1/512 0 0000 010 0 -6 2/8 2/8 2/512 最大的非规格化数 0 0000 111 0 -6 7/8 7/8 7/512 最小的规格化数 0 0001 000 1 -6 0 8/8 8/512 0 0001 001 1 -6 1/8 9/8 9/512 最大的规格化数 0 1110 111 14 7 7/8 15/8 240 通过这个例子，我们应该可以看出，从非规格化数到规格化数的平滑过渡。也是上面非规格化数取为 1 - Bias 而非 -Bias 的原因。注意的是，即便 32 位浮点数表示的范围超过 32 位整数，同样的位数能够表示的数字是一样多的。超过一定范围之后，可以表示的整数将不再是连续的。有道习题是：给出不能够准确描述的最小正整数公式。答案是 2^(n + 1) + 1 这个数只跟 n 有关系，对于 32 位单精度浮点数来说就是 2^24 + 1 远小于 32 位整数的表示范围的。 浮点的舍入 就近舍入 1.01111 舍入 1 位时就是 1.1 向偶数舍入（向下和向上同样距离），举个例子：1.01101 舍入 4 位时，可以选择向上 1.0111 或者向下 1.0110 向偶数舍入则选择 1.0110 这跟我们十进制的四舍五入可能会有不同表现：1.445 舍入两位是 1.45 1.435 舍入两位是 1.43 原因是 1.43 离 1.435 要更近一些因为精度问题。1.435 的二进制形式为 0x3fb7ae14, sign = 0, exponent = 0x7f, fraction = 0x37ae141.44 的二进制形式为 0x3fb851ec, sign = 0, exponent = 0x7f, fraction = 0x3851ec 差 419441.43 的二进制形式为 0x3fb70a3d, sign = 0, exponent = 0x7f, fraction = 0x370a3d 差 41943 PS：C 和 JavaScript 得到的都是同样的结果。 最后，好奇为什么这么设计浮点数或者工程师们是怎么想到这样设计浮点数的。我们可以看到在数值比较小的时候密度比较大，能表示的精度也很高，这也是巧妙之处。一般人能想到表示浮点数的方法可能比较符合直觉：整数部分 + 小数部分。相比之下，孰优孰劣,自见分晓。 程序的机器级表示这一章信息量很大，回顾 Intel 的体系结构，整数寄存器，汇编指令格式，寻址方式。 从寻址方式看，C 中的指针是很自然不过的存在，寄存器间接寻址，以及相对基址变址 Imm(Eb, Ei, s) 表示的地址是 (Imm + R[Eb] + R[Ei] * s)，可以满足很多情况下的指针运算，对于 (int*) 的指针 p++ 时 s = 4 只需简单的将 R[Ei] 加 1 即可做到。后面提到，对于基本类型的数组访问，s 都能满足。 之前学习的时候，汇编和 C 都是单独的课程，这一章将 C 编译成对应的汇编指令。 if while for switch 不再叙述。 函数调用和返回 call 指令只是将返回地址（当前 PC 值）入栈，然后跳转到被调用函数起始处。ret 则从堆栈弹出返回地址并跳转到那个位置。寄存器使用惯例：寄存器 %eax %edx %ecx 被划分为调用者保存寄存器, %ebx, %esi 和 %edi (%esp %ebp) 被划分为被调用者保存寄存器。这个在后面的 csapp 实验中会非常有用，更多的使用惯例：https://en.wikipedia.org/wiki/X86_calling_conventions 书中的一个例子： 1234567int P(int x) { int y = x * x; int z = Q(y); return y + z;} GCC 可以将 变量 y 包括在寄存器中，以避免出栈入栈操作。 esp 一直指向栈顶，很少变化，ebp 一般在函数起始设置，便于寻找传递的参数。 1234567pushq %rbpmovq %rsp, %rbp//// movl 8(%rbp), %eax//// 如果有使用需要恢复 %rsp movq %rbp, %rsp popq %rbp 在堆栈上存储空间分配，随便写的一段代码，假设有四个变量。 12345void call(int i) { int y = 3; int z = 4; int j = 8;} 生成的汇编代码： 12345subq $16, %rsp // 分配 4 * size(int) 空间movl %edi, -4(%rbp) // 把参数赋值给 imovl $3, -8(%rbp)movl $4, -12(%rbp)movl $8, -16(%rbp) 缓冲区溢出书中的例子： 1234void echo() { char buf[4]; getBuf(buf);} 堆栈上只分配了 4 个字节给 buf, 如果 getBuf 写入的超过 4 个字节就会出现缓冲区溢出 buffer overflow。栈顶存放的是返回地址，所以很容易被攻击。本书设计的实验 Attack Lab 就是让你利用这个漏洞进行攻击。简单来说就是覆盖返回地址，可以是当前堆栈，然后在堆栈上放置自己的指令。现在的操作系统和编译器也有一些应对之法：随机栈（栈地址不再是连续的），栈破坏检测，限制可执行代码。即便不能执行堆栈上指令，也可以利用 ROP 攻击。本身是因为返回地址还在堆栈上，可以让它指向我们想要执行的代码片段，可利用的代码片段通常是以 ret 结尾，可以拿回控制权，所以称为 ROP (Return-oriented Programming)。 处理器体系结构大部分篇幅都在讲指令流水，这倒不陌生。 寄存器文件的两个写端口允许每个时钟周期更新两个程序寄存器，可以用特殊端口，不选中寄存器。原则：不会为了完成一条指令的执行而去读由该指令更新的状态反例：push 先将 %esp 减 4 再将更新后的 %esp 值作为写地址。 书中的图片 4.39 和 4.41 解释了很多内容。除了控制指令之外，写回阶段也可能存在冲突，这个是数据相关性问题，还有一个问题是取指和访存也可能相关。数据冒险类型：程序寄存器；程序计数器（控制转移）；存储器和取指。 避免数据冒险 暂停，在解码阶段暂停，插入 nop 转发，不必等到写回阶段，不能处理加载使用。还是需要等待上条指令的访存写回结束。 还有很多指令流水的细节在之前可能没有接触过。 优化程序性能这一章非常实用。编译器优化不能改变代码的行为，书中第一个例子很有启发性： 1234567void twiddle1(int *xp, int *yp) { *xp += *yp; *xp += *yp;}void twiddle2(int *xp, int *yp) { *xp += 2 * *yp;} 如果 xp == yp 这两个函数的结果可能会不同，编译器必须假设不同指针可能指向同一位置。 妨碍优化的因素：存储器别名使用；函数调用（无法确定一个函数是纯函数） 消除循环的低效率：移动代码（只需执行一次的代码）只是运算的话编译器优化就可以做到。 减少过程调用 消除不必要的存储器引用 提高并行性 循环分割 寄存器溢出（不足） 在做对应的实验的时候，因为环境的原因（较高的GCC 版本7.5.0以及本地机器配置更好），编译器优化的结果都比基准测试高很多，即使是同样的代码性能也比别人的高出一倍。实验中的一些发现： 可能因为缓存的原因，第一个函数调用耗时较长，注意多注册一次进行对比。 简单的运算编译器已做优化，没必要人工避免重复运算。 即便在循环内消除简单的函数调用（max, min），也不会带来明显提升，可能是编译器已对此类函数进行了优化，也可能是函数本身开销很小，参数寄存器足够，只有返回地址进栈出栈。 循环内消除复杂的函数调用（avg）提升明显。 循环分割几乎对性能没有改进 优化必须定位到瓶颈 存储器层次结构这一章的内容比较熟悉，不确定现在的存储器性能与 CPU 相比是否差距在缩小，书上的数据截止到 2000 年。检查了下自己的机器：Memory LPDDR3 的速度是 2133 MHZ CPU 是 2.9GHZ，硬盘的数据看不到。这样看的话，差距并没有在继续拉大。 局部性（空间和时间），可以说层次结构就是建立在局部性之上的。 链接这一部分内容不熟，所以整个记录下。假设我们有这样的两个 c 文件： 1234567//main.cvoid swap();int buf[2] = {1, 2};int main() { swap(); return 0;} 1234567891011//swap.cextern int buf[];int *bufp0 = &amp;buf[0];int *bufp1;void swap() { int temp; bufp1 = &amp;buf[1]; temp = *bufp0; *bufp0 = *bufp1; *bufp1 = temp;} 运行 gcc -v -O2 -g -o p main.c swap.c 可以发现先是生成了两个目标文件 main.o 和 swap.o，然后 ld 将两个文件链接起来。 链接器就是以一组可重定位目标文件为输入，生成一个可执行文件。为了创建可执行文件，链接器的任务： 符号解析 重定位 ELF 可重定位目标文件ELF 头部.text 已编译程序的机器代码.rodata 只读数据.data 已初始化的全局 C 变量，局部变量在运行时被保存在栈中，即不在.data 也不在.bss.bss 未初始化的全局 C 变量，在目标文件不占用空间.symtab 符号表 存放在程序中被定义和引用的函数和全部变量信息.rel.text.rel.data.debug 调试符号表.line C 代码行号和.text机器指令之间的映射，调试使用.strtab 字符串表 符号表中每个表目： 123456int name: 字符串表中偏移int value: 对应节中偏移int size: 大小char type:4 (data, func,) binding:4;char section: 节头索引 从这儿可以看出，符号表的建立主要是链接需要，本地变量是不在符号表上的。如果一个符号没有在当前模块定义，编译器会假定它定义在其它模块。如果有多个定义，要么抛错，要么选择一个。规则是不允许有多个强符号（函数和已初始化的全局变量），可以有多个弱符号（未初始化的全局变量）。 静态库在编译的时候，并不检查某个函数是否在外部已经定义。在链接的时候就需要指定目标函数的模块了，但是对于标准函数来说，一一指定也太过复杂。静态库可以理解为一组目标文件，毕竟我们在执行链接的时候也不希望一个一个指定依赖的模块。如果所有的标准函数打包成一个大的目标文件，然后进行链接，每次生成得到可执行文件都太过庞大和臃肿。在与静态库链接的时候可以只从中只取需要的文件，整个过程就是维护一个集合：可执行文件和未定义符号，对于目标文件来说直接就链接了，对于归档文件，尝试匹配为定义符号，如果某个存档成员 m 定义了该符号，将其添加进来。继续扫描，直到集合不再变化。如果最后集合依然含有未解析符号，那就抛出错误。这儿就要注意，库和目标文件的顺序是有影响的，不难理解。在 windows 下静态库文件是 **.lib 而 linux 下是 **.a 。可用命令 objdump -a /usr/lib/某个路径/libc.a 查看静态库文件包含的目标文件。 重定位完成符号解析之后，接下来开始重定位，首先合并模块，并为每个符号分配运行时地址。重定位分两个步骤： 重定位节和符号定义。将目标文件中的相同节合并成为同一类型的新节。 重定位节中的符号引用。 第一步不难理解。现在说说第二步，我们上面说了编译代码的时候不关心外部定义的，但是如果我们调用一个外部函数，怎么确定它的地址呢？现在已经编译成 call address 的机器指令了的。如果我们之前没留空了，现在就需要补上了。要正确的补空，就需要重定位表目这个东西。 重定位表目代码的重定位表目：.relo.text 数据的在 .relo.data 里面的条目格式也简单 12345typedef sturct { int offset; // 在节中的偏移 int symbol: 24 // type: 8;} Elf32_Rel; 可执行目标文件链接器生成的可执行目标文件和之前介绍的可重定位目标文件相似， .init 节定义了一个小函数，初始化代码会调用。当然刚说的 .relo 节不再需要了。还有一个段头表值得一说： 123LOAD off 0x0000000000001000 vaddr 0x0000000000401000 paddr 0x0000000000401000 align 2**12 filesz 0x00000000000001cd memsz 0x00000000000001cd flags r-x off 文件偏移， vaddr/paddr 虚拟/物理地址 ，align 段对齐，filesz 目标文件段大小，memsz 存储器中段大小，flags 运行时许可。 加载可执行目标文件shell 会调用加载器来运行它。默认情况下，代码段是从 0x8048000 开始（32位）， 0x400000 （64 位），当然可以链接时可以指定。这儿应该有一张图，linux 运行时存储器映像。从 0x8048000 往上包括，只读段，读写段，运行时堆（malloc 创建），0x40000000 共享库，0xc0000000 往下是用户栈，往上是内核虚拟区。 更多阅读《程序员的自我修养-链接、装载与库》 动态链接共享库静态库的缺点很明显：无法共享，必须重新链接才能更新。共享库可以在运行时加载到任意地址，这个过程叫动态链接。（window 下 .dll 文件， Unix 下 .so 文件） PIC 与位置无关的代码上面讲的重定位技术，会修改代码段的引用，这样做需要链接器修改代码。如果代码是位置无关的，就可以在任意地址加载和执行这些代码。PIC 就是把这些需要修改的空移到数据段上，在数据段开始的地方创建了一个全局偏移量表 GOT。这个实现起来也不难，就是看你需要 5 条指令而不是一条。PIC 函数调用也需要几条指令拿到地址然后调用函数，为了节省指令，编译系统使用延迟绑定，将过程地址的绑定推迟到第一次调用该过程时。第一次调用过程的开销很大，但是其后的每次调用只需要一条指令和一个间接的存储器引用。为了实现延迟绑定，需要过程链接表 PLT，说白了就是第一次结束之后修改 GOT 的跳转位置。在反编译的时候可以看到有很多这样的条目。 12340000000000400cb0 &lt;strcpy@plt&gt;: 400cb0: ff 25 6a 33 20 00 jmpq *0x20336a(%rip) # 604020 &lt;strcpy@GLIBC_2.2.5&gt; 400cb6: 68 04 00 00 00 pushq $0x4 400cbb: e9 a0 ff ff ff jmpq 400c60 &lt;.plt&gt; 异常控制流虚拟存储器这个更熟悉不过了，这个设计也很精妙。 在读这一章的时候，我在想，这些需要软件和硬件配合的设计在最开始整个系统由一个人设计的时候可能会出现。在现在分工越来越高的情况下，估计很难再出现这样的设计了。 程序间的交互整个第三部分都很熟悉,这儿只是简单讲解了系统IO，网络编程，并发编程。","link":"/2020/03/29/journeyman/CSAPP-%E7%AC%94%E8%AE%B0/"},{"title":"使用 React 开发邮件模板","text":"刚来土澳的时候很不习惯每天检查邮件，大小事都是邮件，在国内很多时候都是手机短信通知或者应用内通知。在澳洲待了几年，也习惯了每天查看下邮箱。现在这家公司绝大多数于用户沟通都是通过邮件，包括关键的用户通知，还有营销推送。我们每天发送大量的营销邮件，公司使用 Orrto 这个平台，这个工具可能很少人听说，举个场景：用户在我们网站上进行了一些操作，填了邮箱信息，就成了一个潜在用户。我们会在 Ortto 中创建一个 Journey，这个潜在用户进入这个 Journey 之后，我们会开始会发送一些邮件，过几天会再次发送一封类似的邮件提醒，当然在此过程中他已经完成了相应订单，我们就要将其从这个 Journey 中移除。总的来说，发送邮件的场景很多，我们的邮件模板梳理也非常多。 邮件开发中的问题总的来说，邮件中的页面相对比较简单，更像是一个运营的页面，很少需要用户交互，更多的是作为一个流量入口。开发一个静态页面，写写 HTML 和 CSS，倒是非常简单，但是要适配各种邮件客户端就非常麻烦了。很多邮件客户端对 HTML 的支持还不如 IE6，事实是就算只支持几个主流的最新的邮件客户端，开发也不能用 div + css 布局，而必须用 table 布局，说的就是 2024 年。 最开始的时候我们维护了一些常见的邮件模板，每次有新邮件需求时，都是通过复制粘贴的方式搞定，只要不是新的布局还是很快就能搞定的。只是后来模板越来越多，加上设计也倾向于整点新活，维护起来也感觉越来越累，所以就希望像写前端页面一样轻松。 解决方案React Email因为我们前端选用的是 React，开源的 react-email 就是我们第一个考虑的，不过当时它还是 beta 版本没有采用。同时觉得也不复杂，就按照我们的需求实现了一个类似的库。个人的话肯定推荐使用开源的 react-email。之后我们就根据设计稿，配置不同的主题，开发常用的组件。另外一个特殊的地方是我们的邮件需要支持不同的模板引擎，比如 Liquid 和 Handlebars。 一封设计相同的邮件可能通过不同的平台发送，在模板开发时屏蔽这些不同，我们定义了这样的模板组件。 12345&lt;If condition=&quot;expression&quot;&gt; &lt;If.True&gt;[...components to show when condition true]&lt;/If.True&gt; &lt;If.IfElse condition=&quot;alternate expression&quot;&gt;[...components to show when alternate condition true]&lt;/If.IfElse&gt; &lt;If.False&gt;[...components to show when condition false (optional)]&lt;/If.False&gt;&lt;/If&gt; 最后的开发体验接近平时的前端开发，包括本地开发使用 Storybook，预览时根据 Mock 数据渲染页面，构建时只生成模板。 特性支持如果在开发中，不确定某个特性是否兼容，可以在 caniemail 上查看， 与 caniuse 类似。要注意的是这个网站给的也并不完全可靠，最终结果还是在实际平台上测试，我们使用的是 Litmus 预览在各个邮件客户端上的效果。实际当中会有很多的坑，比如并不是所有的元素都支持 padding；widht:100% 不生效，只能一个一个针对性的处理。 主题支持各个客户端对 dark 模式支持不尽相同，各有各的规则，并不完全受控制。当时是只有 Apple Mail 较好的支持 Media Query，其它客户端有它们自己的规则来反转颜色。最终我们基本放弃了在这上面的适配，不过还是要求所有的图片必须有背景。 各种坑邮件开发中的坑还是很多的。除了邮件客户端渲染支持的各种问题，还有邮件投送问题。比如上面说到我们的营销类邮件一般通过 Ortto 发送，用户可以取消订阅，即便用户将其标记为垃圾邮件，也不能营销业务邮件的收发。营销邮件与业务邮件必须隔离，包括引用的静态资源。我们遇到的一个实际案例就是：用户将我们的营销邮件标记为垃圾邮件后，无法收到我们发送的业务邮件，最后与邮件服务商沟通才得知，因为它们引用相同的外部资源。 最后邮件开发的需求还是非常常见的，不知道为什么像 react-emails 这样的工具很晚才出现，相关的文章也很少见到。 这儿先简单总结下我们最近项目中做的改动，具体技术细节以后再谈。","link":"/2024/04/24/%E4%BD%BF%E7%94%A8-React-%E5%BC%80%E5%8F%91%E9%82%AE%E4%BB%B6%E6%A8%A1%E6%9D%BF/"},{"title":"如何防止重复处理 SQS 消息","text":"问题一般来说在我们的系统中，消息处理必须保证幂等性，以防止消息重复处理。在我们的系统中，下面两种情况可能导致相同消息被重复处理： 调度器和消息生产者：调度器或消息生产者可能会被多次触发，比如时不时有些任务因为超时而被多次触发。 队列管理：如果一个 Lambda 实例处理消息超时，另一个实例可能会在 visibility timeout 设置不合适的情况下得到重新处理相同消息的机会。 如果消息被多次处理，我们可能会向客户发送重复的电子邮件和短信，甚至礼品卡都可能重复发送。所以，我们需要一个通用的机制来确保相同消息不会被多次处理。 解决方案思路很简单：我们将使用 DynamoDB / Redis 缓存来存储消息 ID 和处理状态。当接收到消息时，我们将检查记录以查看是否已处理。如果已处理，我们将丢弃当前消息。如果没有，我们将处理消息并更新缓存。考虑到我们当前的无服务器架构，DynamoDB 是我们的默认选择。 消息处理有三种情况： 首次处理消息：处理消息。 消息已处理或正在处理：丢弃消息。 消息处理失败：重新处理消息。为了处理这种情况，我们需要为记录添加锁超时。如果消息在锁超时后仍处于正在处理状态，要能够被再次处理。 实现 创建 DynamoDB 表 message-processor。这是一个普通表，具有主键 messageId。 实现下面接口的服务：12345678910111213141516171819202122interface IMessageProcessorService { /** * Here use DynamoDB message-processor table as the fact store to decide if a message has been seen before * @param messageId unique identifier for each message * @param lockTimeoutInSeconds how long to lock the message for processing. It gives another chance to reprocess the message if it fails. * @returns boolean: true indicates the lock is acquired and should continue the processing. * false indicates the message is already being processed or being processed by another instance. */ acquireProcessingLock(messageId: string, lockTimeoutInSeconds: number): Promise&lt;boolean&gt;; /** * Mark the message as processed, preventing it from being processed again * @param messageId */ markMessageProcessed(messageId: string): Promise&lt;void&gt;; /** * Remove record of failed message processing, allowing it to be processed again * @param messageId */ releaseProcessingLock(messageId: string): Promise&lt;void&gt;;} 下面的代码片段展示了如何实现 acquireProcessingLock 方法：(我们使用了内部库简化代码) 1234567891011121314151617181920await this.store.replace( { _id: id, status: 'PROCESSING', timestamp: Date.now(), }, { condition: { $or: [ { _id: { $exists: false } }, // insert new record { $and: [ { timestamp: { $lt: Date.now() - lockTimeoutInSeconds * 1000 } }, { status: { $eq: 'PROCESSING' } }, ], }, ], }, },); 最后，我们使用一个简单函数封装既有的处理程序： 123456789101112131415161718192021222324252627282930313233export const makeHandlerIdempotent = async &lt;T&gt;( handler: MessageHandler&lt;T&gt;, IdGenerator: (message: T) =&gt; string, { messageProcessorService, lockTimeoutInSeconds, logger, }: { logger: ILoggerService; messageProcessorService: IMessageProcessorService; lockTimeoutInSeconds: number; },): Promise&lt;MessageHandler&lt;T&gt;&gt; =&gt; { return async (message: T) =&gt; { const id = IdGenerator(message); const acquiredProcessingExclusiveLock = await messageProcessorService.acquireProcessingLock( id, lockTimeoutInSeconds, ); if (!acquiredProcessingExclusiveLock) { logger.info('processMessageIdempotent: message has already been processed', { message }); return; } try { const result = await handler(message); await messageProcessorService.markMessageProcessed(id); return result; } catch (error) { await messageProcessorService.releaseProcessingLock(id); throw error; } };}; 总结总的来说，防止分布式系统中消息处理似乎是一个常见的需求。在实现过程中，发现一个类似的解决方案 How to prevent duplicate SQS Messages?，解释的也很详细。","link":"/2024/05/11/How-to-prevent-duplicate-SQS-messages/"},{"title":"《Clean Architecture》 笔记","text":"看书名不难猜出这是 Bob 大叔的另一本书，无意间翻到这本书的时候瞬间被吸引，因为书中所引出的问题也是自己最近在思考的问题。 1. 架构到底是什么首要的问题是，我们讨论的架构到底是什么？这个问题的答案可能相当模糊，每个人所给出的回答也不仅相同。书中指出，设计与架构没有区别，架构包含所有底层设计细节。底层细节和高层架构是不可分割的，所谓的底层和高层本身就是一系列决策组成的连续体，并没有清晰的分界线。软件架构的终极目标是，用最小的人力成本来满足构建和维护该系统的需求。我们可以用满足用户需求的成本来衡量架构的优劣。 两个价值维度：行为价值和架构价值。行为价值可以理解为新增代码有没有满足需求，架构价值可以理解为新增代码有没有使架构变得更好。相信大家都有这样的经历，接手一个旧项目，维护起来极度痛苦而自己也不得不按旧的架构来编写丑陋的代码。大多数的程序员忍无可忍的时候总是会希望重写整个项目，可管理者总是拒绝。在管理者看来，项目的重写没有带来任何行为价值，所以如果不是频繁的维护是得不偿失的。 2. 编程范式接下来谈论了编程范式，在之前从来没有想过要为这个争论，来到澳洲之后与之共事的同事背景各不相同。我本身对函数式编程并没有偏见，认为函数式编程有自身适用的场景，其中的一些概念也值得学习。但是之后的几次争论让我开始怀疑，其中一次争论：我们项目中服务请求代码是这样的 request(session, url, payload);，每次都需要显示的传递 session，session 在整个应用周期是不会变化的，消费者也无需感知。实在忍受不了到处都在取 session, 传 session, 于是我提议将其改为 request(url, payload) request 自己获取 session。 本来以为一个显而易见的改进会被接受，可不想遭到两个开发的反对：request 不再是纯函数；对测试不友好。即使我搬出设计原则，他们也无动于衷，当然 FP 的信仰者是不会被 OOP 的原则所影响的。这件事也促使我思考，究竟在编程之中我们应该遵循哪些原则，什么样的代码才算上是好代码，我们是不是应该为测试而编程。还有千万小心小心那些固执的书呆子类型的开发，他们搬出的名词其实自己一点都没有理解。之前自己特别愤慨：把简单问题复杂化才显得出自己的能力。在实现前端权限鉴权时，照搬中间件的概念用一组中间件完成了本可以用一个简单的函数实现的任务，不敢相信这是高级开发写的代码。 之前也也多少阅读过关于编程范式的文章，包括为什么动态语言不需要设计模式。这儿 Bob 大叔也指出其实这些语言没有太多本质区别，面向对象编程语言中的特性我们都能够在结构化编程语言中找到。封装，继承，多态其实都不是面向对象编程语言才有的。根本来说： 结构化编程对程序控制权的直接转移（GOTO）进行了限制和规范。面向对象编程对程序控制权的间接转移进行了限制和规范。函数式编程对程序中的赋值进行了限制和规范。 看到这副图的时候才忽然发现控制反转的奇妙之处。 3. 设计原则广为人知的 SOLID 原则。 SRP 原则曾经包含 任何一个模块的应该有且仅有一个被修改的原因。这些原则也适用于结构之中。 4. 组件构建原则这儿说的组件是软件的部署单元，可以完成独立部署的最小实体，比如 Java 的 jar 文件或者 DLL 文件。哪些类应该被组合成一个组件呢？三个基本原则： REP: The Reuse/Release Equivalence Principle CCP: The Common Closure Principle CRP: The Common Reuse Principle 软件复用的最小粒度应等同于其发布的最小粒度。第一个原则看起来好像不言自明。第二个原则是共同闭包原则，我们应该将比较可能同时修改的类放到一个组件中。这个原则与 OCP 比较紧密，也可以说是 SRP 的组件版，平时的新增功能应该只涉及到很少的模块和文件。第三个原则是共同复用原则是不要强迫一个组件的用户依赖他们不需要的东西。 组件耦合三个原则 无环依赖原则 组件依赖关系图中不应该出现环。 稳定依赖原则 依赖关系必须要指向更稳定的方向。 稳定抽象原则 稳定的组件应该是抽象的，不稳定的组件应该包含具体实现。 5. 软件架构 软件架构这项工作的实质就是规划如何将系统切分成组件，并安排好组件之间的排列关系，以及组件之间互相通信的方式。而设计软件架构的目的，就是为了在工作中更好地对这些组件进行研发、部署、运行以及维护。其策略就是要在设计中尽可能长时间地保留尽可能多的可选项。保留可选性就是推迟细节相关的决定。 软件架构设计本身就是一门划分边界的艺术。GUI 与业务逻辑无关，数据库与业务逻辑无关，它们之间应该有条边界线。插件式架构比较形象，GUI 和数据库都是可插拔的，它们都不会影响到业务逻辑。 层次可以用策略（业务逻辑）距离输入输出的远近来衡量。我们经常听到的原则：高层模块不应该依赖底层模块，二者都应该依赖于抽象，抽象不应该依赖于细节，细节依赖于抽象。书中举了加密程序为例： 这里面的问题就是高层组件依赖于底层的读写函数。更好的设计应该是： 在这个图里面可以看出，底层细节与高层策略就是解耦的，而且底层组件是可插拔的。 尖叫的软件架构一章里说，当你阅读的是图书馆的建筑设计图时，整个建筑设计都在尖叫着跟你说：这是一个图书馆。当我们查看应用程序的结构和源码时，它应该在喊：XX系统，而非一堆技术名词。在我们按层组织项目时，尤其常见，首先看到的就是controllers, services。良好的架构应该尽可能允许用户推迟和延后决定采用什么框架，数据库，Web 服务以及其他与环境相关的工具。这个听起来很有道理，但是在实际做项目时，往往会先考虑这些或者这些本身就是既定的。 首先是独立于框架，UI，数据库以及外部设备。外层代表的是机制，内层代表的是高层策略，依赖关系是底层机制指向高层策略。 Main 组件负责创建，协调监督其它组件的运转。Main 组件是系统的底层模块，处于整个架构的最外层，主要负责为系统加载所有必要信息，然后将控制权交回系统的高层组件。 关于服务。微服务近年来非常流行。原因可能有：服务之间是隔离的，服务被认为是支持独立开发和部署的。虽然服务化可能有助于提升系统的可扩展性和可研发性，但服务本身却并不能代表整个系统的架构设计。对微服务迷信的人，想用微服务拆分单体项目，很可能得到的只是一个分布式的单体项目。最近就有同事讨论拆分单体应用，某个项目太大了维护起来费力，不免想用 lerna 管理多仓库的形式。我个人并不认为不应该拆分，而是认为项目本身的问题不在那儿。即使拆分了，得到的可能也是多个单体项目，即便是单体应用也可以有很好的组织和边界。仔细想一下：为什么将项目划分为几个包就能够解决问题了，前端的包与文件夹有多大的不同？查看下每个文件，每个模块的依赖，是否满足上面提到的原则，重新组织项目要比拆分来的更实用。 6. 实现细节Web，数据库和应用程序框架都是实现细节。以 Spring 为例作为依赖注入框架还可以，但是使用 @autowired 的话业务对象就感知到了具体的框架。最近也刚好想引入一个新的框架 NestJS，我们也知道一旦引入框架很可能与框架相绑定，所以不得不仔细评估。","link":"/2020/02/23/journeyman/Clean-Architecture-%E7%AC%94%E8%AE%B0/"},{"title":"《国富论》篇一笔记","text":"几年前曾购得本书，但最终只读了一小部分就束之高阁了。最近有一些经济学问题一直萦绕在我脑中，希望从书中找到答案。同时自己也在做些读书计划上的调整，准备集中一个领域做主题阅读而非”泛泛而读“。 《国富论》出版于 1776 年，这一年美国独立，第一次工业革命已经开启，而中国是乾隆四十一年。即使有着这样的心理准备，读这本书的时候总觉得时间跨度没有那么大，书中提到的一些商业和工业模式实在很难相信是几百年前就有的。虽然说当时中国的财富可以比肩欧洲，但是个人认为当时的中国已经落后太多了。书中提到的政治制度，法律制度，商业贸易程度，当时的中国都需要很长时间来追赶。在思想的上的差距或许更大，本书的中译本最早是在 1902 年由严复完成的。即使是现在 2020 年，这些差距依然很大。在普遍缺乏理性思维的社会中，对国学和古装剧的喜爱和追捧并不见得是好事。 接下来进入正题：所谓国民财富就是供给国民每年消费的一切生活必需品和便利品。篇一：论增进劳动生产力的因素及分配劳动生产物给各个阶层的自然顺序。 社会分工提升劳动生产力。书中以扣针制造业为例论述了分工带来的效率提升，分工在其它制造业上也有同样的效果。但是农业具有特殊性，农业不能采用完全的分工制度，才使得农业在劳动生产力的增进上，总是跟不上制造业的步伐。所以，品质同样优良的小麦，在富国市场的售价未必都比贫国低廉。在制造业方面，贫过却无力与富国竞争。分工提升效率的原因：劳动者因为专业掌握了技巧；免除了切换工作的时间；适当的机械简化和节省了劳动。在最后一点上，作者又指出，这些机械的发明好像也是分工的结果。相比较而言，如果只专注于某一项工作，更可能带来新的改进。分工使社会更为富裕，分工无处不在。 接下来讨论分工的起源：交换。市场大小会限制分工。由于分工起源于交换，所以限制分工程度的是交换能力的大小。这个不难理解一些职业只有大城市才有，农村的工匠经常兼营几种性质类似的行业。相比于陆运，水运开拓的市场要广大的多。贸易和水运的重要性相信看过《大国崛起》或玩过《文明》的人都有极为深刻的感受。地中海，海面平滑，距离海岸近，适宜初期航海。大河文明和地中海文明的兴起就是得益于便利的贸易。 紧接着谈货币的起源和效用。从物物交换到交换媒介的出现，金属最终成了所有的国家都决定使用的媒介。金属不易磨损，还可以任意分割这些特性注定了’货币天然是金银‘。一个有意思的现象：每个国家小额的货币还是金属制的硬币。各个国家选择的金属可能不同。金属有两种极大的不便：不方便称重；不方便化验真假。人们为了解决这个问题，引出了铸币制度。‘铸币的两面都被盖住，有时就连边缘也会被盖住。刻印铸币时，金属的纯度和重量都得到了确定。’ 仔细研究手上的货币，周边的锯齿也是防止被人动手脚（牛顿设计），古代钱币可能仅考靠周边的凸起鉴定吧。即使这样，私自铸币在古代看起来也不像是太有技术含量的事情，古代禁止私自铸币的刑罚不可谓不严酷，依旧屡禁不止。另外，国库亏损时，官方滥发薄币也是常态。回到铸币上来，铸币的名称原本是要表明金属的重量或数量的。现在的货币单位是重量单位过来的。世界各国的君主都是贪婪而偏私的，他们欺骗臣民，次第削减货币最初所含金属的真实分量。君王和国家采用这种办法，就能用较小量的银来偿还债务、履行各种契约。 人们在用货币进行交换货物时，都要遵循一定的法则。这些法则，决定了商品的相对价值和交换价值。三个问题： 交换价值的真实尺度是什么，即什么构成了一切商品的真实价格 真实价格由哪些部分构成？ 有时自然价格与真实价格不一致，是什么原因导致的？ 衡量一切商品交换价值的真实尺度就是劳动。劳动的真实价格，就是一定数量的生活必需品和便利品，它们与劳动等量。劳动的名义价格，就是一定数量的货币，它不一定与劳动等量。作者指出劳动名义价格存在差异的原因是金银价值的变动。随着时代变化，铸币所含的金属分量可能会发生变化，另外金银价值本身也会变化。人们在计算商品的交换价值时，多是以货币量为尺度，而不是按它所能换得的劳动或其他商品的量。当且仅当时间和地点相同时，货币才是衡量一切商品交换价值的正确尺度。在世纪范围内，等量谷物支配等量劳动的可能比等量白银更大，所以谷物更适合作价值尺度；而在以年为期限时，等量谷物支配等量劳动的可能又比等量的银更小，因此白银更适合作价值尺度。 衡量某一特定商品在不同时间，不同地方的真实价值。劳动量无法衡量，但是谷物时价与劳动时价比较一致，所以我们可以比较谷物价格。金银铜三种铸币中，往往有一种会被特别选定为主要的价值尺度，可以成为本位币。因为金银铜这些金属的价格本书就经常变动，金币与银币的比值，只要市场能够唯一决定。因为这一比例带来的便利，很多国家会规定这一比例。当不同金属铸币有不同价值时，如果法定兑换比例保持不变，那么支配一切铸币价值的金属就是价值最昂贵的那一种。古代中国最初应该是铜本位，汉唐时绢也是硬通货，经常看小说也提到赏赐与馈赠都有绢，因为铜币可能通货膨胀崩溃，当时读的时候还觉得直接送银子不就得了么。 商品的自然价格组成：劳动工资、资本利润、地租。市场价格受供需影响。供求关系已经广为人知，这儿就不再赘述。讲完商品的市场价格与自然价格的差异，接着要讨论的就是组成部分的变化。 工资率是在什么状况下自然而然地确定的，这一状况受到了社会的贫富、进步程度的哪些影响 利润率是在什么状况下自然而然地确定的，这一状况受到了社会的贫富、进步程度的哪些影响 货币工资和货币利润之间的比例，是由什么支配的 支配地租的因素，以及使得一切土地生产物的真实价格变动的原因是什么 劳动生产物是劳动的自然报酬，或者是劳动的自然工资。劳动者的普通工资是由劳资双方协商的，或者说是由契约决定的。决定劳动工资增高的因素，不是现有的国民财富有多庞大，而是国民财富的不断增加。也就是说，劳动工资增高的因素是国家的发展速度。无论一个国家有多富有，只要它在长时期内处于停滞不前的状态，它的工资都不会很高。书中以大不列颠为例，说明了世纪内劳动的真实报酬有很大程度的增加。人口生产受制于人口需求 为何当今发达国家的人口呈现负增长？劳动过剩？支配劳动价格的因素有两种，一是劳动需求，二是生活必需品和便利品的价格。荒年，食品价格上涨，劳动需求也因物价上涨而减少，从而使得劳动价格重新降低。 资本利润可由货币利息率得知。当资本增加时，劳动工资提高而利润降低（资本涌入某一行业）。探讨工资和利润的关系是因为有时高工资会降低利润。劳动和资本用途的不同，造成了货币工资及货币利润的大不相同。这个原因有两个：职业性质本身以及各国的政策由职业性质造成的不平衡根据我所观察到的情况来看，一方面能补偿某些职业的微薄货币得利，另一方面又能抵消另一些职业的优厚货币得利的情况，总共有五种。一、职业本身是否令人愉快；二、学习该职业的难易程度、学费的多少；三、该职业有没有安定性；四、职业责任的轻重；五、取得职业资格的可能性大小。 这些东西不难理解，记得经常看到报道说澳洲的搬砖工薪资如何之高，仔细思考一下就得知不大可能：如果一个职业没有任何门槛又可以获得高薪，很多人都会涌入这一个行业，也不存在这么多流浪汉了。有些媒体在宣传时薪的时候闭口不谈职业的稳定性，这个不难猜测其中隐藏的问题，这其实和道旁苦李是一样的道理。即便存在一棵李子树并不苦，但是我们也往往容易高估我们幸运的概率。不平衡政策主要有三种：限制某些职业竞争人数；加大某一职业的竞争程度；限制劳动和资本的流动性。作者讨论了同业组合和学徒制的不适用性。 最后讨论下地租，这儿的地租与租金不同，而是按比例抽取产出。工资和利润决定了价格的高低，而地租则由价格决定。作者讨论了三个问题：一是总能提供地租的土地生产物；二是时而能够提供地租的土地生产物；三是在不同的改良阶段，这两种土地生产物（或其制造品）自然产生的相对价值的变动。 一国土地和劳动的全部年产物，其全部价格都由土地地租、劳动工资和资本利润这三部分构成。这三个部分，代表了分别以地租、工资和利润为生的这三个阶级的人民的收入。这三个阶级，是文明社会的基本阶级；他们的收入，是其他所有阶级收入的总源头。 本书中关于中国的描述总觉得不太确切，因为当时中国与欧洲的联系并不密切。如果贸易联系并不紧密，对物价的评定是否有足够的证据支撑值得怀疑。另外一个方面，读此书的时候会觉得制定的一些法律称不上深思熟虑，带来的效果只是阻碍了经济的发展。除了这些法律就是代表制定者自身的利益之外，也不得不考虑当时确有需要才制定的这些法律。比如对面包价格的制定，对于当时的社会来说，市场并不像今天这么广大，小城镇的面包师足够以垄断市场，饥荒年代商人囤货肯定会加剧物资的紧缺。个人认为，质疑这些法律条文不适用时，也需要思考是什么造就的那样的法律。 回过头看第一篇内容，本书的逻辑非常紧密，要比教科书更容易阅读。书中又对上面的很多点也展开了讨论，并列举了大量的实例来说明。这本书耗时十年完成，在那个年代收集书中的数据困难程度不难想象。","link":"/2020/02/18/journeyman/The-Wealth-of-Nations-Part-1/"},{"title":"《Eat that frog》笔记","text":"这是一本关于时间管理的技巧性书，书中有21个技巧来克服拖延。本来不打算写读书笔记的，因为最好是按照书上方法亲自实践的一番。这个总结摘自邮件，顺便整理了一下。本书内容概况来说就是思考和定义目标，制定周详的计划，任务拆分，设置优先级，专注最重要的事情。 Set the table写下你的目标，仔细思考你的目标，越清晰越具体越好。拖延的主要原因就是没有目标，没有动力。所以首先要做的就是树立清晰的目标。可以是提升专业技能，或者学习金融知识，或者是抚养孩子。写下来并设定截止日期，列出你可以做的任务以达成目标。组织优先级和顺序。立即执行，每天完成一些任务以达成目标。 Plan Every day in advance制定每日，每周，每月计划。每天晚上休息之前可以制定第二天的计划。这是必需的，花在计划上的一分钟都会节省你之后的10分钟。 Apply the 80/20 Rule to everything利用二八原则。时间管理就是个人管理，人生管理。 Consider the Consequence这个是在强调思考重要性的时候，想一下结果。尤其是长期结果，可能看起来重要的事情长期来看没那么重要了。 Practice Creative Procrastination针对优先级低的任务有意的拖延。 Use the ABCDE method任务划分优先级：A1 A2… mustB1 B2 … shouldC1 C2… would be niceD… DelegateE …eliminate Focus on Key Result Areas专注于只有你才能做的事情。（工作上）思考因为什么公司付你薪水。 Apply the law of three80% 的人共同的三个目标：财务和事业；家庭和个人关系；健康和体质。可以接着思考三个重要的财务和事业目标，三个最重要的健康目标，可以做的三个任务以达成某一目标。 Prepare thoroughly before you begin好好准备，包括清理桌面。只留下与最重要目标相关的东西。一句话就是，桌面就是为目标准备的。 Take it one oil Barrel at a time一次迈一小步。列出你需要完成目标的步骤。 Update your key skills持续学习。每天阅读至少一小时在自己的专业领域上。在通勤时听音频节目。参加一些课程。 Identify your key constraints找出实现目标中最大的不足， Put pressure on yourself设置deadline 想象自己在某天之后出门等等。 motivate yourself into action激励自己 Technology is a terrible master拒绝成为科技的奴隶。每天早上和下午关掉手机和电脑一个小时。可以尝试坚持在就餐时放下手机。 Technology is a wonderful servant每天只检查两次邮箱。 Focus Your attention集中注意力 Slice and Dice the Task Create large chunks of time 培养紧迫感进入’心流‘状态 一次完成任务一旦开始就不要停下。考验自律的时候 对我来说，我没有制定每月计划，每周计划从季度 OKR 里分解。每天早上几分钟列出任务，然后完成早上的一个学习任务之后才能看手机。晚上10点之后不用电脑和手机。对于长期任务比如健身和学习，只有分解成切实可行的小任务并且定在每周计划上才有作用。","link":"/2020/02/14/journeyman/Eat-that-frog-%E7%AC%94%E8%AE%B0/"},{"title":"工作中的前端工程化问题","text":"看过张云龙在git上关于前端集成的讨论，总结下工作中的问题。去年初的时候读过，之后也思考总结过我们平时开发项目中遇到的问题。现在再重新梳理一遍，主要是组件化。InfoQ 上的文章 &amp;&amp; github 上文章 先谈谈在工作中遇到的问题。平时工作中负责的管理系统较早的开始在两三年之前，采用了某个 SPA 框架，以及搭配的 UI。有几个现基于此技术系统需要不时升级维护。在开发过程中，经常遇到该 UI 引起的很诡异的 bug，并且该 UI 源码较难读懂维护。于是在新的系统中使用了另一套 UI，新的 UI 代码容易理解，也容易在此之上扩展。 遇到的问题最开始接触这些项目时，规规矩矩按照框架的配置规范开发页面，遇到问题看着代码排查，觉得还很顺手。时间久了，做的多了，感觉有不少重复的工作，比如每次添加页面做的事情有些过程是完全重复的。在一个项目中有一个复杂的页面，代码有 1000 多行。维护这个复杂的页面实在麻烦，于是开始思考解决这种问题。 在那个复杂的页面中，确实有很多事情要做，主要有几个大块头：一个按钮点击之后弹出对话框，选择几个数据，这样的组件有5个左右。然后显示在页面上；还有个是选择后的资源列表有优先级要求，要求支持排序；还有最下面的非常独特的功能强大的可编辑表格。 组件封装对这个问题，解决的方法是在原来的基础UI上，封装业务UI。将上面提到的大块头封装为一个业务级的UI组件，每个UI组件只对外暴露 setValue（编辑页面时，填充内容）， getValue（提交表单时，获取组件数据），render （渲染组件）。封装完业务UI之后，工作轻松了不少，不过还有些重复的劳动力，就是刚提到的每个页面很相似，只有少许的不同，每次新增模块，复制一份修改，也显得太不专业了。不过项目快结束了，不管了。 多个项目中的组件模块化重构完那个项目之后，接手了几个项目的维护工作。发现了更多的问题，总结一下大的问题： 几个项目的组织不尽相同，虽然项目大体是一致的，但在配置信息、用户信息、导航处风格不一样（插一句：导致如此的原因与选用的框架有关）。 有些项目还处于之前提到的状态，类似功能在几个项目中多种实现。 上面提到的新增一个模块，复制修改，在几个地方配置。 维护公共模块。几个项目中公用框架和基础UI，都是独立的。 采用的框架是独立的，遇到问题只能看代码，前人踩过的坑后人接着踩。 受上面文章影响，平时也用 grunt 工具，考虑用 yeoman 解决这些问题。对第一三问题，除了规范，就是使用 yo 定义 generation 生成项目脚手架，新增模块和组件都能用命令生成一个空的文件。对第四个问题最好的解决办法是，在一个独立的项目上维护这些公用库，搭建自己的私有 bower，维护自己的 cdn。对公用库，要求严格些，必须通过测试，有齐全的文档和 demo。第五个问题，可以在 gitlab 上记录 issue。 后来跟大家讨论过上面的问题之后，大家普遍觉得那个框架太旧了，不如投奔开源的，先看看 angularJS 吧。 文章之外业务UI 为什么有差异？项目时间长，前后负责项目的产品和码工都不是同一个人，更不用说几个不同的项目了。如果前端UI库齐全，可以反过来要求产品设计遵循规范的UI。","link":"/2015/01/07/journeyman/fis-in-work/"},{"title":"《浪潮之巅》笔记","text":"之前自己对畅销书一直有成见，这本书一直没在自己的读书清单上。最近因为内人的强烈推荐，一口气把这本书读完了。先说说感受：这本书的信息量很大，囊括了信息产业革命之后所有知名技术公司的兴衰。好多年前，听朋友在谈这本书的：回看过去，全球技术的变革就像浪潮一样。当时不以为然：这都能算是一个概念，这和蓝平长二畅销书一个套路嘛。如果当时朋友这么推荐的话：过去几十年信息产业公司巨头的兴起和衰落，其实有着深层次的原因，通过这本书，可以了解未来互联网格局的变化；当时或许就不会错过这本书。虽说现在读这本书没有那种恍然大悟或者相见恨晚的感觉，但也不得不说如果早几年读这本书或许自己会有不一样的选择。再说说这本书不好的一面：过分的强调基因的作用，给人一种宣扬宿命论的感觉；另外作者的主观色彩比较浓厚，毕竟这本书就是他个人的视角（可以想象一下，如果这本书出自一个做研究的学者，对每个公司可能会有非常慎重公平的评价）。 整个社会发展的趋势是不可阻挡的，2013 年毕业的时候，在任的国家政府就一直努力控制房价，各种手段用尽房价不跌反涨。房价上涨的根本原因是：城市化过程中大量的购买需求以及货币大放水。即使政府用尽手段也无法改变经济规律，而政府的干预往往是以牺牲效率为代价的。当时恐怕很多人也看不懂，不明白为什么房价会上涨，什么时候会停下。当然现在中国的房价可以说是控制住了，不过失去定价权的市场也称不上为市场经济了。其实当时我也不明白，直到后来看到任志强的言论才恍然大悟，可是这位敢言的勇士最近也被调查了。还有另外一件事也跟趋势有关：在 15 年加入阿里时，当时有几个比较被人看好的部门，一个是阿里云，另一个是村淘。阿里云自不必说，村淘当时马云说要投入 100 个亿，同时政府对农村也给予了很大的支持。当时公司食堂也经常见到村淘的宣传，一年之后就很少再听到内容的宣传了。如果认清城市化这个趋势，不难发现农村的市场是无法和城市相比的。国家一直在提的振兴东北，其实也是逆势而为。如果哪天朝鲜开放了，东北根本不需要你扶。 1. AT &amp; T 的衰落虽然大家普遍认为 AT &amp; T 的衰落从反垄断拆分开始，但是事实可能并非如此。在拆分之后，公司业务依然在增长。利令智昏：1995 年，AT &amp; T 将自己分为三个部分：从事电信服务的 AT &amp; T，从事设备制造的朗讯，从事计算机业务的 NCR 。第一次的拆分我没看出太多问题，看样子这些确实是独立的业务。不过作为整个公司发展可能会更有优势，向移动电话业务转变可能会更有优势，但是移动电话的发展也有可能受传统电话部门阻碍。公司拆分会对公司的运营造成很大的影响，短时间内员工可能都在讨论分家的事情，这么看如果只是因为贪婪而拆分确实是利令智昏。外来的冲击才是致命的，这家百年老店无力应对新兴公司的挑战。 2. IBM说实在的我也一直以为 IBM 已经过气了，对它的服务了解不多。 IBM 在小沃森的领导下，开始领导电子技术革命。 IBM 投资发展计算机，主要客户是政府，军方，大型银行以及跨国公司。IBM PC 可以说很有希望占领 PC 市场的，但是当时占它的分量并不大，没有给予重视，另外因为反垄断的原因，他也无力阻止其他 PC 的兴起。 郭士纳时代，IBM 成为服务型技术公司。在研发方面比较特别的一点是：降低研发费用，不再保留原来的没有效用的研究，加强与大学的合作。 3. 八叛徒1947 年，肖克利发现了半导体P-N结的单向导电性，并利用这一原理发明了晶体管。1956 年，肖克利在湾区创办了肖克利半导体公司，年底获得诺贝尔奖。虽说笼络到一批人才，但是肖克利不是一个好的老板，傲慢专横，唯我独尊。第二年，摩尔等八人出走创办仙童半导体公司。仙童公司发展顺利，诺伊斯发明了集成电路。1959 年菲尔柴尔德回购了所有的股份，创业团队也逐渐失去话语权，导致人才不断流失。不断有人从仙童离开出去创业，也算是仙童孕育了半导体产业。仙童对创始人与高管的出走，员工跳槽开始变得习以为常，听之任之。这个可以说是硅谷文化的一种文化现象。仙童公司在诺伊斯出走之后，就落伍了，可以说辉煌的时间只有短短几年。但是仙童的影响确实非常大，从仙童走出去的人又开创了新的公司，把半导体技术传播开来，而这些新的公司也完全与旧式企业不同。 4. 苹果苹果的故事，大家耳熟能详了。1984 年苹果推出麦金托什，一款图形化交互界面的 PC，可以说是真正面向个人的 PC。1985 年，乔布斯被踢出苹果，创办 Pixar 动画工作室。之后的事大家都知道了。 作者认为苹果的黄金期已经过去。去年的 AirPods 可以说是比较新的增长点吧。不过无线耳机，智能手表这些产品苹果几乎没有什么创新，产品推出的时间也都落后于其它公司。另外，传说中的苹果汽车以及自动驾驶也不见动静。从这些方面看，苹果是要落伍了。不过在我看来问题不大，这些产品用户粘性不大，护城河不深，即使无线耳机不是苹果发明，苹果也能够凭借技术和设计优势占领市场。另外，苹果还有大量的现金以完成技术收购。苹果过于依赖硬件销售，现在有转服务的想法，也不能说不是一个办法。不过话说过来，如果手机市场丢掉的话，谁还愿意溢价购买它的服务呢。 盛田昭夫，70年代的乔布斯，这个之前只在公众号上有所耳闻。有必要找些关于他的书读读。 5. 信息产业的生态链计算机发展的规律：摩尔定律、安迪-比尔定律以及反摩尔定律。摩尔定律，这个大家都比较熟悉了。2019 年英伟达 CEO 表示，摩尔定律已经失效。现在不妨思考下，摩尔定律失效会对半导体行业产生怎样的影响。安迪-比尔定律之前听说过，不过没有仔细思考。在 PC 时代，这是安迪和比尔可以操纵的，也确实能够感受到。在移动互联网时代，软件厂商应该没有动力去做这件事，再说，像苹果减速门这样的事情是应该受法律制裁的。在开始使用安卓手机，确实能够感受到手机速度的下降，当时归因为安卓的系统问题。其实不管怎么说，现在的新款手机还是比旧款速度快很多，即使装上一大堆的应用，可能大家不会觉得手机整体速度有什么变化。大家期待的新功能好像也只有更强大的摄像头，随便一张照片要上 M 大小，在旧手机上传输或处理这些大的音视频文件的时候也会让人觉得自己的手机变慢了。影响手机寿命最主要的就是电池，不过电池的更换并非难事。最后，手机工作的环境相对来说是比较恶劣的，时不时摔一下，还可能掉水池里，而且几乎每时每刻都在使用，严格来说它的寿命确实会短一点。总的来说，我并不认同在安迪-比尔定律适用于手机。大家购买新款手机，往往看中了新手机的特性，不然总感觉自己的手机还能再用几年。指纹解锁，面部解锁，全面屏等这些新特性才是大家购买的动力。对于游戏玩家来说，可能会对硬件性能比较敏感，那是另一回事。反摩尔定律就是反过来看摩尔定律，硬件厂商必须赶上摩尔定律规定的更新速度。这也解释了为啥硬件厂商这么苦逼。 6. 英特尔1968 年摩尔和诺伊斯创办了英特尔公司。不过要成为一家巨头并没那么容易，20世纪80年代，日本经济达到巅峰的黄金时期，美国来自日本的冲击很严重，当时的英特尔不得不放弃内存业务，转而专注于处理器。如果当时选择在存储器上与日本企业死磕的话，那估计也不会现在的英特尔了。英特尔的成功很大程度上受益于 PC 的崛起，为了抢占 PC 市场 IBM 竟然直接使用了 8086 处理器，英特尔一战成名。当时的对手摩托罗拉败的着实可惜，书上说原因时提到，统帅水平相去甚远，最后提到说摩托罗拉不够专注。最后一点私以为不足以成为原因，大公司的业务范围更多，但并不代表不专注（就像我们不能说微软做 office 不专注），更多的是旧时代的公司没有新公司这么拼命吧。《只有偏执狂才能生存》 – CEO 安迪·格鲁夫在这儿插一点，书上强调了 CEO 的作用，已经讲到了几个著名的 CEO，不过并没有详细的介绍他们的作为。 指令集之争：现在来看，CISC 以及 RISC 之争没太大意义， 现在的 RISC 也加入了复杂的指令， 而 CISC 也引入了高性能流水线。 错失移动时代：给我的感觉是英特尔连挣扎都没挣扎就败了。 7. 微软在大学的时候读过一本关于微软的书《比尔盖茨的野蛮军团》，不过那本书出的时候微软正是风头正劲的时候。微软的故事就不再叙述了，只是觉得作者对微软有成见，作者更看好 Google 。不过从股市上看的话，微软这几年的表现反而比 Google 要好的多。比较来说，我自己更认可 Google 的服务。 8. Oracle在此之前对甲骨文公司的不多，只知道它是家做数据库的，后来收了 Sun。据说这家公司没啥技术，养了一帮律师，专门碰瓷打官司。还有人说，碰瓷 Google 是埃里森为乔布斯复仇，不过八成是为了钱。这个人的私生活就不谈了。总体来说，Oracle 没多大创新，不过也没有致命错误。2020 年了，有点吃惊这样的公司竟然没在去 IOE 成风尚的时候没落。 9. 思科思科是做交换机和路由器的，也是华为的竞争对手。思科的发展还是比较顺利的，可以说正是互联网兴起的时候，有大量的需求。成就思科的是 CEO 钱伯斯，可以说没有思科，会有其他公司生产交换机和路由器，但是没有钱伯斯思科就不会是现在的思科。钱伯斯认为 IBM 的失误是出在文化而非技术上。所以他就致力于打造一个健康的文件：善待员工，40%的期权分给普通员工；满足客户需求，倾听下属和客户。公司管理使用网络组织结构，是从钱伯斯开始（关于这一点没有找到更多相关文章）。维持创新的方式：鼓励员工内部创业，创业成功思科有优先权收购。 托尔斯泰讲，幸福的家庭都是相似的，不幸的家庭各有各的不幸。在信息产业中，这句话要反过来，成功的公司各有各的绝招，失败的公司倒是有不少共同之处。我的看法不同：一个致命错误就足以导致一家公司失败。所以说失败的公司可能是各种各样的原因，而成功至少需要各方面都不差。 思科的对手华为，在 1998 年的时候开始学习 IBM 的管理方式，而且执行的比较彻底。从这点看，任正非确实不一般。实际上，中国企业兴起的时间都很短，在企业运营和管理上都没有理论指导，在这方面确实需要向发达国家学习。阿里巴巴也算是从通用那学习了价值观那一套，而其他很多的 IT 公司可以说根本没有自己的文化。 最后，这么多年过去了，大公司的弊端依然很难根除：大公司人浮于事，组织臃肿，管理层短视。可以说现代企业管理的方式根本没有跟上技术发展。 10. 雅虎关于雅虎不再多说。值得说的就是雅虎的商业模式，在当时确实可以说是关键性的创新。抛开商业模式，我个人觉得国内的人给予雅虎过高的评价，没有雅虎，会有其它的门户网站。作者认为当时的雅虎应该和谷歌合作，专注于品牌广告。个人认为根本不可行，门户网站衰落是必然的，商场上可是没有永远的朋友，只有永远的利益。雅虎根本没有一个清晰的战略，只是顺应了时代的浪潮。 11. 惠普个人真看不懂为何并购一家衰落的公司。或许当时谁能想到 PC 市场竞争会如此激烈，更糟的是反摩尔定律的存在， PC 行业的利润都被 WinTel 夺去了。并购康柏形成对戴尔的优势 拜托，不是市场大了就有竞争力了，因为没有竞争力，市场份额比较小，怎么并购能够带来竞争优势。 之前以为廉价出售打印机，高价卖墨盒行得通。想一下就会发现，如果墨盒能够被其它公司生产，简直是给他人做嫁衣。 马克·赫德改革：对 PC 强化与戴尔直销模式差异的代销，简化供应链；丰富打印机产品线；突出技术服务。可以说赫德的改革是成功的，后来的继任者可能没有那么大的能力，但是即便赫德在任，PC 和办公设备也很难与亚洲企业进行竞争。不过在金融危机之后，企业软件和服务往云计算和 SaaS 服务转型的时候，惠普落伍了。 可能一个公司在很长的一段时间内都不会去思考转型的问题，在趋势来临之际转型必须迅速，执行者必须有足够的魄力。就像当年移动互联网兴起之际，战场全面转到移动端，国内很多公司给予很多移动用户补贴，在关键时期掉链子那就真的落伍了。 12. 摩托罗拉摩托罗拉在二战之后开始兴盛，到 80 年代初发明大哥大，可以说是风光无限。当时在无线通信，数字信号处理，CPU 这几个领域都有机会成为王者，可惜最后哪一个市场都没吃掉。单纯看无线通信，数字手机不难追赶，失去市场是因为只注重质量而忽视易用性。 在任何时候，都不要对技术情有独钟，市场永远是正确的。— 钱伯斯把钱伯斯的这句话放在这儿简直不要太合适，可惜当年的摩托罗拉不是在钱伯斯领导下。 铱星计划 - 这个计划有点酷，可惜失败了 13. 硅谷创业成功需要的因素：一是创始团队；二是商业模式；最后最重要的是运气。之前看过一个统计，分析得出的结果最重要的也是运气，大家都以为想法才是最重要的，其实并非如此。很多成功公司最后最的事情与最初都不相同，即使是我们熟知的阿里巴巴，最初也只是提供 B2B 服务的，谷歌最初要提供的搜索和现在的也不一样。一个想法能否成功也是看时机的，比如比尔·盖茨在 90 年代就看好的智能家庭，现在还不见雏形。再比如苹果 90 年代要做的 PDA (掌上电脑)，几乎要了苹果的命。现在的互联网老大的业务也不是第一个做起来的。看看共享单车兴起的时候，有多少人参与希望能分一杯羹，结果呢，曾经风光无限的小黄车都黄了。 硅谷是成王败寇的地方，也是嗜血的地方。在硅谷工作并不轻松，996 根本不算啥。对叛逆和失败的宽容。多元文化。 作者说的这些点个人以为都是表象，最根本的原因是信息行业本身。信息时代一家公司成功将会迅速占据全球市场，在这以往是不可想象的。半导体技术发展太快，使得公司不得不最大程度上激励员工，同时公司前所未有的成功又吸引了更多人加入竞争。每一个新的变革带来的影响都是深远巨大的，在变革中落伍，巨头也会迅速陨落。所以，每个公司对会对创新特别敏感，对多元文化有利的创新更是为其包容。投资者对失败宽容是因为其中一个的成功带来的收益足以弥补之前的损失。 按照我的理解，世界各地所有的信息产业区都会有硅谷的这些特点，而各个地方可能有略有不同，国内缺乏创新是因为在很长一段时间我们有大量技术可以复制。当然有些地区无力与硅谷竞争，只好在一些细分的领域或者市场上竞争，这样的公司不大会成为巨头，也就不具备上面所说硅谷企业的特点。 14. 失之交臂的公司Sun工作站战胜小型机和工作站时候，不曾想面对 PC 它就是小型机。从 Sun 的失败中，施密特总结出了反摩尔定律。 Java 一直未能找到盈利点，跟微软合作也不看看微软的历史，可以说根本没看清微软这个对手。 Novel局域网领域的微软，没听说过。 网景网景的失败可以说是必然，之前也是这么以为的。佩奇的反击手段：改变商业模式（之前都是卖软件，现在浏览器都是免费的）；与 PC 厂商联手预装；打造自己的门户网站。 RealNetworksRealPlayer 又一个微软垄断下的牺牲品 15. 风险投资现在可能大家多多少少对这个并不陌生，不过可能了解的并不全面。两年前我在找工作的时候，才发现一家风投公司竟然会为初创公司招聘技术人员，More than just money。红杉资本，凯鹏华盈这些知名风投公司都是随着硅谷的兴起而创立。个人认为关注创投信息非常有必要，36 氪 就会推送初创公司的融资信息，从中可以窥探投资风向。 16. 信息产业的规律性 70-20-10 律 赢家通吃 诺维格定律 当市场占有率超过 50% 的时候，就必须发掘新的增长点。 基因决定定律基因：企业文化，做事方式，商业模式，市场定位虽说有一定道理，但个人比较不认可这一种说法，我并不认为阿里做不好社交，腾讯做不好电商。人不是关键，只要钱到位，人力和技术都不是问题。关键是对手太强大而且占领了大部分市场，完全追赶不上来，时机太过重要。想一下，最开始只有淘宝一家的时候，即使不怎么好用还是能够迅速扩张市场的，后来的腾讯如果做的不够好用根本没机会改进。即使腾讯做到市场第二的位置，付出的代价也可能是巨大的。 17. 斯坦福大学简·斯坦福真正的慈善 国内高效差距太大 纽曼+洪堡洪堡教育体系，技能教育与职业教育是大学的中心任务，同时强调研究对大学的重要性。可惜中国没有学到家，忽视了研究生教育。在作者描述的体系中，没有看到对健全人格的培养，或许受篇幅所限，抑或这里只是突出斯坦福。 18. Google作者对老东家评价极高，也对 Google 非常了解。佩奇的格局确实非凡。最后，Google 在云计算上落伍真是让人不甘心。 19. 投资银行辟谣·罗斯柴尔德·神秘家族金融公司分类：商业银行，投资银行，共同基金，对冲基金，私募和风投，资产管理公司。 投资公司：高盛：炒作确实可恨；经常看到报道银行与政府达成和解，支付巨额罚金，该！摩根士丹利：国内称之为大摩，J P摩根或者摩根大国内称之为小摩 这些投资公司可谓是呼风唤雨啊！ 20. Facebook帕克的出局真不好说是谁主使的，但是扎克伯格至少是知情的。对比下国内的校内或者人人网，其实还是有点难以想象 Facebook 成功的这么顺利。 21. 成功的转基因诺基亚其实诺基亚做智能手机的时间也不晚，塞班与安卓竞争个人认为不是关键。即使诺基亚不做塞班，直接使用安卓也无济于事。除非在安卓之前占据大部分市场，但在当时是不可能的。生产手机就像生产 PC 一样没有技术优势，诺基亚在当时的时间点与其它厂商相比是在同一起跑线上的。与亚洲制造相比，是没有竞争力的，遗憾之处在于它没有意识到操作系统的重要性。3M第一次了解这家公司是一篇文章讲到它的创新模式，3M 的创新能力确实很强。来澳洲之后，才发现不少公司每年都会分配一段时间（一周）来支持员工做自己想做的事情，后来才了解这些举措也是受政府鼓励的（政府会给予税收上的减免）。这在国内是闻所未闻的。最后，百年老店不一定要追逐技术浪潮，像巴菲特投资的可口可乐等公司，消费品行业的利润也可以是持久稳定的。GE杰克·韦尔奇 22. 生产关系的变革股权和期权制度期权的作用是合理分配未来财富。我一次在想拆迁和长远发展的时候，也想到了期权。按我的构想是这样子的，比如要拆迁了，不会直接赔偿高额的现金，而是会以期权的形式分年份发放。比如说将未来十年该地区的税收增值的 10% 作为基金，基金的收益作为补偿。 工程师文化 扁平式管理这个在国内等于我们公司很小。本质上来说，国内根本没做到，只是学了学样子。上级也没有放权到下级，等级制度依然很难消除，也不会完全信任员工。私以为文化原因，国内不可能做到。 轻资产公司 23. 印钞机：商业模式这个绝对值得强调，之前的章节也都有提及。 AT &amp; T 19 世纪末只收服务费而不收安装费。中国电信 2000 年都不懂这个道理。 微软甲骨文就是卖软件，之前软件是与硬件捆绑，出售硬件盈利的。IBM 将服务费与软件费一并收取，甲骨文则是把服务费分开所以更容易抢占市场。Google 的广告系统，eBay 的电子商务腾讯的虚拟商品 可以说商业模式上的创新甚至比技术的创新更重要。想一下如果网景浏览器是免费的，自己做网站门户。 24. 互联网 2.0这儿说的这些产品是抖音这样的公司：内容来源于用户；开放的平台；交互性；非竞争性和自足性 博客；Youtube；微信（公众号，小程序）Twitter 还有小红书，Pinterest 25. 金融风暴的冲击从当前的格局来看，欧洲自顾不暇，美国依然强大，中国在崛起。个人认为，中国与西方依然有很大的距离 26. 云计算软件即服务 27. 汽车革命电动汽车在加速上与内燃机汽车本来就有比较优势，适合制造跑车。特斯拉的切入点非常准确。 这儿谈谈对新技术的看法，再次把钱伯斯的话搬过来。 在任何时候，都不要对技术情有独钟，市场永远是正确的。— 钱伯斯 先谈特斯拉，书里也提到了特斯拉的优势（天时地利，电池技术，辅助驾驶），认为在三五年内这些优势会消失。这儿谈谈我的看法。本质上，特斯拉还是制造汽车，至于电池还是内燃机驱动，个人认为这不是关键点，很多人对它高估了，认为它是下一个 Apple。想一下，单从汽车这一交通工具来说，它对整个社会生产力发展没有任何推动，所以不会带来革命性的变革。相比智能手机而言，它能带来的变革微乎其微。大家看好特斯拉，是看好创造奇迹的马斯克，认为它可能会吞占整个汽车市场，那可是要了传统汽车厂商的命，现在传统厂商在电动汽车上面的投入非常大，竞争会非常激烈。至于辅助驾驶，特斯拉排在第二第三梯队呢，特斯拉成为领头羊的可能性也不是没有。就目前来看，马斯克在国内被封神，特斯拉是被高估的。未来有很大的不确定性。 不得不说过去几年炒了太多概念，抛开概念来看，究竟这些技术解决了什么问题，给用户带来了什么价值，是不是旧技术解决不了的问题，最关键的是商业模式。你会发现很多东西根本经不起这几个问题的拷问。软件即服务，几乎所有的企业都在喊，有些甚至没有想过为什么是不是自己也适用。纯属吐槽某些公司修改定价模式。之前的一大波 O2O 创业潮解决了一点不痛不痒的问题，根本赚不到钱。区块链，说实话，绝大部分区块链公司做的事情不用区块链也完全可以实现，比如物流跟踪。很多公司都是自己在玩，而不是多方参与，单节点也好意思叫链。 28. 工业革命和颠覆式创新的范式小米 29. 信息时代的科学基础个人以为这一章干货很多，解释了背后的理论依据。 从机械论到三论（控制论，信息论以及系统论）工业时代，泰勒管理学理论：效率优先；同构的树状组织架构；可预测性；人性化管理 方法论的革命控制论思维不断调整变化现在的敏捷开发自不用说。特斯拉汽车的制造也利用了互联网的思维，这个很多文章都有提及。 信息论思维信息就像工业时代的资本 系统论整体性能未必能通过局部性能的优化实现。 信息时代思维指南预测 vs 反应拥有 vs 连接局部 vs 整体 企业制度背后的科学原理宽容失败从一方面说没有人能够预测准确，错误总是不可避免。期权期权制度的本质是从存量分配变为增量分配扁平式管理增加带宽，是沟通变得顺畅，合作变得容易。分权减少不必要的通信权威失灵信息量太大，不确定性太多，无法预测 30. 下一个 Google未来新产业：新能源：可能性不大生物和制药技术 看来有钱人最怕死大数据医疗和 IT 医疗绿色农业：不看好，不过 Beyond Meat 真是一个奇迹。电子商务：格局已定移动互联网和 IoT：这个 90 年代盖茨就要做的不知道近十年有没有突破。潜力倒是很看好，不知道市场会怎么变化。个人还是很看好小米模式的。O2O：新汽车行业：人工智能 整体看下来，生物制药与医疗这个行业不管怎么都是高利润行业。最有可能参与的可能是大数据和人工智能，而能够参与这个的恐怕都是些拥有海量数据的公司，还有一个就是政府。国内政府与阿里云合作，在建设的城市大脑等项目，希望真的是利国利民。","link":"/2020/04/10/journeyman/%E3%80%8A%E6%B5%AA%E6%BD%AE%E4%B9%8B%E5%B7%85%E3%80%8B%E7%AC%94%E8%AE%B0/"},{"title":"作用域与闭包 - 最简解释器实现","text":"如果你对 JS 闭包的实现有兴趣，就跟着这篇文章一起实现最简单的解释器，相信你会对闭包有新的认识。 开始之前如果是从头开始实现一个解释器，避免不了从词法分析和语法分析开始。我们对这部分工作没有兴趣，所以直接使用现有的分析器 acorn 。acorn 是一款比较流行的分析器，babel 底层依赖的就是 acorn。 另外，esprima 也是一个不错的选择。这儿我们选择 acorn，还有一个经常会用到的在线工具 astexplorer。acorn 解析得到的抽象语法树符合 estree 的规范，项目中也依赖了 @types/estree。 在此之前，希望你大概明白 AST 抽象语法树是怎么回事，知道如何遍历一棵树。 这儿选择 Typescript，在开发中也随时可以查看特定节点的数据类型，代码也比较容易阅读。 测试使用 Jest，运行起来比较简单。即使并不追求 TDD，测试也可以帮助我们持续迭代。建议配置在 IDE 中运行和调试测试。 算术表达式解析我们从最简单的算术表达式开始，这节的目的就是对算术表达式进行求值：给定任意的算术表达式，形如 1 + 2 * 3 - 4 / 5 我们能够正确得到结果。 AST 抽象语法树在开始之前，我们需要先打开 astexplorer，对 AST 有一个感性的认识。当左侧的表达式为 1 + 2 时，右侧的得到的 AST 是这样子的 （这儿以 JSON 形式表示）： 1234567891011121314151617181920{ &quot;type&quot;: &quot;Program&quot;, &quot;body&quot;: [ { &quot;type&quot;: &quot;ExpressionStatement&quot;, &quot;expression&quot;: { &quot;type&quot;: &quot;BinaryExpression&quot;, &quot;left&quot;: { &quot;type&quot;: &quot;Literal&quot;, &quot;value&quot;: 1 }, &quot;operator&quot;: &quot;+&quot;, &quot;right&quot;: { &quot;type&quot;: &quot;Literal&quot;, &quot;value&quot;: 2 } } } ]} 这儿移除了标注代码位置的数据，以及字面量的 raw 属性，其中 raw 表示原始字符串。如果将右操作数改为 16 进制形式 1 + 0x2，我们就会发现 raw 为 '0x2'。这棵语法树根节点表示一段程序，其 body 属性是一个数组表示可能有很多条表达式语句。对于二元表达式来说，有左右两个节点。这儿的左右节点是字面量。我们改动一下左侧内容为 1 + 2 * 3 我们看到，右侧的节点成为了一个二元表达式，而这个新的二元表达式的两个左右节点分别为字面量 2 和 字面量 3。 代码实现我们给自己的解释器起一个名字，随便起一个就叫 Z2（V8）吧。我们可以先写一个简单的测试： 1234it('should return sum when adding two numbers', () =&gt; { const z2 = new Z2(); expect(z2.run('3 + 4')).toBe(7);}); 我们借助 acorn 得到 AST，之后遍历这棵语法树得到执行结果就可以了。对每个节点类型，我们在一个单独的函数中处理。整个的文件结构： 12345678910111213141516171819202122import { Parser } from 'acorn';class Z2 { run(code: string) { const program: Node = Parser.parse(code); return this.evaluate(program); } evaluate(node: ESTree.Node) { switch (node.type) { case 'Program': return this.evaluateProgram(node); case 'ExpressionStatement': return this.evaluateExpressionStatement(node); case 'BinaryExpression': return this.evaluateBinaryExpression(node); case 'Literal': return this.evaluateLiteral(node); default: throw new Error(`Unknown node type: ${node.type}`); } }} 现在我们只需要实现单独的各个节点处理函数就可以了。各个节点只关注当前的节点操作就可以了，对 evaluateBinaryExpression 来说，我们需要递归的调用 this.evaluate 以对子节点进行求值。 1234567891011121314151617evaluateBinaryExpression(node: ESTree.BinaryExpression) { const { operator } = node; const left = this.evaluate(node.left); const right = this.evaluate(node.right); switch (operator) { case '+': return left + right; case '-': return left - right; case '*': return left * right; case '/': return left / right; default: throw new Error(`Unknown operator: ${node.type}`); }} 而字面量的处理就更简单了 123evaluateLiteral(node: ESTree.Literal) { return node.value;} 现在可以可以测试一下，对于混合的算术运算，Z2 依然返回正确的结果。 123456it('should return correct value given mixed math expression', () =&gt; { const z2 = new Z2(); expect(z2.run('3 + 4 * 2')).toBe(11); expect(z2.run('8 + 12 / 3 + 4 * 2')).toBe(20); expect(z2.run('1 + 2 * 3 - 4 / 5')).toBe(6.2);}); 当然即使有括号改变优先级，一样不会影响程序的正确性，因为 acorn 已经给我们返回了正确的语法树。 1234it('should return correct value given expression include parentheses', () =&gt; { const z2 = new Z2(); expect(z2.run('(3 + 4) * 2')).toBe(14);}); 变量和环境这次我们尝试引入变量，我们的代码也很简单。我们可以添加一条新的测试用例： 1234567it('should calculate correct value given expression with variable', () =&gt; { const z2 = new Z2(); expect(z2.run(` var a = 3; a * 9; `)).toBe(27);}); 我们在 astexplorer 查看一下这两个代码的语法树：第一部分是变量声明；第二部分的表达式没有太大变化，只是左侧的操作数类型是标识符 { type: 'Identifier', name: 'a' }，右侧依然是字面量。也就是说在对表达式求值时，我们知道的只有标识符的名称 a，所以，我们需要从一个地方获取到标识符的值，简单的理解为变量环境 Environment。 现在这个变量环境只是一个符号表，即便如此，我们还是创建一个单独的类吧。 1234567891011class Environment { private symbolTable = {}; set(name, value) { this.symbolTable[name] = value; } get(name: string) { return this.symbolTable[name]; }} 在 Z2 run() 中每次运行代码之前，都先创建一个新的环境 this.env = new Environment();。接下来的工作就是添加对变量声明和标识符的处理函数， 1234567evaluateVariableDeclaration(node) { node.declarations.forEach((declaration) =&gt; { if (declaration.id.type === 'Identifier') { this.env.set(declaration.id.name, this.evaluate(declaration.init)); } });} 变量声明是一个数组因为我们可以同时声明多个变量，这儿我们只处理左侧类型是标识符的变量声明。暂时不考虑形如 const { name, id } = user; 复杂的声明。我们做的也很简单，只是将变量值存放在环境中。 至于标识符的处理就更简单了，只需读取变量的值。 123evaluateIdentifier(node: ESTree.Identifier) { return this.env.get(node.name);} 同样的方式，我们添加对赋值语句的支持，同时添加一些测试用例。 函数和闭包接下来我们要引入函数和闭包。我们先来考察一下最最简单的函数声明和调用： 123456var a = 3;function outer() { var a = 5; return a;}outer(); 相比较于变量声明，函数声明包含的属性有点多，不过我们先只关注最基本的情况，其中最重要的是参数 params 和函数体 body，暂时不考虑 params，在解释函数声明时，同样的我们需要把函数存放到上下文中，简单起见，我们只是把整个节点存放进去。对于函数调用，这是一个 CallExpression 我们需要做的就是根据标识符名称找到我们存放的函数对象（现在只是一个节点），然后解释执行函数。在上面的例子中，两个同名变量是不同的，函数内部是一个单独的环境。在进入函数之前，我们需要创建一个新的环境，将当前环境设置为新环境，在函数调用结束之后，我们需要恢复原来的环境。 实现代码如下，同时我们还要添加对 ReturnStatement 和 BlockStatement 的处理函数。 123456789101112evaluateFunctionDeclaration(node: ESTree.FunctionDeclaration) { this.env.init(node.id.name, node);}evaluateCallExpression(node: ESTree.CallExpression) { const fnNode = this.env.get(node.callee.name); const env = new Environment(); const currentEnv = this.env; // 保存调用者环境 this.env = env; const result = this.evaluate(fnNode.body); this.env = currentEnv; // 恢复调用者环境 return result;} 闭包的处理我们希望函数内部可以访问函数外部的变量，我们添加一个测试用例： 12345678910it('should be able to access outer variable in function', () =&gt; { const z2 = new Z2(); expect(z2.run(` var a = 3; function outer() { return a; } outer(); `)).toBe(3);}); 在上面的实现中，我们创建的 Environment 是一个全新的。为了访问到外部的环境变量，我们做需要一点点的变通：把当前的环境作为外部环境引用传进去。const env = new Environment(this.env);我们的思路是：在查找标识符的时候，也就是 Environment.get(name) 函数实现中，先在当前的环境查，如果查找不到，我们就在外部查。修改后的 Environment 类： 1234567891011121314151617181920212223class Environment { private outer: Environment; private symbolTable = {}; constructor(outer: Environment = null) { this.outer = outer; } set(name, value) { this.symbolTable[name] = value; } get(name) { if (this.symbolTable[name] !== undefined) { return this.symbolTable[name]; } if (this.outer) { return this.outer.get(name); } return undefined; }} 修改代码通过上面的测试用例。 现在我们思考一下在我们创建一个新的变量环境时，是否应该传入当前的变量环境。的确，我们这样做通过了测试。但是，我们得思考一下，这意味着什么？这意味着，一个函数中的变量依赖于调用者环境。而一个函数可能在几十个地方被调用，而我们根本无法得知函数里面的外部环境变量究竟是在哪儿定义的。 我们来看看实际当中我们希望的外部环境是什么？现在我们添加一个闭包的测试用例，当然这个测试现在不会通过。 1234567891011var b = 5;function outer() { var b = 10; function inner() { var a = 20; return a + b; } return inner;}var fn = outer();fn(); 我们对闭包并不陌生，上面这个的这个例子中，fn 只是 inner 的引用，在我们调用 fn 时，我们希望外部环境是 inner 所在的环境，也就是说我们希望将函数声明时的环境作为外部环境。修改代码，在函数声明时保存整个函数和当时的变量环境。 123456789evaluateFunctionDeclaration(node: ESTree.FunctionDeclaration) { this.env.init(node.id.name, this.createFunction(node));}createFunction(node) { return { parentEnv: this.env, node, };} 在函数调用时将其作为外部环境 12const fn = this.env.get(node.callee.name);const env = new Environment(fn.parentEnv); 修改代码通过上面的测试用例。现在我们再来看闭包的话，我们发现所谓的闭包就是函数和它所在的环境。完整的代码见 https://github.com/fedeoo/learning-ecma262/tree/master/src 总结闭包已经被大家讲烂了，个人觉得理解一个东西最好的办法就是实现一遍。简单的解释器非常容易实现（如果不需要做语法分析），建议大家读完之后自己实现一遍，也就 100 多行代码。这儿并没有遵照 ECMAScript 规范来实现，因为要引入不少概念，比如运行时返回的是 CompletionRecord 类型，解析标识符得到的是 Reference 类型。如果想严格的按照规范实现或者学习规范，可以参考项目 engine262。针对想从头开始写解释器的同学，建议阅读 esprima 源码。除非你是在学编译原理，跟人觉得没有必要从头开始，事实上已经有不错的工具，根据产生式来直接生成语法树了。","link":"/2020/06/07/journeyman/%E4%BD%9C%E7%94%A8%E5%9F%9F%E4%B8%8E%E9%97%AD%E5%8C%85-%E6%9C%80%E7%AE%80%E8%A7%A3%E9%87%8A%E5%99%A8%E5%AE%9E%E7%8E%B0/"},{"title":"对当前前端技术栈的理解","text":"最近被问到对一些对技术的理解，虽说很多时候自己感觉对技术选择有自己的理解，知道背后的原因，但还是不大能说的上来。这儿就总结一下对所经历的一些技术的理解吧。谈到理解，总免不了介绍技术出现的原因，解决的问题，可能的方案以及未来的可能的形态。比如，不少人认为前端玩的一些概念是在后端存在很久的东西，这个现象也确实存在。虽然 web 技术诞生有一定的年头了，不过就前端而言，可能要在 2005 年之后 Ajax 出现之后以及 SPA 成为趋势之后，才算是真正的开始发展。JS 和 CSS 存在的问题也才逐渐的突出起来。 前端工程化关于工程化这个问题，个人以为张云龙的一系列博客已经把这个问题讲的很清楚了。工程化可以说是一个很大很笼统的话题，说实在的，我没有找到相关的英文文章，好像这个是国内才有的名词一样。个人认为这个概念本身也很模糊，有人按模块化、组件化、规范化、自动化来进行划分。模块化，组件化估计都不再是一个问题了，自动化又可以用 CI/CD 来取代。不过，这儿也分别谈谈对这些内容的理解。 模块化模块化意味着分而治之，也是计算机解决复杂问题所惯用的方法。现在估计很难找到一门语言没有自己的模块化系统，一门语言不仅要有自己的模块系统，还要有包管理系统，这都是生态的基础。但是 JS 很长时间根本没有这些东西，在 Ajax 出现之前大家也没有使用 JS 构建复杂的交互应用，问题并不突出。分散在各个文件的变量在没有命名空间的情况下很容易产生冲突，管理各个文件之间的依赖也是一个问题。 个人认为模块系统出现是早晚的事情，当然也有人认为是从 Node.js 出来的 CommonJS 推动了模块系统。ECMAScript 标准迟迟未确定，CMD 与 AMD 出现并应用在很多的应用当中。现在 CMD 与 AMD 都已经成了历史，很少会见到有人问它们之间的区别了。 说完 JS，还要谈下 CSS。有人将 CSS 理解为一份配置，这份配置还不支持变量，代码复用，逻辑控制。不过，还好 sass 和 less 的出现弥补了这些缺陷。CSS 也没有作用范围的概念，一份样式会影响到整个应用。不过各种命名规范， css modules 以及 style in JS 也算是一种解决方案。或许 web components 提出的方案是最为完美的，不过它并没有成功。 在我看来 CSS 有点特别：CSS 的目的是将内容与表现样式分离，很奇怪的是为什么只有前端存在这个东西。如果我们用 flutter 或者 swift UI 开发与 H5 应用类似交互的应用，我们可以做到非常的相似，不过这些技术都没有类似 CSS 的东西。从这点说，CSS 不是一个必要的存在，很大程度上是一个历史的产物。至于 CSS 会不会消亡，或者被 style in JS 取代，我是持否定态度的。自身对 style in JS 的感受是：认同这个观点但在实际应用中并没有带来太大的收益，甚至有时把问题搞复杂了；对于绝大多数以展示为目的的网站来说，CSS 依然是一个最佳的方案。 组件化组件化也是分治思想的体现。组件化的思想不止是在前端，像 《架构整洁之道》 有些历史的书中的花了很大的篇幅在讲设计原则和组件构建原则；即使里面讨论的组件和前端的组件并不一样，不过里面的设计思想是可以借鉴的。与模块化相比，组件是一个可复用的功能单元，包括 JS，CSS 以及其他资源。在 React 流行的今天，估计大家对组件这个概念已经司空见惯了。在 React 之前，大家觉得 web components 会是未来的方向，当时的polymer 就像现在的 React 一样香。 webpack 相比与 gulp，一个特点就是它提供的管理资源方式是以组件为单位的或者说我们是以组件的视角来管理资源的，而使用 gulp 我们是按文件类型来管理资源的。这也是随着 React 的流行 webpack 取代 gulp 主宰市场的一个原因。 在 React 之前，大家也会封装组件，但是没有将所有的东西当成组件对待。就目前来看，这种思想俨然已经成为主流，使用flutter 以及 swift UI构建应用时也是一样的思想：将大的页面或者组件拆分为小的组件并把它们组装起来。 在我最开始接触 React 的时候，想找一些最佳实践，找到官网上的 Thinking in react 翻译成中文或许是 React 编程思想，与 Java 编程思想（Thinking in Java）相近，不过这个只是很短的一篇文章。整个文章也只是在教我们如何把大的组件做拆分。所以，在我看来，React 开发就是设计可维护和可复用的组件。到底怎样设计组件或者我们应该遵循哪些原则，个人认为 SOLID 以及 组件构建原则（组件构建原则可以说是 SOLID 的延伸）。 规范化规范化比较容易理解。如果我们有多个前端项目，我们希望他们的组织结构，编码规范都尽可能一致。就编码规范来说，一般来说用工具就可以维护 ESLint CSSLint StyleLint。另外，可能有设计规范，组件交互的设计语义可能适用于不同的场景。 自动化自动化简单来说就是用工具解决人工重复劳动。比如自动构建和部署，测试。 构建我们需要理解我们为什么需要构建工具以及我们构建的目的。一个根本原因是本地开发和线上需求不一致。我们的构建需求有很多： 编译 JS 和 SCSS 以支持不同浏览器。 压缩资源以更好优化资源加载速度 文件指纹以充分利用浏览器缓存 生成 HTML 文件 本地开发 HRM Soucemap 即使没有 webpack 一定会有其它的工具解决上面的问题。记得最开始接触前端时使用的是 java 工具来压缩文件。Node.js 的出现可以说将这部分内容交给前端自己解决了。从 grunt 到 gulp 再到现在的 webpack，工具逐渐成熟，但是解决的根本问题是存在重合的。 当我们只有一个项目时，我们会考虑用 webpack 或者 create-react-app 作为构建工具。非常自然地，如果我们有多个项目，我们不希望维护多份 webpack 的配置，如果 create-react-app 不能满足我们的需求，我们可能会基于 webpack 封装自己的构建工具。这也是 roadhog 之类工具二次封装的目的。就业务需求而言，我们对 webpack 的配置细节不感兴趣，可能我们的配置只是想告诉工具：我们使用 scss + css modules。二次封装可以屏蔽掉这些细节，让我们不再需要感知到 webpack 。我一直希望有一个统一的配置规范，大概类似与 chart 图表数据格式吧。这样子，不管是 roadhog 也好，或者其它的二次封装工具，都可以很好的兼容。 自动部署前端的部署方式比较简单，将打包的资源发到 CDN 即可。部署方式每个公司流程不同，我们现在使用 bitbucket pipeline 做的持续集成。在每次代码提交或者分支变动都会触发对应的任务，同时也有定时任务，每周指定时间重新部署开发环境。至于发布方式，我们使用 terraform 作为编排工具，将构建的资源同步到 S3，同时设置文件的缓存策略。同样用 terraform 管理 CDN 的配置，将我们域名的请求映射到 S3 上。 测试可以说之所以我们能够做到自动部署，对每次发布有信心，是因为我们有足够的测试确保我们的系统稳定。关于测试我已经在一篇文章里面谈过了 谈谈测试与代码质量 前端框架首先要指出的是：引入一个框架是有代价的，必须慎重考虑，我们最不希望的就是我们的项目与一个具体的框架耦合。想象一下，项目从 Angular 迁移到 React，或者从 Vue 迁移到 React，有多少代码是可以复用的。同时，引入框架也意味着牺牲了灵活性。在一些架构设计指导中，最后的最后才会对框架进行选择，而这在前端却恰恰相反，往往最最开始做的决策就是确定选用哪一个流行框架。前端之所以不同，一个原因是因为前端中的框架与后端中的语言一样是最基础的东西，现在使用原生 JS 开发直接应用根本不现实。最少也需要 jQuery 这样的库来解决兼容问题，但是 XSS 安全问题又防不胜防。另外一个主要原因是，前端的特征决定的，前端在整个系统设计中是最上层的，也是最不稳定的，我们需要使用框架迅速构建原型。个人认为，这个也是前端难题之一：我们认为的理想项目设计可能并满足客户需求，我们必须妥协和权衡。 React 不是一个完整的框架，与 Angular 这种大而全的框架比，React 更像是一个 UI 库。自然会有一些前端团队封装自己的框架，比如 Dva.js。记得当初团队也想做类似的事情，之后离开团队也不知道了。整体来说，大家已经对 Redux 流程已经比较熟悉，也并不觉得 React-Router 需要再进行封装。 异步操作使用 redux-saga 的感受是，这个确实比 redux-thunk 强大，但是绝大部分情况 redux-thunk 已经够用了，而且代码更为简洁，再说 redux-saga 的测试写起来一点也不愉快。 最后最不愿意看到的就是将 dispatch 注入到组件中，dispatch 可以做任何事情，本质上就是破坏了组件的封装。主观上的原因是我们不希望依赖太多的框架，即使做了选择，替换的成本也不要太高。 个人对 Angular 还是颇有好感的，大而全的框架虽说上手有难度，但是开发起来效率真的很高。绝大多数应用还谈不上性能瓶颈问题，很多项目只是本身的设计问题。就我经手的项目而言，有些项目也是用最新技术栈，前端开发也不是新手，但是项目代码只能说不敢恭维。 MVVM 的出现让表现和数据分离，前端只需要关心数据变化，而不用关系操作 DOM。这种分离让业务逻辑更加容易复用。Vue 最初更像是简易版的 Angular，当初调研的时候很难想象现在会这么成功。 依赖收集 Vue（Mobx）与 Redux 相比较的话，更像是比较不同的编程范式，函数式或者面向对象。在土澳函数式编程比国内更为流行，以至于前端团队有些人主张用 Reason 作为开发语言。个人感觉 redux 更加无脑一些（虽然有点啰嗦），Mobx 所需要的抽象设计能力要求更高，不大相信一般的团队能够管理好。就性能而言的话，Redux 比较简单：有人把它比喻成一棵大树，所有的变动都要经过树干。这是它的缺点，任何的变动都可能导致整个应用的重绘。connect 会保证如果没有数据变化不去触发 render，但是这个粒度是 container 的粒度。当数据发生变动的时候，依赖收集则可以精确地调用观察该数据的函数。当然函数一定是包装后的函数。依赖收集是有代价的，所以初次渲染的性能肯定不如 Redux。后续数据变化时，虽说会重新收集依赖，但是成本低了很多，仅有观察的函数会触发更新。一般来说更新时，性能要比 Redux 好些。单单进行这样的比较意义不大，如果真的存在瓶颈，每个都能够进行优化。根据具体场景，比如渲染列表中的元素项经常发生变动，那我们可以控制依赖收集的粒度，或者在 redux 中将 container 划分的更细一些以减少不必要的 render。 GraphQL微服务的出现和流行，对前后端合作有着很大的影响。微服务更加倾向于提供通用的 API，前端一个组件所依赖的数据可能对应到几个不同的微服务上。前端与微服务端就需要一个网关服务，网关整合各个微服务为前端提供更为友好的 API，可以简单理解为 Facade 模式。有时也成为 BFF。前端作为消费者，要比后端更为关心这些接口，也更适合维护 BFF。就个人经历而言，还是有不少团队将这部分工作交给后端来做，个人认为可能的原因是：网关服务与前端开发差异较大，即使使用 Node.js 也可以说是完全不同的东西，而网关服务对后端同学而言只是一个普通的服务而已，对前端同学来说可能公司的基础设施根本就不支持 Node.js 服务。 刚说到 BFF 由前端维护更为合适，但是 GraphQL 的出现可能改变了这一点。如果后端提供的微服务本身是支持 GraphQL 同时又提供了一个网关来整合，就没有理由再交给前端来维护了。 对于 open API 来说，经常会根据 query 参数返回不同的数据字段，或许就是 GraphQL 的雏形，GraphQL 是一种更加优雅的方案。单单将 GraphQL 与 REST 相比较，各有优劣，GraphQL 并不会取代 REST。 GraphQL 的优点 query 非常优雅简洁，返回结果与 query 相近 按需组合查询，节省网络带宽 提供 schema 保障前后端的规范。类型校验 强大的 subscription 缺点也很明显： 完全不同的 cache 机制，不能有效利用 HTTP 缓存 N+1 query 引起性能瓶颈 query 组合的粗细粒度，不能第一时间呈现给用户信息。 或许 GraphQL 更适合需要适配多端不同 query 的情况吧。 微前端个人并不看好微前端，主要是觉得其实这个新概念并没有解决什么实际的问题。最经典的例子是几个不同技术栈的整合，现实中很少见到这样的场景。比较常见的是，整个项目太大了，划分为不同的产品，交给不同的团队，每个团队都希望高度自治独立部署发布。简单来说我觉得微前端的问题是很难找到一个全局最优解。如果是单一的应用，可以减少相同代码的加载，可以做更好的拆分动态加载来达到更好的性能。最重要的是，团队组织的问题反映到产品上，很难提供给用户一个顺滑的体验。每个团队要求的自治和独立发布通过更好的组件化也能够实现，微前端不是唯一的方案。如果功能划分的功能之间没有联系，那设计成独立的子应用，会更简单。 微前端需要解决的问题：主应用加载器，路由管理，消息通信。 目前公司的技术选择Graphql公司成立的有些年头了，不少页面还是 asp 弹窗的古老形式，交互并不友好。近几年，后端迁移切换成微服务，网关服务也是后端在做的。前端新项目也是最新的 React + Redux 技术栈。在整合产品的时候，我们的 BFF 技术栈选择基于 GraphQL 和 Node.js。就我们的项目来说，在我看来没有特别的原因选择 GraphQL，我们并没有多端不同 query 适配的情况，还未用到 subscription。使用 GraphQL 更像是对新技术的一种投资。我们选择的是 Apollo + express，算是比较大众的一个方案，因为Apollo 比较容易入门，生态也是最强大的。 使用感受：前端代码更简洁一些。带来的问题是，附加的一些页面数据可能会影响到首次 query 性能，导致不能及时的呈现给用户关键信息。N + 1 query 问题并不是很常见，可以根据情况单独的处理。另外网关这层对微服务这层做了 cache。 因为是第一个应用，没有太多的约束。这里面也有一些问题： 第一个问题比较普遍，没有一个一个规范。 鉴权，日志，异常处理，配置加载都是自己处理的，不能为其它项目复用。 模块化的问题，Apollo 官方文档是按功能组织 GraphQL 的文件，复用是一个问题。Federation 太过激进。 所以，我考虑在 Apollo 之上引入企业框架，将这些可以复用的部分移出去。egg.js 不合适：egg.js 定义了一套规范，在此之上有不少可用的插件，在我们这儿没有什么需要的插件，定时任务这些也用不到。egg.js 内置对多核支持，对我们来说，性能也不是问题，长期 CPU 和内存利用率在个位数，如有需要也是选择 PM2。 模块化的需求是这样的场景：我们考虑到一些业务模块的复用，比如我的这个网关会有用户模块，新的网关服务也可能需要。我们希望能够直接复用这个模块，但是 Apollo 官网的方案 Federation 太过激进。 graph-module 也可以满足我们的需求。个人更倾向于 nest.js，有几个原因： 提供了 IoC 容器，可以更好的组织代码 与 GraphQL 整合的不错 内建的一些日志和异常处理功能可以直接使用 生态已经不错了，另外发展的势头也很好 支持 type-script 注意：依赖注入改变了控制流程，单说依赖注入是不与框架耦合的，只是在与具体容器整合的时候才有部分耦合的代码。更直白点说，如果我们换一个 IoC 容器，特定的注解和模块的注册是需要修改的，其它部分逻辑是不变的。 大公司与小公司的机会当前端团队有了一定的规模之后，自然而然的就会有技术团队和业务团队之分，这样的好处是统一技术规范避免重复建设。可以说，就是这样的分工造就大公司非常丰富的基础设施。一方面我们很羡慕技术团队有机会接触新技术，但是也知道技术团队并不好做。难做的是在大公司里面其实很多东西都已经很成熟了，再进行创新已经是非常难的事情。公司需要的是产出，而不仅仅是调研，业务团队就像是客户，必须切实的解决他们的痛点，他们才愿意接受这些技术。 大公司的一些需求小公司可能从未遇到过，所以也不太容易感受到大公司推出的技术解决的问题。通常大公司有很多的部门，每个部门的业务可能截然不同，每个团队可能有多个产品。所以，很自然地，基础架构团队可能会考虑支持不能部门的业务，所提供的工具有很强的定制能力。比如， 阿里的 egg.js，介绍的时候就是框架的框架，很难说它对应的是哪个产品，你可以基于 egg.js 定制一套适合你们部门的统一的基础框架（假如 begg.js）。然后你们部门的团队再选择基于 begg.js 开发。如果你有类似的需求，估计不需要解释也能明白。而大多数人还是将 egg.js 与其它框架直接比较，毕竟他们的需求就是如此。而 egg.js 官网又没有强调这种区别，直接为更广大的开发者开放像 begg 这样已经封装过的框架。另一个产品，Fusion UI 也是为了支持不同部门的不同产品，避免组件库的重复建设，估计更少公司会有这种需求。即使是构建工具，也可能每个部门也有不同的定制需求，记得有一个团队曾经就在考虑提供封装构建脚本的能力。 可以看到，大公司的技术团队有机会比较深入的探索新技术，也有能力追求的更加极致。相比较而言，小公司缺乏基础设施，机会也比较多，不过比较更多的是整合一些方案，毕竟也不适合投入太多。 未来的一些可能现在后端都在拥抱 serviceless 了，前端是时候考虑 spaless 了。像 codesandbox 这样，我们可以不关心构建，部署，监控，性能优化。甚至部署的服务可以根据收集的数据反馈，动态的进行优化。 从整体上想，类似的重复工作或许将来都可以在这样的平台上来完成了。","link":"/2020/07/07/journeyman/%E5%AF%B9%E7%8E%B0%E5%9C%A8%E6%8A%80%E6%9C%AF%E6%A0%88%E7%9A%84%E7%90%86%E8%A7%A3/"},{"title":"谈谈测试与代码质量","text":"平常开发中，你花多少时间写测试？覆盖率有多少？除了单元测试，其它的整合测试以及 UI 测试有实践么？ 以我个人经历来说，在国内工作时几乎不写任何测试，仅限于倒腾过测试。即使阿里这样的大公司，业务部门也几乎不写任何测试，基础研发部门或许很有节操，具体情况不得而知不好瞎猜。为什么不写测试，这跟整体的氛围有关，真有人在意你的代码质量吗？没有！大家在意的是业务结果。原来的负责人升职加薪之后项目交给后面的人维护，如果新需求不多，代码还过得去维护着就行。需求实在太多，维护不下去了，那只能重写了（自动化测试都没有，谈不上重构）。 国内开发节奏太快，着急于抢占市场，不断试错，计划赶不上变化，这周加的功能下周都可能就要废掉。这种情况下，写测试完全得不偿失，自然也就不会花时间在写测试上了。那如何保证代码质量呢？一般在上线截止日期前，集中时间手动测试，或许有些公司将这些任务外包给其它公司来做，毕竟大多数的开发可不屑于干这么枯燥无聊的活。 现在就职的公司在测试上做的可谓非常专业。我们的 QA 团队会支持各个业务团队，QA 是质量辅助而非质量保证。这儿有个文章介绍质量保证与质量辅助。QA 的职责更多的是监督，指导我们来完成测试任务，而非自己测试。他们会收集各种数据，建立指标来评估代码质量。另外，也会帮助新团队或新项目建立自动化测试，指导新人完成测试工作。 再说说我的发布模式，我们的发布周期是每日发布，也就是说当你的代码合并到 master 时必须保证它是正确的。因为第二天固定时间就会部署到线上，部署之后自动化测试通过就认为部署成功。除了单元测试，我们还需要用 UI 测试覆盖到所有的关键路径，也即冒烟测试。冒烟测试只会测试整体流程，允许存在 bug 但不是严重的，不影响正常的流程即可。 一个潜在的 bug 发现的越早所需的修复成本越低。单元测试并不能保证万无一失，模块与模块之间功能是否匹配还需要整合测试来保证。现在的问题是我们为什么而测试，于是，在开发前我们列出所有的需求点来，以保证我们会用适当的测试来覆盖这些需求点。 最后，在后端和前端之间，我们加入了合约测试以保证后端的 API 变动不会影响到前端代码。 现在的趋势是，开发将承担运维和测试的工作，而运维和测试只做些支撑性的工作。某些大厂强制测试转开发，就我之前见到后端写前端的经验来看，除非是内驱的并且公司给予足够的时间来转，否则只是添乱。口号喊的很好，全栈工程师，简单易上手，但是只要不让我接手维护他们的代码随便他们怎么折腾都行。 说说发布周期，每日发布就必须要求开发对代码有信心任何时候合并进去就要能够部署，避免开发将测试责任推卸。缺点是用户不能很好的利用缓存，用户重新加载新资源却并没有任何新的可用功能。将任务切分很细只会带来更多的工作量，而开发分支上本来就不是直接可交付给用户的。个人来看，除非必要的 bug 修复，完全可以等到 Sprint 结束一起发布。 关于测试，我并不是认为测试写多了，代码质量就会变好。测试只应该是辅助，不应该为测试而改动代码的可读性或使得代码变得复杂。真有一个极端的同事，几乎所有的代码都是依赖注入，一个功能函数的参数是一个接口。比如，fetchUser() 依赖的 request 只是一个接口，你需要一层一层往上查看，才知道这个是如何实现的。我知道这会让我的代码非常容易测试，但毫无疑问它让代码变得更复杂了。如果说好的架构师懂得如何权衡各种技术做出取舍，好的程序员一样也需要懂得取舍。测试的增加必然会拖累产品迭代的速度，有时我常会想，我现在增加的一个 UI 测试，真的以后这儿会做改动吗？或许那时交互的需求早就变了。我同样好奇这些测试到底帮助我们揪出了多少个潜在的重大 bug，是否值得我们的投入。 关于是否应该面向需求点检查覆盖率，我们也有不同的意见。个人不太认同，因为我们做单元测试很多时候是知道这个模块会被复用，其它人在使用这个模块时，如果需要改动，单元测试可以很好的保证其不被破坏。这个可复用的模块即使已经被整合测试覆盖到，也应该有对应的单元测试。整合测试并不能够帮助我们迅速定位问题，只是确保某一个功能是否完好。 对于测试，总的来说，它带来好处，但是也不是免费的。对可复用的模块，基本的单元测试是必需的，写单元测试也可以帮助你发现代码异味。关键路径的冒烟测试也是有必要的，至少确实会减少人工成本。业务逻辑代码因情况而定，非关键功能且你认为是一次开发即可的，你老板也不会让你投入这么多时间写测试的。如果你没有写过测试，开始尝试用单元测试覆盖你的可复用模块，尝试写些 E2E 测试减少重复劳动吧（E2E 测试远并没有你所想的复杂）。","link":"/2019/05/26/journeyman/talk-about-test/"},{"title":"读《见识》","text":"很惭愧的说，《见识》是我读的第一本吴军的书。《浪潮之巅》因为太畅销的缘故，一直未曾列入个人书单。书名《见识》，内容由《硅谷来信》专栏文章整理而来，所以话题很泛。就单一方便而言，不如名家随笔随想深入，但是非常贴合日常生活的缘故，观点非常中肯，非常有说服力。需要指出的是，即使所讲道理相同，不同的人在不同的阅历也会有不一样的感受，于我而言，受益匪浅。 序言中讲《命运》，提到他朋友说他去腾讯是一个不错的选择，原因是：这家公司有独一无二的价值。读到这儿时，触动很大，回想自己过去在做出选择时从来没有这样的眼界。我们常常说有时候选择比努力还重要，对一个人来说 10 年前选择阿里巴巴或者一家更高薪的小公司，现在看可是云泥之别。当然，我们并不能预料到今天阿里巴巴能够这么成功，如果一个人做错了选择我们也不能因此说此人没有眼光，因为不少聪明的专业投资人都错过了，何况我们呢？不可否认的是，就是这样的见识往往对我们的命运影响巨大。（想下自己目前的公司就不符合这一原则，甚是惭愧） 这本书首先谈幸福与成功，这样的安排很有心思。在《人生的智慧》中讲，人生的智慧就是最大程度的追求幸福。我们必须思考为什么活着，然后找到自己的答案。死亡是一个沉重的话题，但我们必须学会面对。非常朴素的道理非常难懂，对我而言至今依然没有顿悟。或许太多人想着逆袭，太多人愤愤不平，作者在《这个世界并没有欠你什么》给予回复，以正三观。婚姻和家庭在幸福中至关重要，不得不说在我们选择伴侣的时候太不成熟，而大多数人没有意识到人生是不断学习和提升自己心智的过程。还有一篇谈让父母成熟起来，我自己也承认在与父母沟通这方面做的很差，慢慢地才发现与父母之前的隔阂越来越大，以至于经常普通的问候都不想重复。人到中年，这些都是需要面对和解决的问题。 人生需要做减法。为何硅谷有这么多印度高管，作者给出的解释不错。我想补充的一点是中国的文化制度和在这种教育体制下培养出的人才性格也是一个很重要的原因。个人认为，若比较专业人才我们一点不差，但是成为高管需要的是软实力，这些恰恰是国内高材生最为欠缺的。国人选择太多，人太浮躁其实大家也知道因为我们喜欢攀比，聚会时经常打听谁谁换了新工作拿的薪资有多少。作者在《西瓜与芝麻》中讲的捡芝麻行为，作为一个珍惜自己时间的人很少这么做，时不时也会有这样的行为。对抢月饼一事，我也有不同的看法，说他们捡芝麻不无道理。对我而言必须时刻警惕，回想下过去工作中，有太多时间花在自认为一文不值的功能上。而现今所在公司想做一个平台，或许真的不如聚焦在一个产品上来的好，把自己所擅长的做到极致。之前读的一篇关于德国制造的文章，也讲到同样的事，德国聚集了非常多的行业第一的小企业，他们在经济危机中有更强的生命力。如果什么都想做，可能什么都做不成功。 玻璃心，输不起。记得之前读的一篇文章讲陈浩南为何能够东山再起，就说到愿赌服输，甘心受罚。说实话，一般人还真难做到这点，输了总是不甘心。无论贫富，父母要培养孩子：有见识，有爱心，守规矩。 不做伪工作者。衡量伪工作可以比较最终效果与投入。英文我们会强调 outcome 而非 result， 这儿是有差别的，可以将 outcome 理解为长远的最终效果。在刚开始工作时， 可以说我和普通的大学生一样对工作缺乏一个正确的认识，只是想着把交给自己的事做完就好了。在过去一年才读了一些关于管理的书，这一章的第一板斧让我想起了《Making Yourself Indispensable》，这本书确实非常实用。OKR 也是今年才在公司内部推行。对我来说是，知易行难，一直没有认真对待。关于职业的误区，可以说这些问题自己都有，过去一年改善很多，与领导的沟通交流，以公司视角去看问题都有所提高。有些经验教训都是花时间获得的，所以可以说如果职场新人在第一份工作中就有人传授这些经验，可以少走很多弯路。现在对我来说，选择一家公司，需要考虑的就是：公司或者产品是否独立无二；其次才是团队。 关于投资，年轻人最好的投资就是投资自己，这句话一点不假。同时我们自己有点闲钱的时候就会蠢蠢欲动，我自己也损失了不少钱在股市。但是年轻人迟早要载跟头，不如早一点犯错，早一点吸取教训。很多人说炒股犹如和自己的欲望博弈，很多人根本做不到理性。就像一个赌徒，连续赢了 10 次之后就觉得自己的技术高明一样，但是归根结底这只是一个零和游戏，最终依靠赌博发家的毕竟是少数。我们也能找到少数的幸运儿，但这和中彩票的概率是一样的，只是中了彩票之后我们会把它归于运气。成功时必须看到运气的成分，失败时从自己身上找原因。 说话与沟通，凡事莫争，就算争赢了也无济于事。所以说，讲话要达到目的。","link":"/2019/11/07/journeyman/%E8%A7%81%E8%AF%86/"},{"title":"《高效能人士的七个习惯》笔记","text":"老实说，在读这本书之前以为这本书会讲如何提高工作效率。之前很不屑励志或成功学类书目，因为往往这类的书都是在谈论一些空洞的原则，读过之后无所裨益。问题是，一个人的心智并不会随着生理的成熟而自然的成熟。对个人成长至关重要的东西从来不曾系统的出现在教育课程上，父母对这方面的教导可谓极其有限。 这本书第一章在抛出一些问题之后，就指出现代成功书籍与经典的差异，经典书籍像富兰克林自传非常强调个人品德修养，而现代书籍则重点强调技巧和个性。这是一个非常有意思的点。小时候听故事，浪漫主义文学，都会阐述好人有好报，美好的品性总会得到回报。对名人随笔，语录并没有多深的理解，但也不作怀疑。不及长大就识人性贪婪可憎，也不再将古人所言奉为金科玉律。这本书重新强调了品德的重要性，也是做一切事情的根本。比如我们知道应该多去赞美他人，但是怎样做的发自肺腑呢？这就要看个人的修养了，否则会有假装的嫌疑。本书强调自内而外反求诸己，我们需要认识到我们的观念影响到我们的认知，我们需要检讨自己的观念的和接受他人的观念，以此逐渐变得客观。文中讲他的一个经历，在车上遇到几个熊孩子而父亲不管不问，他感到不耐烦，就对那位父亲说，你是不是应该管管你的孩子。当他得知这位父亲刚刚失去自己的妻子时，观念立马改变了。这就是观念转换的力量。观念应以原则为中心，这些原则包括公正，诚实，正直，尊严，服务（贡献社会）以及卓越。这本书将习惯解释为知识，技巧以及欲望的结合，知识决定为什么做和做什么，技巧决定怎样做，而欲望是动机。 习惯一：积极主动我们的行为取决于自身的抉择而非外界环境。这其实很难做到，我们的心情总会受外界影响，天气，外人评论，老板，但是我们总是有选择的，我们可以选择积极应对。书中一个例子影响颇深，一个学生请假说自己不得不参加网球活动。于是他让学生权衡利弊自己做决定，并说自己也会选择网球队但是不会说自己是被迫的。大部分都很讨厌负能量的人，遇到问题只会抱怨不会想着改变。自己也在追求客观不受外界影响，有时发现真的很难说服别人做出一些改变，有些人整天抱怨，你劝说他做出一些尝试，而他又总会有各种理由。 习惯二：自始至终想象一下参加自己的葬礼，你希望得到怎样的评价，你希望成为怎样的一个人。类似的测试还有，你不久于人世，写下你想做的事情。想想就很沮丧，这个季度的个人 OKR 首先就是改善自己与亲人的关系。看到这个测试之后，就加一条关于朋友的。这一章需要确定给自己的原则最终目标，以及个人宣言。 习惯三：要事第一事有轻重缓急，主要问题是我们往往把忽略重要而不紧急的事情，往往一再拖延，而这些事情长期来看能够带来显著的变化。比如健身，比如读书等等。还有就是对琐事不忍说不。一个办法就是我们每周设定目标时将这些重要的事情列在清单上并认真对待。最好从长期目标上分解出来一些任务出来。（没有什么可以授权帮助提升效率的） 习惯四：利己利人一般来说，与朋友比较容易相处，在选择朋友的过程中就已经确定了他的品行，所以在相处时问题也不大。但是往往在与亲戚的相处时非常困难，很难达到利己利人。往往会有一些亲戚得尺进寸，明明占了便宜还不领情，做不到利己利人最起码要独善其身或者好聚好散，又或买卖不成仁义在。五个要领：品格，关系，协议，制度，过程。书中提到对协议的利用：与儿子协议整理庭院，协议的五要素：预期结果，达成目的的原则限度，可用资源，考评标准与期限以及赏罚。再次感觉到现代企业经营中的思维方式运用到日常生活中。 习惯五：设身处地这个和开头提到的观念转移相呼应。有效沟通四个阶段：复数语句表示专心聆听；加入解释；掺入感情，体会对方心情；既加以解释又带有感情。古希腊人认为：人生以品格第一，情感居次，理性第三。表达自己也应循着这三阶段。其实男女沟通的时候往往就有很大差异，女性往往强调感受，男性则想着解决问题。所以男性在表述之前，应该体会对方的感情。 习惯六：集思广益最开始讲人际关系的阶段：依赖，独立，互赖。集思广益就对应着互赖。首先需要理清双方将各自的诉求，在此之上提出双方可以接受的方案，需要双方都做出妥协。 习惯六：不断完善工欲善其事必先利其器。准则七就是磨炼自己，从身体，精神，心智以及待人处事四个方面：锻炼身体，陶冶精神（冥想），自我教育，历练待人处事之道。帮助他人。 总的来说，读完此书觉得有点泛泛而谈，道理大家都懂，但是不妨做出一点改变，锻炼自己的心智。","link":"/2020/01/26/journeyman/%E9%AB%98%E6%95%88%E8%83%BD%E4%BA%BA%E5%A3%AB%E7%9A%84%E4%B8%83%E4%B8%AA%E4%B9%A0%E6%83%AF%E7%AC%94%E8%AE%B0/"},{"title":"机器学习笔记","text":"在读吴军的《智能时代》和《数学之美》时常常被技术的力量给震撼到，尤其是其中现实的例子特别具有说服力，未来的发展或许由人工智能驱动。当然在深度方面，自学肯定不如专科博士生，不过未来人工智能或许就像现在的编程一样成为一种基础工具。今年的计划之一就是学习人工智能，因为离开学习太久，学习理论的东西不如从前。这一篇指南 如何用3个月零基础入门「机器学习」 提供的建议非常受用。目前在看 吴恩达的 《机器学习》视频，已完成监督学习部分，这儿做一个阶段性复习。 这个视频所涉及到的理论知识很少，不过跟着练习做完也发现已经可以解决很多问题了。首先是机器学习分为监督学习和非监督学习。监督学习就是已知训练数据集的结果，对未来的一些数据进行预测，比如数字识别，邮件分类，房价预测。非监督学习就是我们不知道结果，比如推荐系统中发现同时购买的商品。 线性回归单一特征参数线性回归大家应该都已经使用过，记得大学做物理实验的时候需要根据收集到的数据找到实验变量直接的线性关系，实验中的数据处理很多用的就是最小二乘法。假设收集到的数据已经打点在坐标系上，要求的一元线性回归方程形如 y = ax + b，我们需要保证这个直线离所有的点尽可能的近。 这个课程给的例子是已知房屋面积和价格的一组数据，我们需要找到它们之间的关系以便我们预测房价。我们可以把假设函数定义为 $$h_\\theta(x) = \\theta_0 + \\theta_1x$$ 为了评估我们的函数模型，我们引入损失函数（Cost Function）（其中的 $\\hat{y_i}$ 是预测值）： $$J(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i = 1}^{n}(\\hat{y_i} - y_i)^2 = \\frac{1}{2m}\\sum_{i = 1}^{n}(h_θ(x_i) - y_i)^2$$ 我们希望这个损失函数结果越小越好（最小二乘法），也就是求它的最小值。我们可以先回顾一下一元函数的极值求法，之前讲求根运算时介绍过牛顿迭代法，我们的递归式 $$x_{n+1} = x_n - \\frac{f(x_n)}{f’(x_n)}$$ 图形化的解释： 初始迭代的点为 $x_0$，之后根据斜率和 $f(x_0)$ 计算第二个点 $x_1$，一直到斜率趋于 0。斜率的正负决定了收敛方向，斜率的大小决定了收敛的速度。再来看下梯度下降，简单来说我们只需要将 $f(x_0)$ 替换为一个常量 $\\alpha$ 比如 0.03。 比较而言，牛顿法具有很好的收敛性，而梯度下降要简单一些。下面是使用梯度下降找到极值的直观例子，就像沿着山坡最陡峭的方向往下走。 从图上可以看出，我们求得的极值是局部最优可能不是全局最优的，这跟我们的初始值有关，不过这个不是问题。多元函数使用的是偏导，对于上面的损失函数，我们的递归式就是形如： $$\\theta_0 := \\theta_0 - \\alpha \\frac{\\partial }{\\partial \\theta_0}J(\\theta_0, \\theta_1) \\\\\\theta_1 := \\theta_1 - \\alpha \\frac{\\partial }{\\partial \\theta_1}J(\\theta_1, \\theta_1)$$ 一直迭代下去直到偏导趋于 0，因为极值点偏导为 0。其中 $\\alpha$ 的选择不能太大，太大可能导致无法收敛，太小的话迭代次数较多。注意上面的计算需要同时计算，对比下上面牛顿法的迭代式，左侧的 $\\theta_0$ 和 $\\theta_1$ 是第 $n + 1$ 的 $\\theta$ 而 $\\theta_0$ 和 $\\theta_1$ 是第 n 次的 $\\theta$。 接下来就是求偏导了: $$\\begin{aligned}\\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1) &amp;= \\frac{\\partial }{\\partial \\theta_j}\\frac{1}{2}(h_θ(x_i) - y_i)^2 \\\\ &amp;= (h_θ(x_i) - y_i)\\frac{\\partial }{\\partial \\theta_j}(h_θ(x_i) - y_i) \\\\ &amp;= (h_θ(x_i) - y_i)\\frac{\\partial }{\\partial \\theta_j}h_θ(x_i)\\end{aligned}$$ 对于 $\\frac{\\partial }{\\partial \\theta_j}h_θ(x_i)$，当 $\\theta_j$ 为 $\\theta_0$ 时值为 1，当 $\\theta_j$ 为 $\\theta_1$ 时值为 x。 多元线性回归上面的例子只有一个特征参数，实际当中的特征参数会有很多，这儿就扩展一下。 $$h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + … + \\theta_nx_n$$ 其中 $x_0$ 始终为 1。我们又可以将上面的假设函数写成向量的形式： $$h_\\theta(x) = \\left [ \\begin{matrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ … \\\\ \\theta_n \\end{matrix} \\right ]\\left [ \\begin{matrix} x_0 &amp; x_1 &amp; x_2 &amp; … &amp; x_n \\end{matrix} \\right ] = \\theta^TX$$ 损失函数为： $$J(\\theta) = \\frac{1}{2m}\\sum_{i = 1}^{m}(h_θ(x^{(i)}) - y^{(i)})^2 = \\frac{1}{2m}\\sum_{i = 1}^{m}(\\theta^Tx^{(i)} - y^{(i)})^2$$ 其中 $x^{(i)}$ 和 $y^{(i)}$ 对应第 i 组数据。 偏导 $$\\frac{\\partial }{\\partial \\theta_j}J(\\theta) = \\frac{1}{m}\\sum_{i = 1}^{m}(\\theta^Tx^{(i)} - y^{(i)})x_{j}^{(i)}$$ 上面的假设只是多元一次函数，其实假设函数式可以为任意多项式形式。以上面例子，我们可以把 $x_1^2$ 当成一个新的特征 $x_2$来看待。 $$h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_1^2 + … + \\theta_nx_n$$ 特征缩放上面房价的例子中，房屋的面积范围是 100 - 1000 这取决于实际数据，而我们通常选择的步长参数 $\\alpha$ 是比较小的一个数。这种情况下为了减少迭代次数，我们可以把已有的数据映射到 [-1, 1] 这个范围内。 $$x_i := \\frac{x_i - u_i}{s_i}$$ 其中 $u_i$ 为第 i 个特征的平均值，$s_i$ 为第 i 个特征的范围。 Normal equation梯度下降是求解 $\\theta$ 的一种方式，我们要求解的 $\\theta$ 满足 $$X\\theta = y$$ 我们可以用 求解线性方程的方式求得 $\\theta$，因为 $X$ 不一定是方阵，我们需要两边同时先乘以 $X^T$ $$X^TX\\theta = X^Ty$$ 然后两边同时乘以 $X^TX$ 的逆矩阵，有 $$\\theta = (X^TX)^{-1}X^Ty$$ 如果特征矩阵不可逆，可能是存在冗余特征或者是样本太少特征太多。 矩阵求逆的时间复杂度为 $O(n^3)$ 在特征数比较少（比如小于 1000）的时候比较有优势。 上面这些就是线性回归的内容，现在就可以使用它解决房价预测的问题了。 逻辑回归还有一个常见问题是分类问题，比如判断肿瘤是恶性还是良性，垃圾邮件分类。这类问题明显不能使用线性回归解决，假设函数 $h_\\theta(x)=\\theta^TX$ 结果可以是任意值，而实际值 y 只可能为 0 或 1。所以，我们必须确保假设函数 $h_\\theta(x)$ 满足 $0 \\leq h_\\theta(x) \\leq 1$。我们引入 Sigmoid 函数 $$g(z) = \\frac{1}{1 + e^{-z}}$$ 假设函数为 $$h_\\theta(x)=g(\\theta^TX)$$ 假设函数表示的意义是 y = 1 的可能性。可以表示为 $$h_\\theta(x) = P(y = 1|x;\\theta) = 1 - P(y = 0|x;\\theta)$$ 一般来说，当 $h_\\theta(x) \\geq 0.5$ 时 y 为 1，当 $h_\\theta(x) &lt; 0.5$ 时 y 为 0。也就是当 $\\theta^TX \\geq 0$ 时 y 为 1，当 $\\theta^TX &lt; 0$ 时 y 为 0。如图所示，在我们对样本分类时，其实就是在找到一条线，将其划分为两部分，在直线下方的小于 0，在直线上方的 大于 0 。当然上面也已经提到过，我们的假设函数可以是任意多项式的函数，不一定是一条直线。 损失函数这儿不能使用线性规划的损失函数，因为得到的不是凸函数。下面是我们的损失函数$$J(\\theta) = \\frac{1}{m}\\sum_{i = 1}^{m}Cost(h_θ(x^{(i)}), y^{(i)})$$其中当 y = 0 时 $Cost(h_\\theta(x), y) = -log(1 - h_\\theta(x))$；当 y = 1 时，$Cost(h_\\theta(x), y) = -log(h_\\theta(x))$。损失函数的效果是当实际值 y 与 计算值相同时，损失为 0，如果相反则趋于无穷大。因为 y 要么为 0 要么为 1，损失函数也可以写成$$Cost(h_\\theta(x), y) = -ylog(h_\\theta(x)) - (1 - y)log(1 - h_\\theta(x))$$ 完整的损失函数为 $$J(\\theta) = \\frac{1}{m}\\sum_{i = 1}^{m}\\left [ -y^{(i)}log(h_\\theta(x^{(i)})) - (1 - y^{(i)})log(1 - h_\\theta(x^{(i)})) \\right ]$$ 对应的偏导 $$\\frac{\\partial }{\\partial \\theta_j}J(\\theta) = \\frac{1}{m}\\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_{j}^{(i)}$$ 看上去和线性回归的很相似，这儿不再证明。 最后，如果目标分类大于 2 种，我们只需要每次分出一种，把其它的看作一类即可。 正则化和过拟合问题假设我们有一组数据，数据的分布如图所示，我们可以大概看出它应该是一条曲线，左侧的图表示我们在用直线来拟合这个函数，可以看出不太合适，我们称之为欠拟合。同样的道理，右侧我们使用的是一个高阶的函数来拟合，虽然曲线穿过了所有的点，但是我们知道这样的函数不具有普适性，我们称之为过拟合。 对于过拟合来说，除了减少参数，还可以使用正则化的方式。简单来说就是，我们希望保留一些特征参数，但是又不希望它的权重太重。我们在损失函数中加入惩罚项，注意我们并不惩罚 $\\theta_0$ 所以， j 是从 1 开始的。 $$J(\\theta) = \\frac{1}{2m}\\sum_{i = 1}^{m}(h_θ(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j = 1}^{n}\\theta_j^2$$ 总之，我们需要求得损失函数的最小值，所以不难理解当 $\\lambda$ 比较大时，$\\theta$ 会非常而趋于 0。 正则化的线性回归递归式（当然 $\\theta_0$ 不变） $$\\theta_j := \\theta_j(1 - \\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i = 1}^{m}(h_θ(x^{(i)}) - y^{(i)})x_j^{(i)}$$ 对于 Normal Equation 来说，变为 $$\\theta = (X^TX + \\lambda \\cdot L)^{-1}X^Ty$$ 其中 L 类似单位矩阵，只不过第一个位置的元素为 0 。 神经网络神经网络的表示神经网络的模型非常直观，一般的表示为： Layer 1 是输入层，Layer3 是输出层，中间的是隐藏层。 隐藏层节点又称为激活单元，第 j 层的 i 个单元记为 $a_i^{(j)}$。$Θ^{(j)}$ 表示第 j 层到 j + 1 层的映射矩阵。对于上图中的节点有：$$\\begin{aligned}a_1^{(2)} &amp;= g(\\Theta_{10}^{(1)} x_0 + \\Theta_{11}^{(1)} x_1) + \\Theta_{12}^{(1)} x_2 + \\Theta_{13}^{(1)} x_3) \\\\a_2^{(2)} &amp;= g(\\Theta_{20}^{(1)} x_0 + \\Theta_{21}^{(1)} x_1) + \\Theta_{22}^{(1)} x_2 + \\Theta_{23}^{(1)} x_3) \\\\a_3^{(2)} &amp;= g(\\Theta_{30}^{(1)} x_0 + \\Theta_{31}^{(1)} x_1) + \\Theta_{32}^{(1)} x_2 + \\Theta_{33}^{(1)} x_3) \\\\h_{\\Theta}(x) = a_1^{(3)} &amp;= g(\\Theta_{10}^{(2)} a_0^{(2)} + \\Theta_{11}^{(2)} a_1^{(2)}) + \\Theta_{12}^{(2)} a_2^{(2)} + \\Theta_{13}^{(1)} a_3^{(2)})\\end{aligned}$$ 上式中在每一层都会加入第 0 项。更一般化的等式可以写成 $$z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}$$$$a^{(j)} = g(z^{(j)})$$ 整个来看，如果没有隐藏层的话和之前接触的模型并无不同，更多层模型可以应对更为复杂的模型。具体的应用例子是以逻辑 XNOR 为例 可以看出如果没有隐藏节点的话，是不能直接实现想要的逻辑功能的，更多的隐藏层和隐藏节点意味着更为复杂的模型函数。现在，还是比较容易理解隐藏节点的增加可以使模型更能够拟合现实数据。剩下的问题就是如何确定这些隐藏节点的参数。 后向传播首先需要定义损失函数 Cost Function，我们约定网络总层数为 L，$s_l$ 为第 l 层的节点数目，K 为输出节点数目。因为神经网络的输出可能有多个节点，$h_{\\Theta}(x)_k$ 为第 k 个输出的假设函数。回忆下逻辑回归的损失函数为 $$J(\\theta) = \\frac{1}{m}\\sum_{i = 1}^{m}\\left [ -y^{(i)}log(h_\\theta(x^{(i)})) - (1 - y^{(i)})log(1 - h_\\theta(x^{(i)})) \\right ] + \\frac{\\lambda}{2m}\\sum_{j = 1}^{n}\\theta_j^2$$ 神经网络对应的损失函数略微复杂，只是因为输出节点可能为多个，另外需要将所有的参数都加入惩罚项： $$J(\\Theta) = \\frac{1}{m}\\sum_{i = 1}^{m}\\sum_{k = 1}^{K}\\left [ -y_k^{(i)}log(h_{\\Theta}(x^{(i)})(k)) - (1 - y_k^{(i)})log(1 - h_{\\Theta}(x^{(i)}(k))) \\right ] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j = 1}^{s_{l+1}}(\\Theta_{j,i}^{(l)})^2$$同样的，我们的目的也是求其最小值。过程时这样的：给定训练集 ${(x^{(1)}, y^{(1)})\\cdot \\cdot \\cdot (x^{(m)}, y^{(m)})}$初始化 $\\Delta_{i,j}^{l} := 0$对每个训练数据 t 从 1 到 m： 令 $a^{(1)} := x^{(t)}$ 使用前向传播计算 $a^{(l)}$ l = 2,3,…,L 使用 $y^{(i)}$ 计算 $\\delta^{L} = a^{(L)} - y^{(L)}$ 计算 $\\delta^{(L - 1)},\\delta^{(L - 2)},….,\\delta^{(2)}$ $\\Delta_{i,j}^{l} := \\Delta_{i,j}^{l} + a_j^{(l)}\\delta_i^{(l + 1)}$ 最后再更新 $\\Delta$ 矩阵$D_{i,j}^{(l)} := \\frac{1}{m}(\\Delta_{i,j}^{l} + \\lambda\\Theta_{i,j}^{(l)})$ 当 j != 0$D_{i,j}^{(l)} := \\frac{1}{m}\\Delta_{i,j}^{l}$ 当 j = 0 反向传播公式有：$\\delta^{L} = a^{(L)} - y^{(L)}$$\\delta^{l} = (\\Theta^{(l)})^T\\delta^{(l + 1)} .* {g}’(z^{(l)})$ 神经网络有点黑盒的感觉，上面的公式推导并不是太难。为什么这样子可行，不是太容易理解。从之前的例子来看就是更多的节点构建的模型可以更复杂，更容易拟合数据。 支持向量机支持向量机要从逻辑回归说起，对于单项的损失而言，下图中上面是单项的损失函数，下面是对应的函数曲线。 直观上看就是我们使用曲线下面的直接来替换原来的函数，我们令 $$\\begin{aligned}cost_1(\\theta^Tx^{(i)}) = max(0, K(1 - z))cost_0(\\theta^Tx^{(i)}) = max(0, K(1 + z))\\end{aligned}$$ 然后用 $cost_1(\\theta^Tx^{(i)})$ 和 $cost_0(\\theta^Tx^{(i)})$ 分别替换 $(-logh_{\\theta}(x^{(i)})$ 和 $(-log(1 - h_{\\theta}(x^{(i)}))$ 得到新的损失函数 $$J(\\theta) = C\\frac{1}{m}\\sum_{i = 1}^{m}\\left [ y^{(i)}cost_1(\\theta^Tx^{(i)}) + (1 - y^{(i)})cost_0(\\theta^Tx^{(i)}) \\right ] + \\frac{1}{2}\\sum_{i = 1}^{n}\\theta_j^2$$ 这儿的 C 和 $\\lambda$ 一样，可以看出当 C 值比较大时，为了取得最小值，损失函数必须为 0， 也就是说当 y 为 1 时，$\\theta^Tx^{(i)} &gt;= 1$，当 y 为 0 时，$\\theta^Tx^{(i)} &lt;= -1$ SVM 又称为大间隔分类器，因为当 C 比较大的时候，决策边界离正负样本的距离都比较远。它的数学原理是这样子的：$\\theta^T &gt;= 1$ 也就是向量的内积，因为惩罚参数项当 $\\theta$ 值变得越来越小时，为了使内积满足 $\\theta^Tx^{(i) &gt;= 1$，根据 $\\theta^T \\dot x^{(i) = \\left | \\theta \\right | \\left | x^{(i)} \\right | cos\\alpha$ 夹角必须尽可能的小， 也就是必须使得 $\\theta$ 尽可能的与数据集尽量在同一方向，表现出来的就是与决策边界正交。 核函数对于非线性分界线的情况，比如只有两个特征 $x_1$ 和 $x_2$, 我们可以扩展我们的特征，假设函数可能是 $\\theta_0 + \\theta_1 x_1+ \\theta_2 x_2 + \\theta_3 x_1x_2 + \\theta_4 x_1^2 + \\theta_5 x_2^2 + … &gt; 0$ 我们可以根据给定的 x 和 地标 $l^{(1)} l^{(2)} l^{(3)}$ 计算出新的特征 f $f_1 = similarity(x, l^{(1)}) = exp(-\\frac{\\left | x - l^{(1)} \\right |^2}{2\\sigma ^2})$ 这个相似度函数又叫高斯核函数，在 x 与 $l^{(1)}$ 比较接近时取得 1，较远时为 0。 而 $l^{(1)} l^{(2)} l^{(3)}$ 的选择则是 $l^{(1)}=x^{(1)}； l^{(2)}=x^{(2)}； l^{(3)}=x^{(3)}$。感觉上将原来的 n 维的 $x^{(i)}$扩展为 m 维的 $f^{(i)}$。在模型训练中的成本函数也就变为 $$J(\\theta) = C\\frac{1}{m}\\sum_{i = 1}^{m}\\left [ y^{(i)}cost_1(\\theta^Tf^{(i)}) + (1 - y^{(i)})cost_0(\\theta^Tf^{(i)}) \\right ] + \\frac{1}{2}\\sum_{i = 1}^{m}\\theta_j^2$$ SVM 参数选择 Large C: Lower bias high variance 训练数据偏差更小，可能过拟合。Small C: Higher bias low variance Large $\\sigma^2$ $f_i$ 变化更缓慢，Higer bias low varianceSmall $\\sigma^2$ $f_i$ 更陡峭， Lower bias high variance 意味着只有当两个向量非常相近时才认为相似，训练出的参数可能过拟合。 逻辑回归和 SVM 的选择如果 n 非常大（相当于 m），逻辑回归或者 SVM 没有核函数 如果 n 比较小，m 适中， SVM + 高斯核函数 如果 n 小 m 比较大，添加更多特征，使用逻辑回归或者没有核函数的 SVM 非监督学习聚类非监督学习的训练集 ${x^{(1)},x^{(2)},x^{(3)},…,x^{(m)}}$ 要分类数为 KK-means 算法比较直观：首先随机初始化 K 个 cluster centriods $\\mu_1, \\mu_2,…,\\mu_K$，之后重复以下步骤： 对所有的 m 个数据： $c^{(i)}$ 设置为离 $x^{(i)}$ 最近的 cluster centriods 索引 对所有的 K 个： 对索引为 k 的点求平均，然后设置为新的 $\\mu_k$ 对应的损失函数 $$J(c, \\mu) = \\frac{1}{m}\\sum_{i=1}^{m}\\left | x^{(i)} - \\mu_{c^{(i)}} \\right |^2$$ 这个算法也和随机初始化时选择的位置有关，所以一般会重复多次找到效果最好的一次。 至于 K 的选择，没有什么更好的办法，所谓的 Elbow 方法就是慢慢增加 K 一直到不能看到明显变化。 维度规约维度规约顾名思义，将原来高维度的数据降维，同时要保持数据大部分的特征。比如降低到二维可以可视化呈现，或者进行数据压缩。 Principal Component Analysis(主成分分析) 算法将 n 维数据降低到 k 维 计算 方差矩阵$$\\Sigma = \\frac{1}{m}\\sum_{i = 1}^{n}(x^{(i)})(x^{(i)})^T$$ 计算矩阵的 eigenvectors$$[U, S, V] = svd(Sigma)$$ 这部分只讲了怎么计算，没有解释为什么，涉及到的线代知识后面再补上 异常检测这个涉及到大数定理和高斯正态分布，简单来说已有的数据符合高斯正态分布，我们计算出正态分布的 $\\mu$ 和 $\\sigma^2$ 对于给定的新数据，计算分布的概率。如果概率小于某个值则认为是异常。 异常检测其实和监督学习有点像，只是一般来说异常监测的正例数目非常小而且没有一定的特征，比如是否是诈骗邮件。 大规模机器学习简单思路：先用小部分数据训练模型，逐渐增加数据规模。在线的机器学习，并不保留数据，根据新来的直接数据更新参数。MapReduce 不再多说。 OCR 例子视频的最后介绍了一个机器学习的常见应用： OCR。整个流程有：文字检测；字符切割；字符分类。检测的方法没有很好的办法就是滑动窗口，移动窗口逐个检测，逐渐调整窗口大小，字符切割也需要用到滑动窗口。实际感觉这些流程处理起来也不简单。最后还介绍了人工制造数据，比如通过反转图片，扭曲图片来生成新的数据。 小结整个视频涉及到的数学知识并不多，而且学习之后的课程练习也可以实实在在的感受到确实可以用来解决一些问题，总体来说确实比较适合的同学吧。","link":"/2020/08/02/journeyman/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"一个内存泄漏问题分析","text":"一般来说，借助于强大的 GC 和 lint 工具，前端还是很少会碰到内存泄漏问题的。这篇文章说下我最近遇到的例子以及排查的过程。 内存泄漏的检测还是非常容易的：打开 Chrome DevTools 选择 Memory 选项，点击 Take heap snapshot 等待查看内存大小。重复这个步骤，如果你发现内存大小定期增长，或者增长的很有规律，那么八成出现内存泄漏了。这个是 Google 的 文档 问题检测我们的应用是这样组织的，采用微前端架构，涉及到几个项目，一个 Shell 负责管理具体渲染哪个页面，应用默认是 Documents 页面，还有一个 Teams 页面分别属于不同的项目。 在 Teams 页面采集内存信息，回到 Documents 页面等待页面加载完成再回到 Teams 页面再次采集内存信息。重复这个过程几次，这是结果截图。为了确保数据准确，在隐私窗口测试以免受插件影响，在每次收集之前都点击 Collect Garbage。每次都多次采集直到得到 4 个相同的值表示内存大小稳定。这个截图上，5，6，7与8相同就移除了，不过足以说明应用确实存在内存泄漏了。 问题诊断现在我们知道有内存泄漏，先比较下 Snapshot 19 和 Snapshot 15 的内存信息. 😱 好吧, 太多对象了，几乎是组件树上的所有示例都有在列，毫无头绪。因为涉及到三个项目，完全不知道如何下手。不过不管怎样，先从可以做的做起，先把开发环境准备好。当然开发环境本身就有更多干扰因素，不过好歹还是有了第一条线索。当我尝试复现问题时，根本不等 Documents 页面完成渲染就切换页面，控制台有一个警告信息： Warning: Can’t perform a React state update on an unmounted component. This is a no-op, but it indicates a memory leak in your application. To fix, cancel all subscriptions and asynchronous tasks in the componentWillUnmount method. 因为有错误堆栈信息，所以很快发现，这个错误时因为没有清理定时器导致的。虽说本书是有逻辑来清理定时器的，但是没有考虑到这些逻辑因为用户页面跳转中断。所以说最好还是在 componentWillUnmount 完成所有的清理工作。 修复这个问题并且排查了所有的定时器之后，发现内存泄漏还在，看起来没那么容易解决。 为了方便问题排查，先修改 Teams 为仅渲染普通文本，依然稳定复现内存泄漏。不过其次最为可疑的就是全局的事件监听，排查一遍发现有些监听未被移除。幸运的是，发现一个低级错误，本来 componentWillUnmount 应该移除监听结果又添加了一遍：. 123456componentDidMount() { window.matchMedia('print').addListener(this.printHandler);}componentWillUnmount() { window.matchMedia('print').addListener(this.printHandler);} 把 addListener 改为 removeListener 之后重新检查一遍，发现还是存在内存泄漏。 检查了所有的事件监听之后，确信没有遗漏，在查看内存信息的时候看到 onLoad 事件回调，对了，on 事件给漏了。在 shell 里一些可疑代码： 1234567const tag = document.createElement('script');// ... some codetag.onload = () =&gt; { resolve();};// ...document.body.appendChild(tag); 不管怎样，我们应该清理到这些事件： 123456tag.onerror = () { tag.onload = tag.onerror = null;};tag.onload = () { tag.onload = tag.onerror = null;}; 重新检查内存泄漏，发现还是存在。上面的代码因为只允许了一次所以不会导致内存大小变化。这个时候都怀疑是不是第三方库的原因了。不过还是当把 Documents 页面换成普通文本时发现，没有问题了。所以问题肯定在 Documents 组件上。 为了进一步缩小范围，试着把 render 方法移除，发现问题这样都有问题。 所以说，问题还是在 componentDidMount 和 componentDidMount 上的事件监听上. 但是看起来一切正常。 因为对 matchMedia 这个实验特性不熟，又再次查看了下文档. 这次注意到：它说每次都会返回一个新的对象 … 意思就是 matchMedia('print') !== matchMedia('print') 这也就是为什么 matchMedia('print').removeListener(this.printHandler); 压根没有的原因。修复这个问题之后，再次检查就没有内存泄漏的问题了。 总结内存泄漏很少碰到，当然也很难调试犹如大海捞针。 除了排查定位：定时器，全局的事件监听，以及全局对象是优先排查的对象。","link":"/2019/08/11/novice/A-memory-leak-case/"},{"title":"Distribute tracing - newrelic","text":"newrelic 提供的 Distribute tracing 功能非常实用。阿里内部使用的是鹰眼系统，因为一直做前端开发所以我并不是很清楚到底怎么工作的。虽说之前也多少翻过一些文章介绍大概的系统的架构，但是一直以为在调用下游服务时是显式传入当前 TraceId 的。直到最近使用 newrelic 的 Distribute tracing 追踪才了解到还有一种无侵入的方案。 比较流行的全链路监控方案 Zipkin Pinpoint Skywalking 也有完全无侵入的方案，但都是 JAVA 编写的。而且在搜寻对应的 Node.js 实现时，找到的仓库代码往往并不齐全。无奈只能在 newrelic 源码中寻找线索，果然发现大量的 instrumentation，其中就有对原生 http模块上方法的拦截。这样就可以解释为什么我们不需要显式传给下游 TraceId。 或许不难猜到，在发起新请求时，只需要把当前的 traceId 附带到请求头上即可。在接收请求时解析请求头上信息，并将其传给当前新建的 transaction 作为 parentId。但是还有一个问题：如果有多个请求同时处理，如何保证传给下游正确的 TraceId。假设服务端的伪代码是这样的： 123456app.get('/frontend', (req, res) =&gt; { asyncTask((is) =&gt; { const result = await http.get('/downstream'); res.json(result); });}) 如果同时有两个请求 /frontend(1) /frontend(2) 怎么保证调用链不串呢？因为我们并不能保证异步任务的时间，完全有可能第二个请求先调用了下游服务。 Skywalking Node.js 就有这样的问题。 继续查看代码又有新的发现，newrelic 同时也对很多基础模块进行了拦截，包括我能想到的 Timer, Promise Async, FS 等等， 包装回调主要目的在执行回调之前找到当时的 segment。 这么多的拦截代码想当然的会需要一定的开销。暂时未见到 newrelic 上关于这方面的文档。 更多链接https://juejin.im/post/5a7a9e0af265da4e914b46f1","link":"/2020/01/07/novice/Distribute-tracing-newrelic/"},{"title":"浅谈编码能力","text":"若说编程能力，涉及的方方面面太多，这儿只谈谈写代码。你可能也有发现，有些课程优秀的同学代码写的并不优雅，甚至在大公司工作\b几年的人代码写的也可能不堪入目。或许不难理解，在学校的学习的计算机知识与编码能力没有太大关系。我们说代码写的好，往往说的是整洁，清晰，易于维护扩展。 如果不清楚自己编码的水平，可以尝试一下这个挑战：给你两个小时让你实现一个类似 1024 的游戏。要求不依赖任何库，当然可以使用工具和 IDE。如果你没有尝试做过类似的事情，不妨现在试一下，回头再读这个文章。 在你做完之后与 Github-2048 上的实现比较一下，看看有哪些收获。 我经历的一些实际项目中，充斥着大量的反模式和代码异味。我们花了很多时间去清理那些技术债务。依据我的经验来看，里面的错误做法非常普遍。React.js 简单，但是简单并不代表容易。某个框架上手容易，其实也意味着挖坑容易。在真正的将新技术在业务项目中铺开之前，一定要多调研，多参考优秀的实践。 当你用 React.js 堆业务时，思考一下创建的组件是否符合面向对象的基本原则，新手是否能够通过你暴露的属性猜出这个组件的功能。 在你写代码时，哪些原则是你一直遵循的？如果有人指出你的代码中违背了某一原则，你是否能够解释个所以然来。 个人以为提升编程能力的方法有： 推荐阅读《代码整洁之道》《代码大全》《重构》等经典书籍 大量阅读源码 Code Review（自己先 Review 一遍）","link":"/2019/05/23/novice/about-coding-skill/"},{"title":"git 合并策略 之 recursive","text":"前几天老婆大人考察：git merge 时什么情况下进行 auto merge 以及如何 merge ? 我只能回答：如果文件同一行都有修改就会冲突，如果没有冲突就会自动 merge 。另一个问题是：如果一个文件删除了，为什么 merge 时还存在？这个应该是在另一个分支上对该文件做了修改。在阅读后面的文章之前，假定你理解 git 的分支是由 commit 串起来的一条链。如果不明白上一句话，请先补下 git 分支 知识点。 从 auto merge 说起，在我们合并两个分支时（不讨论 Fast-forward），如果两个分支没有冲突，经常会看到下面这句输出：Merge made by the 'recursive' strategy从 merge-strategies 文档中可以看到在 git 合并分支时可以指定合并策略，而 recursive 是默认的策略，该策略使用 3路合并算法。 three-way merge为方便解释，我们新建一个 git 仓库，在 master 分支新建一个 animals.txt，在 animals.txt 中添加这么几行内容： 123catdogoctopus 第一个提交 commit 记为 B，然后切出一个分支 dev，在 dev 分支修改 octopus 为 tigger，commit 记为 E，在文件最后一行后插入一条 elephant，commit 记为 F；切回 master 分支，在文件第一行前插入一条 mouse，commit 记为 C, 修改 octopus 为 cow 记 commit 为 D。分支结构如下： 123B-C-D master \\ E-F dev 其中 master 分支文件内容： 1234mousecatdogcow dev 分支文件内容： 1234catdogtiggerelephant 如果直接对两个文件进行 diff，我们是不知道如何进行 merge 的。因此我们就需要以原文件为参照进行三路合并。现在在 master 分支上执行 git merge dev ，需要进行三路合并的就是 B、D、F 这三个 commit。三路合并状态如下： 除了需要手动解决的冲突，三路合并很符合我们的期望。需要注意的是，三路合并只关心三个点，至于分支的历史它是不关心的。假设 dev 分支 commit F 中 tigger 改回 octopus，最后合并的结果是 cow。 recursive 又是什么？刚刚的三路合并提到了公共祖先，如果两个分支不止一个公共祖先怎么办？下面的希望你看明白了。 merge D 和 F时，发现 C 和 E 都是它们的公共祖先，而且这两个祖先还没有先后之分。如下： 1234B--C---D master \\ \\ / \\ / \\ E---F dev 合并的策略是先合并 C 和 E 得到一个虚拟的公共祖先 G，再把这个虚拟节点作为公共祖先进行合并。那如果合并 C 和 E 的时候发现他们的公共祖先也不止一个怎么办？所以就要递归进行了。查询公共祖先的方法见 git-merge-base 好了，问题来了。如何实现 merge base?（提示：优先级队列） 至此，如果上面有叙述不清楚的地方，可以直接阅读下面的两个文章 three-way-merge merge-recursive-strategy","link":"/2017/02/22/novice/git-%E5%90%88%E5%B9%B6%E7%AD%96%E7%95%A5-%E4%B9%8B-recursive/"},{"title":"introduce to websocket","text":"背景在 webSocket 出现之前，为了实现消息推送，实现方式有轮询和Comet。Comet 又分两种：长轮询和流技术。 webSocket APIwebSocket 使用非常简单，API 比较简单： 123456const ws = new WebSocket('ws://localhost:8080');ws.onopen = function() {};ws.onmessage = function() {};ws.onclose = function() {};ws.send('');ws.close() 握手webSocket 需要借助 HTTP 协议，如果是 https 协议，则使用 wss 协议 123456GET ws://dw-dev.alibaba-inc.com:8080/sockjs-node/056/yqjbei0m/websocket HTTP/1.1Connection: UpgradeUpgrade: websocketSec-WebSocket-Version: 13Sec-WebSocket-Key:PnmANvXSAsaE+HljkwpFLA==Sec-WebSocket-Extensions: permessage-deflate; client_max_window_bits 123Connection: UpgradeUpgrade: websocketSec-WebSocket-Accept:y2AYVF5ss1DbiA0wKhARD3d37fw= 服务端代码 Node.js 实现： 123456789const server = http.createServer();server.on('upgrade', (req, socket) =&gt; { const secKey = req.headers['sec-websocket-key']; socket.write('HTTP/1.1 101 Web Socket Protocol Handshake\\r\\n' + 'Upgrade: WebSocket\\r\\n' + `Sec-WebSocket-Accept: ${calcAcceptHash(secKey)}\\r\\n` + 'Connection: Upgrade\\r\\n' + '\\r\\n');}); 服务端必须返回 Sec-WebSocket-Accept，否则浏览器将抛出错误。 Sec-WebSocket-Accept 的计算方法是 原 key 与 ‘258EAFA5-E914-47DA-95CA-C5AB0DC85B11’ （魔数）字符串拼接再取 sha-1 哈希值的 base64 编码。代码描述： 12const MAGIC_STRING = '258EAFA5-E914-47DA-95CA-C5AB0DC85B11';crypto.createHash('sha1').update(secKey + MAGIC_STRING).digest('base64'); 另外，服务端可以拒绝同一客户端的多个连接以避免 DOS 攻击。 数据帧1234567891011121314151617 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1+-+-+-+-+-------+-+-------------+-------------------------------+|F|R|R|R| opcode|M| Payload len | Extended payload length ||I|S|S|S| (4) |A| (7) | (16/64) ||N|V|V|V| |S| | (if payload len==126/127) || |1|2|3| |K| | |+-+-+-+-+-------+-+-------------+ - - - - - - - - - - - - - - - +| Extended payload length continued, if payload len == 127 |+ - - - - - - - - - - - - - - - +-------------------------------+| |Masking-key, if MASK set to 1 |+-------------------------------+-------------------------------+| Masking-key (continued) | Payload Data |+-------------------------------- - - - - - - - - - - - - - - - +: Payload Data continued ... :+ - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - +| Payload Data continued ... |+---------------------------------------------------------------+ 数据长度可变；第二字节标示的 len 为 126 时，后面 16 位作为 payload 长度。如果为 127，用后面 64 位。 Mask 如果是客户端发送则必需。浏览器随机生成掩码，避免随意伪造发送内容。 1234var DECODED = &quot;&quot;;for (var i = 0; i &lt; ENCODED.length; i++) { DECODED[i] = ENCODED[i] ^ MASK[i % 4];} opcode 4位0x0 denotes a continuation frame0x1 denotes a text frame0x2 denotes a binary frame0x8 关闭0x9 (Ping)0xA (Pong)其它保留 应用webpack-dev-server 断开重连-指数退避算法webpack-hot-middleware SSE ReadMoreaboutwebsocketWriting_WebSocket_servers服务端实现全部代码可以参考 留香的 easy-websocket","link":"/2017/08/31/novice/introduce-to-websocket/"},{"title":"《Making Yourself Indispensable》笔记","text":"当然之前也是非常不屑于成功学的书籍的，不过最近对这类书也有了新的认知，现在个人觉得是一本挺实用的一本书。成功学的书大同小异，读的时候你会觉得道理我都懂，读完之后觉得并没有什么用。 这本书是一本实用型书籍。书中最开始描述了一个理想的场景，你对工作充满激情，老板认可你的价值，同事觉得你很靠谱，并且能够兼顾家庭等等。但是实际生活并非如此，你与不可或缺相去甚远，每天你最不想做的事情就是上班。 作者首先定义不可或缺，让我们树立一个意识，这无关乎智商，能力，教育，每个人都能做到。作者根据自己的观察，给出了一个包括 10 个行为的测评。之后给出几乎每天都会做的 6 个选择，这些选择决定你是否不可或缺。这 6 个选择： Purpose Driven or Goal Driven Paly Big or Play Small Adaptable or Rigid We Centered or Me Centered Priority Focused or Activity Focused Value Others or Disregard Others 六个选择Purpose Driven or Goal Driven这是最重要的一个，平时可能只关注老大分配下来的任务，是否思考过你的最高目标。书中的一个例子：他永远不会忘记的一个水管工在修浴室管子时顺便帮检查其它的水龙头。 Play Big or Play Small这个个人感觉是上一个的延伸，更像是一个人的格局。更多的从团队，从公司视角去思考你能做些什么。 Adaptable or Rigid每个人都有自己的主张，见解。尤其是最终的决策与你的建议完全不同时，你是否依然能够支持甚至推动下去。 We Centered or Me CenteredPriority Focused or Activity Focused避免瞎忙 Value Others or Disregard Others每个人都希望能够得到别人的认可，反过来你也需要认可他人的价值。在一些会给同事一个公开的赞，甚至买一个小礼物（公司报销）即使只是举手之劳。这个在国内并没见到过，关系熟络的反而觉得例外，往往只会在取得一个比较大的成果时才会意思一下。但是个人觉得这也恰恰是我们需要重视的，让别人感受到尊重。 之后紧接着练习，会让你描述你的最高目的，你可以采取哪些行动（Play Big, Being Adaptable, We Center…）。回头再看这些选择都对应了： you are not indispensable unless you use your gifts and principles in service to other people’s success, improvement, or survival. 这本书介绍了 Road Map 循序渐进地让你不可或缺。良性循环和恶性循环。在开始 Road Map 之前 State Your Intentions and Set Your Ultimate GoalsIntention represents the desired purpose, goals, and outcomes that are the basis of your plans and actions. Picture What Success Looks Like to YouIt usually includes four sections: Customers, Team members, Managers, Self-development Test Your picture of successApply the above choices Keep Your Intention and Picture of Success AliveSpend time to review it regulary 简单来说就是，定义你的目标，越清楚越好，之后想下怎么样做（可以采取哪些行动），越具体越好，同时思考你可以利用哪些资源帮助你。 RoadMap 1. Recognize Your Current Reality 认清你的位置需要诚实，客观中立，勇敢 测评 找一个导师（几个建议） 2. The power of taking ownership *3. Gaining Strength through forgiveness4. Self examination to foster solutions5. Master learner - Your default response6. Take action to be successfulCelebrating SUCCESS 总之，书上内容还是非常实用的，即便还未完全照做，但是依然觉得里面的建议十分中肯。","link":"/2019/06/10/novice/notes-of-making-yourself-indispensable/"},{"title":"编程之外的软实力","text":"在刚毕业的时候，我也和大多数人一样，心想自己不擅长与人打交道，比较适合一心搞技术（当然也挺羡慕那时的 leader 管几个人分分任务不再需要写代码）。最初也确实是这样子，花比较多的时间在技术上。现在看来，对于新人来说这依然是第一优先级的事情。 对于刚从学校出来的学生来说，最为推荐的书是《程序员修炼之道-从小工到专家》。不只是技术，这本书还会涉及到效率工具，保持学习，团结交流以及推动变化。因为并非每个人在刚毕业的时候都有一个靠谱的人领路，这本书可以让你少走一些弯路。 在之后工作中渐渐发现有些软实力要比技术更为重要，有时新技术的学习能给你带来的收益远不如这些能力的提升。下面只是个人想到的一些点： 效率在你可以独立承担一个项目的时候，你会留意与其它人的差距，同样的项目交给你，你是否可以做的又快又好。这可能取决于你对工具使用的熟练程度以及是否有能力开发工具减少你的重复劳动。建议保持学习；最近才发现 ssh 到远程服务器使用 Iterm2 设置之后一个快捷键就可以开始工作了。 业务在阿里巴巴，不是技术说的算，更多的是业务。技术也只有在创造了价值的时候才有用，不管是绩效评审还是晋升。每个人都会去思考业务中的痛点，尝试去解决这些问题。很多时候，这些痛点不是你一个人能够解决掉的，很可能涉及到各个方面。这又取决于你是否能够调动足够多资源以及推动其他人解决这个痛点。有人说，在腾讯你能做成一件事要看你能够调动多少资源。 影响他人现在让你说出你身边三个优秀的程序员，你会选谁？然后回想一下他们的共同点。你选他们是因为他们代码写的很溜吗？很可能不是。勇于担当，照顾新人（乐意分享和帮助），谦逊（让你感受到被尊重）你也想成为受欢迎的人？有本叫《How to make friends and influence people》我猜你早就读过中文版《人性的弱点》 管理你有思考过，站在管理者的角度，他们对你的期望和你自己在做的事情是否匹配，当你有些新想法时，能否争取到他们的支持？或者站在公司角度思考，你在做的事情和公司的目标是否匹配？如果你和老大的关系很好，一切会好说，即使你不大欣赏你的管理者，保持一个定期的沟通以及定期的反馈也是必要的。我个人非常喜欢双周一次的与 Manager 的 1:1 会议。所以，对有一定工作经验的人，我一定会推荐读几本管理类的书。比如《Make yourself indispensable》 企业文化当你思考一家公司为什么能够成功时，你会发现可能有很多的因素，它可能在某些方面做的不是很好，但不足以致命。而公司失败一个致命的错误足矣。公司大的时候价值观能够很好的发挥作用，比如阿里巴巴的人才观：聪明，皮实，乐观，自省。直接就筛选掉那些整日抱怨而不行动的人。如果你发现什么做的不够好的地方，那不是正是你要努力推动的吗？ 个人你有没有发现身边一些人做了 1 可是却吹成了 10，而你做了 1 结果说成了 0.5。 在你往上走的过程中表达能力也是急需提升的，包括讲故事的能力。 之所以这么强调软实力，是因为像大多数程序员一样我在最开始对这些东西并没有给予足够的重视，而且也没有一个清晰的路径来培养这些能力。这些只是自己碎片化的吸收以及从跌打爬滚的经验中学习得到的，或许如同大多数道理一样，听起来觉得老生常谈，但只有在自己切身经历之后，才慢慢理解。 与诸君共勉！","link":"/2019/05/21/novice/soft-skills/"},{"title":"【翻译】React.js 初学者应该知道的 9 件事","text":"原文地址：9 things every reactjs beginner should know 2016年1月份的文章，现在才翻译，又落后了半年 现在为止我使用 React.js 已经6 个月了。6 个月 放长远看一点也不长。但是，在 JavaScript 框架层出不穷的今天，6 个月可以称为老前辈了。最近指点了几个新人入门 React ，所以想总结一下写篇文章启发更多的人。下面总结的这些点，一些点是我希望在自己入门的时候就已经知道的，另外一些则是让我真正的理解 React。 本文假定你已经有了一下基本的概念。如果你不熟悉 component、props 或者 state 这些名词，你最好先去阅读下官方起步和手册。下面的代码示例我将使用 JSX 作演示，因为使用 JSX 语法写组件更为简洁，也更具表达力。 1. React.js 只是一个视图库我们从最基本的开始。React 不是一个 MVC 框架，好吧，它根本就不是一个框架。它只是一个渲染视图的库。如果你对 MVC 熟悉的话，你就会意识到 React.js 只对应了V 这部分，如果它插手了 M 或 C 的逻辑，你就需要考虑用其它方法来解了。否则，到最后，你的代码很可能会变成一坨翔。这部分后面会细说。 2. 组件尽可能的小这一点有些显而易见，但是有必要强调一下。每个良好的程序员都知道，较小的类、模块更容易理解、测试和维护，对于组件来说也是一样。我起初犯的错误是低估了 React 组件合适的大小。当然，合适的大小取决于很多不同的因素（包括个人与团队偏好），但是，一般来说，我建议，让组件明显小于你本认为的必需大小。举个栗子，我的个人网站主页上的这个组件，用于展示我的最新博文： 12345678const LatestPostsComponent = props =&gt; ( &lt;section&gt; &lt;div&gt;&lt;h1&gt;Latest posts&lt;/h1&gt;&lt;/div&gt; &lt;div&gt; { props.posts.map(post =&gt; &lt;PostPreview key={post.slug} post={post}/&gt;) } &lt;/div&gt; &lt;/section&gt;); 这个组件本身是一个 &lt;section&gt;，里面只有两个 &lt;div&gt;。第一个&lt;div&gt;有一个标题，第二个&lt;div&gt;只是映射一些数据，使用数据中的每个元素渲染 &lt;PostPreview&gt;。还有一部分抽取&lt;PostPreview&gt;作为独立组件，这点很重要。我认为这是一个组件最合适的大小。 3. 写函数式组件首先，我们有两种定义 React 组件的方式，第一种是用 React.createClass()： 12345const MyComponent = React.createClass({ render: function() { return &lt;div className={this.props.className}/&gt;; }}); 另一种是 ES6 class 写法： 12345class MyComponent extends React.Component { render() { return &lt;div className={this.props.className}/&gt;; }} React 0.14 引入了一个新语法来定义组件，使用属性作为参数的函数： 123const MyComponent = props =&gt; ( &lt;div className={props.className}/&gt;); 这是我最喜欢的定义 React 组件的方式。除了语法上简洁，这种方法还能帮助你界定什么时候需要拆分组件了。我们来回顾下之前的例子，假设下面是没拆分之前的代码： 123456789101112131415161718192021222324252627282930class LatestPostsComponent extends React.Component { render() { const postPreviews = renderPostPreviews(); return ( &lt;section&gt; &lt;div&gt;&lt;h1&gt;Latest posts&lt;/h1&gt;&lt;/div&gt; &lt;div&gt; { postPreviews } &lt;/div&gt; &lt;/section&gt; ); } renderPostPreviews() { return this.props.posts.map(post =&gt; this.renderPostPreview(post)); } renderPostPreview(post) { return ( &lt;article&gt; &lt;h3&gt;&lt;a href={`/post/${post.slug}`}&gt;{post.title}&lt;/a&gt;&lt;/h3&gt; &lt;time pubdate&gt;&lt;em&gt;{post.posted}&lt;/em&gt;&lt;/time&gt; &lt;div&gt; &lt;span&gt;{post.blurb}&lt;/span&gt; &lt;a href={`/post/${post.slug}`}&gt;Read more...&lt;/a&gt; &lt;/div&gt; &lt;/article&gt; ); } 这个 class 还凑合。我们已经从 render 方法中抽取了几个方法，方法足够小，命名合理。我们来试着用函数式的语法重写一下： 123456789101112131415161718192021222324252627const LatestPostsComponent = props =&gt; { const postPreviews = renderPostPreviews(props.posts); return ( &lt;section&gt; &lt;div&gt;&lt;h1&gt;Latest posts&lt;/h1&gt;&lt;/div&gt; &lt;div&gt; { postPreviews } &lt;/div&gt; &lt;/section&gt; );};const renderPostPreviews = posts =&gt; ( posts.map(post =&gt; this.renderPostPreview(post)));const renderPostPreview = post =&gt; ( &lt;article&gt; &lt;h3&gt;&lt;a href={`/post/${post.slug}`}&gt;{post.title}&lt;/a&gt;&lt;/h3&gt; &lt;time pubdate&gt;&lt;em&gt;{post.posted}&lt;/em&gt;&lt;/time&gt; &lt;div&gt; &lt;span&gt;{post.blurb}&lt;/span&gt; &lt;a href={`/post/${post.slug}`}&gt;Read more...&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;); 代码基本一样，无非是将类里的方法暴露为函数。但是，对我来说，区别可大了。在基于类的例子中，我看到的是class LatestPostsComponent {，自然而然会往下扫描闭合括号，然后在心中默想“在这儿这个类结束了，这个组件也到这”。对比函数式组件，我看到 const LatestPostsComponent = props =&gt; {，看到函数结束，就已经知道“这个函数结束，组件也在这结束”。“但是，等等，这个组件外面的代码是些什么鬼？还在同一个模块，哦，这是另一个函数，接收数据然后渲染视图，我把它抽取到出来就行了” 我就不再啰嗦函数式组件有助于我们遵循上面第二点。 以后，React 也会做一些优化，会使函数式组件比基于类的组件更为高效。（__更新__：函数式组组件的性能影响比我想象的要复杂。但是，如果性能不是大的问题，我依然推荐尽可能的写函数式组件，你应该好阅读下这个和这个，选择一个合适自己的） 还有个重要的店，函数式组件有几个 ’限制‘ ，我个人认为是大优点。第一个是它不会有 ref 赋给它，ref 在查找子组件并与之通信上非常方便，我的感受是 这是使用 React 的 __错误方式__。refs 鼓励一种非常直接，近于 jqeury 的方式写组件，远离了 函数式，单向数据流哲学理念，这些理念恰恰是我们选择 React 的初衷！ 另一个大的区别是函数式组件不会有状态依附，我下一个点就是讲… 4. 写无状态组件不得不说，到目前为止，我觉得写 React 应用，最让我头疼的事都是由包含很多状态的组件引起的。 状态让组件很难测试单纯的输入输出函数是最容易测试的，这点可以作为抛弃状态定义组件的理由吗？当我们测试很多状态的组件时，为了测试预期行为，我们必须先将组件设置为“正确的状态”。我们还必须考虑到所有的状态（因为组件可能在任意时刻改变这些状态）和属性（不受组件控制）组合，然后再去考虑那个组合需要测试，怎么测试。如果组件只是一个输入属性的处理函数，测试简直是不能更简单了。（关于测试，后面会讲）。 状态让组件很难推理（定位预期）当你读一段代码中包含很多状态的组件，特别费劲，你需要在脑海中记录组件的状态。这些问题：”状态有没有初始化？”，“如果我在这儿改变状态将会发生什么？”，“有几个地方改变了这个状态”，“这个状态是否存在条件竞争？”，这几个问题非常普遍。跟踪组件变化太蛋疼了。 状态让组件很容易引入业务逻辑我们不应该搜索组件然后才能确定行为。记住，React 只是一个视图库，所以，把渲染逻辑丢在组件里面没问题，但是业务逻辑也丢里面就有问题了。但是呢，如果你的应用状态都在组件里面，那在组件内部访问这些状态就会很方便，这样就会诱使你把业务逻辑也丢在里面。回顾下刚说的那点，这么做单元测试怎么办 - 没有业务逻辑你没法测试渲染逻辑，反之亦然。 状态让组件很难与应用其它部分共享信息父层组件的状态很容易传给下层组件，反过来就费事了。 当然，有时一个组件独立维护部分状态也是有必要的。在这种情况下，尽管放心使用 this.setState 。它也是 React 组件 API 合理的一部分，我并不想是让你觉得应该禁用它。比如，在用户输入时，不需要把每个按键都暴露给整个应用，应该保存自己的状态，在失去焦点之后，输入值会被派发到其它地方存储起来。这种场景是最近一个同事提到的，我觉得这个例子非常恰当。 为组件添加状态还是需要慎重。一旦你开始了，就很容易再加一个状态，不知不觉就不受你控制了。 5. 使用 Redux.js上面第一点就已经说过，React 只是一个视图库。那么问题来了，“状态和逻辑放哪儿？” 我很高兴你会这么问！你可能已经知道 Flux ，一种设计 web 应用的模式，在 React 开发中较为普遍。已经有几个基于 Flux 思想的实现，但是毫无疑问我推荐使用 Redux.js 。 我在考虑写一篇单独的博客，关于 Redux 的特性和优点。目前我推荐你读下官方文档，在这儿我只简单描述下它的工作原理： 组件上的 UI 事件触发时，它们执行属性上传入的回调函数。 这些基于事件创建的回调函数派发 actions Reducers 处理 actions 并计算新的状态 整个应用的新状态流入单一的 store 组件接收新状态作为属性，在需要时重绘 上面的这些概念并非 Redux 独创，但是 Redux 的实现比较清晰简单。从 Alt.js 切换到 Redux ，减少了很多代码量，这儿简单列出比较突出的优点： reducers 是纯函数，简单的 oldState + action = newState。每个 reducer 只处理一部分状态，这些状态可以组合起来。这么做，所有的业务逻辑和状态转换很容易测试。 API 很少，很简单，文档清晰。非常容易学习这些概念，因此很容易理解项目中 actions 和 数据 的流动过程。 按照推荐的方式使用，只有很少的组件依赖 Redux ; 其它的组件只接收状态和回调作为属性。这么做可以保持组件非常简单，减少框架同步。 这儿有几个库配合 Redux 非常爽，我也推荐你使用： Immutable.js JavaScript 不可变数据结构！用它存储你的状态，可以确保状态不会在不该改变的时候改变，并且能够保障 reducer 足够纯洁。 redux-thunk 当你的 actions 不只是更新应用状态，还有其他副作用时，就派上用场。比如，调用 REST API，设置路由或者派发其他 actions reselect 用于可组合的，懒计算的情形。例如，对于部分组件，你可能想要: 只注入整个状态树的相关部分，而非整个 注入额外的衍生数据，比如总数或验证状态，而不需放在 store 中 没有必要在最开始的时候就把这些全部引进来。当你开始有状态时，就可以引入 Redux 和 Immutable.js，有派生状态时引入 reselect，有路由或异步 actions 时引入 redux-thunk。尽早在必要时引入可以省去之后重构的时间。 Redux 是不是真正的 Flux，每个人都有自己的见解。个人觉得它符合 Flux 框架的核心思想，不过这个争论只是个语义问题。 6. 一直使用 propTypespropTypes 很容易为组件添加类型安全保障。他们看起来像这样： 1234567891011121314const ListOfNumbers = props =&gt; ( &lt;ol className={props.className}&gt; { props.numbers.map(number =&gt; ( &lt;li&gt;{number}&lt;/li&gt;) ) } &lt;/ol&gt;);ListOfNumbers.propTypes = { className: React.PropTypes.string.isRequired, numbers: React.PropTypes.arrayOf(React.PropTypes.number)}; 在开发阶段（生产不会），如果任何组件没有给到必需的属性，或者所给的属性与声明的不匹配，React 会打印这些错误信息通知你。这有几点好处： 防止低级错误，捕获 bugs 如果你使用 isRequired，你就不需要检查 undefined 或 null 就像文档所说，列出组件的所有属性，省去阅读代码的人搜索整个组件。 上面的这些点，你可能似曾相识，静态类型支持者的论点。个人来讲，我通常喜欢动态类型带来的开发速度和舒适，但是我发现 propTypes 可以毫不费力的为我的组件添加一些安全感。坦白讲，没有理由不一直用它们。最后一点是，任何 propType 错误时，让你的测试用例失败。下面这个例子有点简单粗暴的，不过可行： 12345beforeAll(() =&gt; { console.error = error =&gt; { throw new Error(error); };}); 7. 使用浅渲染测试 React 组件依然是有点棘手的话题。不是因为太难，而是因为还在发展，还没有出现一个最佳方案。目前来看，我的 go-to 方法是使用 浅渲染和属性断言。 浅渲染很好用，它允许你完整的渲染一个单一组件，而不涉及子元素的渲染。也就是说，结果对象只会告诉你子元素的类型和属性。这样子单一组件单一时间点可以提供很好的隔离。这儿有三种类型的组件单元测试，我自己也经常这么做： 渲染逻辑假定一个组件，因条件不同，可能会显示一张图片，或者一个加载图标： 1234567const Image = props =&gt; { if (props.loading) { return &lt;LoadingIcon/&gt;; } return &lt;img src={props.src}/&gt;;}; 我们可以这么测试： 12345678910111213describe('Image', () =&gt; { it('renders a loading icon when the image is loading', () =&gt; { const image = shallowRender(&lt;Image loading={true}/&gt;); expect(image.type).toEqual(LoadingIcon); }); it('renders the image once it has loaded', () =&gt; { const image = shallowRender(&lt;Image loading={false} src=&quot;https://example.com/image.jpg&quot;/&gt;); expect(image.type).toEqual('img'); });}); 非常简单！当然，浅渲染的API 略微比我展示的复杂。上面使用的浅渲染函数是我们自己的 辅助方法，这个辅助方法包装了真正的 API，使用起来更简单一些。回头看下我们的 ListOfNumbers 组件，下面是我们如何测试映射结果确实正确： 1234567describe('ListOfNumbers', () =&gt; { it('renders an item for each provided number', () =&gt; { const listOfNumbers = shallowRender(&lt;ListOfNumbers className=&quot;red&quot; numbers={[3, 4, 5, 6]}/&gt;); expect(listOfNumbers.props.children.length).toEqual(4); });}); 属性转换在最后的例子中，我们深入测试组件的子元素，确保它们被正确渲染。我们不止断言组件是否存在，同时检查所给的属性是否正确。当组件确实在传递属性之前根据属性做些转换时，这点特别有用。例如，下面这个组件接受一个字符串数组作为 CSS 类名，往下传递一个单引号空白分割的字符串： 123456789101112131415161718const TextWithArrayOfClassNames = props =&gt; ( &lt;div&gt; &lt;p className={props.classNames.join(' ')}&gt; {props.text} &lt;/p&gt; &lt;/div&gt;);describe('TextWithArrayOfClassNames', () =&gt; { it('turns the array into a space-separated string', () =&gt; { const text = 'Hello, world!'; const classNames = ['red', 'bold', 'float-right']; const textWithArrayOfClassNames = shallowRender(&lt;TextWithArrayOfClassNames text={text} classNames={classNames}/&gt;); const childClassNames = textWithArrayOfClassNames.props.children.props.className; expect(childClassNames).toEqual('red bold float-right'); });}); 对这样方法最多的批判是激增的 props.children.props.children 。当然这不是最完美的代码，个人觉得如果一个测试中的 props.children 多的让人受不了，这说明这个组件太大了，太复杂了，嵌套太深。它可能需要拆分。另外一点，我经常听说的是，你的测试太依赖你的组件内部实现，以至于稍微改变你的 DOM 结构都能导致你所有的测试崩溃。这的确是一个很公正的评论，脆弱的测试套件是每个人想要的最好件事。管理这些最好的方式是保持你的组件足够小，足够简单，应该控制因组件变更引起的测试崩溃数目。 用户交互当然，组件不止展示，还有交互： 123const RedInput = props =&gt; ( &lt;input className=&quot;red&quot; onChange={props.onChange} /&gt;) 这是我最喜欢的测试方法： 123456789describe('RedInput', () =&gt; { it('passes the event to the given callback when the value changes', () =&gt; { const callback = jasmine.createSpy(); const redInput = shallowRender(&lt;RedInput onChange={callback}/&gt;); redInput.props.onChange('an event!'); expect(callback).toHaveBeenCalledWith('an event!'); });}); 这只是一个例子，希望你能受到启发。 集成测试上面我内容只覆盖到组件的独立的单元测试，但是你可能想确保你的应用各个部分协同工作，想在测试上走的更远。我对这部分了解的不够深入，但是列出一些基本点： 渲染你的整个组件树（而非浅渲染） 访问 DOM （使用 React TestUtils 或 jQuery 等等）找到你最关系的元素，然后 断言元素的 HTML 属性和内容 模拟 DOM 事件，然后断言产生的效果（DOM 或 路由变化，AJAX 调用等等） 关于测试驱动开发一般情况下，写React 组件时我并不使用测试驱动开发。 在开发组件的时候，我发现我经常会去改动它的结构，我需要使用最简单的 HTML 和 CSS，在需要支持的浏览器上保持一致。因为我的组件单元测试方法大多会断言组件的结构，而测试驱动开发会使我在修改DOM时，忙于修复测试用例，这看起来有点浪费时间。另外一个因素是组件足够简单以至于测试优先的有点基本消失。所有复杂的逻辑和转换都会丢在 action creators 和 reducers，这些地方我能真正享受到测试驱动开发带来的便利。关于测试还有最后一点需要说明。整节内容我都在讨论测试组件，是因为测试基于 Redux 应用的其它部分没有特别之处。作为一个框架，Redux 背后还有些 ‘魔力’ ，可以减少对 mock 和其它测试模板的依赖。每个函数只是一个普通的函数（大多数是纯函数），测试起来真是如沐春风。 8. 使用 JSX, ES6, Babel, Webpack 和 NPM只有 JSX 是 React 特有的。对我来说，JSX 是 React.createElement 的无脑操作。唯一的不足是增加了构建的复杂度，这个问题可以用 Babel 轻松搞定。既然用了 Babel，那没理由不用 ES6 特性，像 常量，箭头函数，默认参数，数组和对象解构，延展和 rest 操作，字符串模板，迭代器和生成器，模块系统，等等。只要你花一点时间设置这个工具，你就能感受到 JavaScript 语言越来越成熟。让我们做的更全面一些，使用 Webpack 打包代码，使用 NPM 管理包。现在我们完全赶上了 JavaScript 的潮流 :)。 9. 使用 React 和 Redux 开发工具谈到工具，React 和 Redux 的开发工具太赞了。React dev tools 让你审查 React 元素的渲染树，在查看浏览器中结果时相当有用。Redux dev tools 更是让人眼前一亮，让你看到每个 已经发生的 action ，它们引起的状态变化，甚至给你回退的能力！你可以作为开发依赖，或者浏览器扩展的形式使用。你也可以用webpack设置热切换，保存代码时你的页面也会跟着更新-浏览器无需刷新。在调整组件和 reducers 时可以立即看到效果，大大提高开发效率。 就这些！我希望在 React 入门上能带给你一个好的开端，帮你避免一些常见的错误。如果你喜欢这篇文档，可以关注我的Twitter 或者订阅我的 RSS 。","link":"/2016/07/18/novice/%E3%80%90%E7%BF%BB%E8%AF%91%E3%80%91React-js-%E5%88%9D%E5%AD%A6%E8%80%85%E5%BA%94%E8%AF%A5%E7%9F%A5%E9%81%93%E7%9A%84-9-%E4%BB%B6%E4%BA%8B/"},{"title":"前端公共资源完全共享的畅想","text":"在每个前端项目中，多多少少都会依赖一些第三方资源，比如Query，React，Angular，core-js，babel-runtime；如果我们能够尽最大程度的复用这些资源，那么我们就能够节约很大的首次请求成本，无需绞尽脑汁即可显著提升首屏首次加载速度。另外提升 webpack 构建速度，扯这么多构建优化方案，就设置 externals 效果最显著； 问题由来前端资源如果可以尽可能的使用公共 CDN 上资源，但是集团内的公共资源太少，只有一些主流的框架或库，这就导致各个团队可能会维护一份自己的公共资源 CDN。现实中使用这些 CDN 略微麻烦，比如新起一个项目就需要这么以下步骤： 看下项目中依赖的哪些流行库可以在 CDN 上找到，譬如 react，先把这个脚本插入在页面模板。开发环境需要一些警告信息，所以最好不要使用压缩脚本。 12&lt;script src=&quot;//cdn.bootcss.com/react/15.0.1/react-with-addons.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;//cdn.bootcss.com/react/15.0.1/react-dom.js&quot;&gt;&lt;/script&gt; 生成环境的模板需要压缩过的脚本： 12&lt;script src=&quot;//cdn.bootcss.com/react/15.0.1/react-with-addons.min.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;//cdn.bootcss.com/react/15.0.1/react-dom.min.js&quot;&gt;&lt;/script&gt; 因为我们是使用 webpack 构建，所以，还需要配置 externals；形如 1234567externals: { react: 'React', 'react-dom': 'ReactDOM', 'react-addons-css-transition-group': 'React.addons.CSSTransitionGroup', 'react-addons-shallow-compare': 'React.addons.shallowCompare', 'react-addons-transition-group': 'React.addons.TransitionGroup',}, 在上面 react-addons 的资源如果不指明，很可能出现某个第三方的组件库引入 react-addons，导致 react 依旧被打包在一起。 维护需要人工成本。 如果哪一天要升级 react 版本，看到 CDN 上有新版本，还要把模板引用的版本号给改掉。 使用工具消灭重复劳动如果我们使用用 webpack 为什么不用一个插件解决这些呢？但是，但是这儿有一个问题：cdn 资源没规则，从 package.json 中完全看不出； 模块的依赖有可能已经打包在一起；如 react-router，将 依赖的小文件一起打包。 依赖不明确；如 bootstap 依赖 jquery 是在 dependencies 中指定的，而 react-router 其实只需要外部的 react，在 peerDependencies 中声明的。 现有的资源路径规则也不确定，如 bootcdn.cn 上 min.js 存在 -min.js 和 .min.js。 好吧，那我就自己维护一份，cdn-webpack-plugin 就是这个插件的雏形。为主流构建工具提供插件，也能够推动资源的共享。 理想与现实理想如果实现了，那前端资源无需 npm install，直接引用 CDN 就可以了。另外我们还能够做一些优化：根据资源共同出现频率选择合适的资源 combo如：react 全家桶 用到 redux 系列， redux react-redux react-thunk。 现实是，我们不可能维护所有资源的所有版本，不可能取代 npm 或 yarn。也不会如它们这么及时，一个第三方库 fix bug 升级，不大可能立即同步到 CDN 上。最大的问题还是第三方的资源不可信，除非几个大公司牵头组织一个第三方的资源库。","link":"/2017/01/02/novice/%E5%89%8D%E7%AB%AF%E5%85%AC%E5%85%B1%E8%B5%84%E6%BA%90%E5%AE%8C%E5%85%A8%E5%85%B1%E4%BA%AB%E7%9A%84%E7%95%85%E6%83%B3/"},{"title":"前端框架的最佳实践（AngularJS 和 React）","text":"对框架的使用者来说，谈一个框架的思想，其实不如谈谈它的最佳实践。就是同一个框架，不同的人实现相同的功能，代码也是千差万别。不同风格的实现自然也有高低之分，而往往这些东西还没有成文的规定，不知道坑了多少前端。这些框架还有另一个特点—–非常容易上手（入坑），从而埋下更深的祸患。这么简单！其实往往是不明所以的时候就开始挖坑了。 2015年初时计划切换框架，因为一直在做的都是管理后台类项目，所以理所当然地投奔到 AngularJS 阵营了。在做切换到 AngularJS 时，需要有一个思维上的转变，原来对 DOM 的操作要转为操作数据，数据发生变化了，界面就会跟着变化。用过 AngularJS 的同学估计都记得一些建议和警告： 不要使用 jQuery (不合时宜地操作 DOM) 尽量避免 global scope 保持瘦 Controller …(快一年没写了，不记得了–!) 其中一些很容易理解，表示的很清楚，这些还好（表示还是都见到 😞 ）。另外一些就很难说了，比如： ng-init 这个标签，有人不顾警告地使用，我是万万没想到，Controller 初始化的部分逻辑要看模板才知道。 类似的情况，实现一个 directive 做拉取数据的逻辑。复用代码的方式有很多，何必这样呢？你这样做数据流还怎么管理？告诉我！ 模板里面写很长很长的表达式。对于模板，一定不要做太多事，一定不要写太多逻辑。流过这么多泪之后，个人非常推崇 Logic-less template。后来在 React 中有人吐槽 JSX {} 中不能写 if 语句，只想说你们真没痛过么？ 关于 AngularJS 现在想起的也就这么多了，项目搭建好后，后端也能撸代码。不过，你得遵照一点要求：取数据在一层；处理数据在 service；controller 简单的调用函数，挂数据在 scope 上；模板保持简单。后端 MVC 中不是也要 保持瘦 controller 的，逻辑丢在 service 么，你前端写这么乱，感情你后端代码也是一粪池？ 最开始接触 React 是在 2015 年，最开始的印象是： JSX 好奇怪，只是做了渲染。今年年初接触 React 时，看到当时项目时，感觉怎么比 AngularJS 还麻烦还难读。当时是纯 React，不难想象到代码结构，一个组件负责一个页面，拉取数据，处理数据，显示对话框，各种逻辑都在里面，数据都在 state 上，一个文件七八百行不成问题。于是想拆，按照 Angular 的思想，取数据和处理数据，总该挪出去吧。可是文件还是很大，于是想找找类似的最佳实践。首先是官网的 thinking in react ，你这是逗我么？这简直是 react hello world。 数据流这么乱，先套上 redux 吧（发明 flux 的人一定是被 AngularJS 的数据流搞懵逼过）。在 redux 中倒是有了一点发现，react-redux 入门文档说 深受 分离容器组件和展示组件 思想启发。姑且叫这么个中文名吧。之后翻译了 React.js 初学者应该知道的 9 件事，开始明白纯渲染组件的好处，官网的文章也不是在逗我。 React 开发最佳实践（一句话攻略）就是：写简单可靠的纯组件，然后使用搭积木的方式组合这些小组件搭起一个页面。 之后当有新人要上手做项目时，关于 React，我真不知道要说些什么。那就说说 redux，要怎么写，怎么维护数据流，保证你的组件简单。对于新人来说，上手也确实容易，最开始甚至不需要搞清楚数据流那一套，只需要告诉他怎么写怎么写就可以了。 功能实现很简单，当我看代码时，总是觉得别扭，我不会这么写。第一个问题是，用继承复用相同逻辑，可是如果我来写的话，只会想到组合，因为逻辑相同的是父容器呀。后来，又有同事问可否有多个父类，我很好奇是什么场景，为什么不用组合。因为这么久的项目只有极个别的地方用到了继承。尽量选择组合，官网也是一样的态度，可以看下官网上 组合与继承 最后一段话。Mixins 毫无疑义被 HOC 取代，react-redux 中的 connect 就是 HOC 一例。相关文章 Mixins Are Dead. Long Live Composition （还是他的） 优先组合这个还好说，如何抽象一个组件这个就麻烦了，现实中比较多碰到的是，组件很庞大，越写代码越长，每个组件都尽量拆成纯函数组件不是每个人都能（愿意）做到的。功能都实现了，上线了再说，谁还管这个啊。随便翻翻 github 上的 React 组件库，一个组件几十个属性，代码五百行起的超级组件，你让我怎么看啊！ 最后一个小问题也是跟组件抽象有关，没有仔细思考组件的输入和输出。说好听点是封装组件的时候欠斟酌，不好听的话是欠缺抽象封装组件的能力。表象是往往丢给一个组件不需要的属性，或者不需要的多个属性，这个组件其实依赖的是一个处理完成的一个名字，而不是把几个揉在一起。对这类问题，想象一下这个场景，团队中的另外一个人想复用你的组件，你还会暴露这么些属性和方法么？这类问题太常见了，自己调自己的业务组件，想这么多还上线么？ 说 React 容易的人，大多是在忽悠人入坑，就像忽悠别人学前端一样。我至今还是不知道怎么教新人写好 React ，这个真心不好教啊。 对于新人，我只能说：前面坑多，少一点浮躁，沉下心来学一段时间再说话。 很早就想写一篇这样的文章，拖了很久，关于 AngularJS 的部分忘太多了。新年第一篇 🎉 🎉 🎉","link":"/2017/01/01/novice/%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%EF%BC%88AngularJS-%E5%92%8C-React%EF%BC%89/"},{"title":"就一个静态页面","text":"周二晚上邮件收到一个需求：下周有个活动页面，就一个静态页面。打开设计稿一看，一个秒杀活动页面，与设计对了一下，发现有个动画要做。之后需求方描述了一遍：倒计时结束，按钮可点，跳转到订单页面。后来，开发一对需求，发现秒杀后面的很多逻辑都没考虑： 时间同步问题 验证码问题 订单有效期 是否影响正常售卖 其它业务逻辑问题… 就目前情况考虑，参与活动的人并不多，而且事急从权，可以简单处理。 时间就先取服务器时间。 验证码，先找个 npm 包 ccap，随机生成一段文本和对应的图片。服务器端在返回验证码图片时，将原文本保存在 session 中。创建订单时检测验证码与 session 中的信息是否匹配。那么问题来了，我们的服务器肯定不止一台，session 是怎么同步的？看看 session 模块好像没有这些逻辑啊，最后 session 是使用了 tair 同步 session。验证通过之后，应该清除 session 中的 验证码信息。如果验证请求是同时发出的，读取 session 内容比较，肯定都是通过的，这就有问题了。也就是说，读 与 清 必须是一起的。这就得用锁了，读取的时候加锁，完成后，释放锁。做完这些，验证码这块还留有问题，上面提的那个 ccap 包，每次重新创建图片的方式肯定太慢，最好是提前生成一批，定期更换。 订单有效期的问题：后端需要一个任务定时检查订单是否过期释放库存。 正常售卖最好与秒杀分开，就算秒杀出什么问题，也不影响正常售卖。 实际的秒杀活动中，为了减少服务器压力，在请求的最开始就直接随机拒掉一批请求了。所以，手黑是有原因的。 这就是需求方说的：就一个静态页面。","link":"/2016/05/15/novice/%E5%B0%B1%E4%B8%80%E4%B8%AA%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2/"},{"title":"探秘 MobX","text":"MobX 是最近在 React 社区比较火的状态管理工具。与 Redux 相比，Mobx 简单又神秘。不止是因为 MobX 比较火，MobX 的双向绑定与 Vue 的实现也是非常相似，十分有必要去了解一下双向绑定的实现。这篇文章通过源码解释 MobX 这些奇怪的“特性”。就像 Redux，MobX 跟 React 也没有关系，我们从最最简单的例子开始。MobX 版本 3.1.7 observable123const store = observable({ title: 'front end developer',}); observable 返回一个新的对象 ref，包含一个 $mobx 属性，$mobx 是一个 ObservableObjectAdministration 对象。这个对象稍微有点复杂，不过其实就是 Object.defineProperty 拦截 getter 和 setter，我们暂不需要弄懂 getter 和 setter 究竟做了什么事情。values.title 是一个 ObservableValue 对象，我们大概可以猜到它的属性 observers 存放了观察 values.title 变化的对象。 123456789101112131415161718192021222324252627{ title: 'front end developer', $mobx: { // ObservableObjectAdministration name: 'ObservableObject@1.user', target: ref, // 指向新返回对象 values: { title: { // ObservableValue value: 'front end developer', name: 'ObservableObject@1.title', observers: [], get() { this.reportObserved(); return this.value; }, set: function(v) { setPropertyValue(this, propName, v); }, }, }, }, get title: function() { return this.$mobx.values[propName].get(); // 这儿的 propName }, set title: function(v) { setPropertyValue(this, propName, v); },} 上面只是一个最简单的对象，如果对象复杂点，也会递归的包装对象。 autorun在一个函数中简单输出 store.title，我们跟踪下 autorun 方法。 123autorun(() =&gt; { console.log(store.title);}); autorun 方法会使用传入的参数创建一个 Reaction 对象 reaction， 然后调用 reaction 的 schedule 方法。 1234567891011121314151617181920{ name: 'Reaction@2', onInvalidate() { function reactionRunner() { view(reaction); // view 传入的匿名函数 } this.track(reactionRunner); }, observing: [], newObserving: [], dependenciesState: IDerivationState.NOT_TRACKING, __mapid: '#3', diffValue: 0, runId: 0, unboundDepsCount: 0, isDisposed: false, _isScheduled: false, _isTrackPending: false, _isRunning: false,} schedule 方法将 reaction 添加到 pendingReactions，最后执行每个 reaction 的 runReaction 方法。 123456789runReaction() { startBatch(); this._isScheduled = false; if (shouldCompute(this)) { this._isTrackPending = true; this.onInvalidate(); // 这个就是初始化用匿名函数构造的一个方法。 } endBatch();} onInvalidate 调用 track， track 调用 trackDerivedFunction，跟踪下 trackingDerivation 123456789101112131415function trackDerivedFunction(derivation, f, context) { // ... derivation.runId = ++globalState.runId; const prevTracking = globalState.trackingDerivation; // 保存当前 trackingDerivation globalState.trackingDerivation = derivation; // 设置全局 trackingDerivation let result; try { result = f.call(context); // 执行我们传入的匿名函数，newObserving } catch (e) { result = new CaughtException(e); } globalState.trackingDerivation = prevTracking; bindDependencies(derivation); // 重新收集依赖 return result;} 这个函数在执行我们的匿名函数之前，设置全局 trackingDerivation 为当前的 reaction ，执行之后又设置回原来的变量。在我们的匿名函数中打印 store.title，回想下最开始 ObservableValue 对象在解析 store.title 值时会调用 reportObserved 。 1234567891011function reportObserved(observable) { const derivation = globalState.trackingDerivation; if (derivation !== null) { if (derivation.runId !== observable.lastAccessedBy) { // 简单优化 observable.lastAccessedBy = derivation.runId; derivation.newObserving[derivation.unboundDepsCount++] = observable; } } else if (observable.observers.length === 0) { queueForUnobservation(observable); }} 这儿的 derivation 就是我们的 autorun 创建的 Reaction 对象。到现在我们瞧出了一下端倪，在一个 Reaction 环境中解析值，则该 Reaction 依赖该 observable 对象。注意这儿并没有直接放到 observing 数组中！执行完当前方法之后，在 bindDependencies 才重新设置了 observing。这一步是必需的，考虑下 autorun 中有条件语句的情景，除了条件语句，如果我们的对象稍复杂点，譬如 store.user.title，我们对 store.user 重新赋值就会改变依赖的 observable。最后分析下下面的代码片段，猜测输出几次： 1234autorun(() =&gt; { console.log(store.title); store.title = 'hello world!';}); 你会发现输出一次，因为初次执行时reaction.observing 为空，执行完之后才会根据 reaction.newObserving 更新 observing。再在外面修改 store.title = ‘changed title’，这次就会正常的输出 changed title 然后输出 hello world! bindDependencies 设置新的 observing 后，还同步更新依赖的 ObservableValue 的 observers，store.title 被哪些依赖到需要更新。在 store.title 发生变化时，setPropertyValue 会触发 propagateChanged 方法 12345678910111213function propagateChanged(observable) { if (observable.lowestObserverState === IDerivationState.STALE) return; observable.lowestObserverState = IDerivationState.STALE; const observers = observable.observers; let i = observers.length; while (i--) { const d = observers[i]; if (d.dependenciesState === IDerivationState.UP_TO_DATE) d.onBecomeStale(); // call shedule d.dependenciesState = IDerivationState.STALE; }} observable 变化时，调用 observers 中每个对象的 onBecomeStale 方法，对 Reaction 对象来说 onBecomeStale 简单的调用 shedule；对 ComputedValue 对象来说则会执行 propagateMaybeChanged，这儿有些优化如果 Reaction 对象状态不是已经更新（ UP_TO_DATE），什么都不做。为说明这个问题，我们造一个例子： 123456789101112const store = observable({ a: 3, b: 4, get sum() { return this.a + this.b; },});autorun(() =&gt; { console.log(store.sum); store.b = 5;});store.b = 6; 上面这个例子只输出一次，注释掉 autorun 中的 store.b = 5 赋值语句，结果当然会输出两次。如果该赋值语句丢在另一个 autorun 中则会输出四次。不知道该如何解释。 结语MobX 的反应系统不难理解，从图中的线条就能看出来。分析源码可以帮助我们搞明白在pixel paint这个例子中为什么它会这么高效，在以后的项目中我们也可以受到启发。双向绑定主要是依赖收集，理解起来比较简单，但是关于性能优化部分的分析本篇文章没有提及，有兴趣的同学可以深入研究一下。","link":"/2017/03/30/novice/%E6%8E%A2%E7%A7%98-MobX/"},{"title":"HMR 踩坑记","text":"昨天遇到一个问题：使用 next/tree 时报错，即使最简单的 Demo 也会报错，而官网提供的则不会，最终问题定位到 react-hot-loader 上。我使用的是 react-hot-loader@next版本，需要在 babel 配置中引入 react-hot-loader/babel plugin。禁用这个 plugin 正常渲染，启用就报错，这个问题太诡异了，所以有必要搞清楚 HMR。 了解 HMR热替换是 webpack 的很炫酷的特性，简直就是黑魔法般的存在。其实原理很简单，见下图（一直觉得别人画的图很厉害，虽然经常看不懂）： webpack-dev-server 服务启动之后与 webpack 建立连接，本地发生变化通知 webpack-dev-server，页面注入 devServerClient.js 脚本保持与 webpack-dev-server 通信。本地发生变化之后，通知浏览器，浏览器拉取最新的变化模块。 浏览器端维护着一个 modules: {} 集合，当某一个模块变更时，我们需要重新执行该模块，缓存模块对象。接下来就是如何重新渲染页面了，我们应该都见过下面这段代码。 123456if (module.hot) { module.hot.accept('./App', () =&gt; { // ... render(); });} 这段代码可以理解问监听模块变化，重新渲染页面。得益于消息冒泡，我们只需要监听最顶层的模块就可以了。重新渲染页面，对于 React 应用来说可以用这种方式： 12345678const render = () =&gt; { const nextApp = require('./App').default; // ...};setTimeout(() =&gt; { ReactDOM.unmountComponentAtNode(MOUNT_NODE); render();)); react-redux-starter-kit 就是这种方式。对于样式文件，sytle-loader 替我们做了热替换。我们在代码中引入样式文件： 1import './Demo.css' 实际上是这样的： 热替换更顺滑大多数的项目示例都是上面这种形式重新渲染页面，用户并不会体会到页面重刷的感觉。不过如果组件有自己的内部状态，这个状态肯定会丢失。为了将替换体验做的更顺滑，Dan Abramov 开发了 React Hot Loader 可以让更新组件时内部状态不丢失。react-proxy 会包装每个组件类，组件发生变化，只是实例原型上的方法改变，并不影响组件实例本身，只不过再次执行的是新的方法。 了解了这点之后，再检查下 next/tree 的代码就会发现，在 TreeNode 组件中包含有这样的代码逻辑：item.type === TreeNode 或者 children.type === TreeNode 来检查如果 children 是否是 TreeNode 类型。现在因为 react-proxy 包装了原来的组件类， children.type 类型其实是 ProxyComponent，虽然有着相同的属性和原型链，但其实是完全不同的对象，所以 children.type === TreeNode 结果就是 false。解决方法：我们在需要比较组件类型时，可以比较该类上的一个标识，或者比较原型链来绕过这个坑。 另外还有一点值得提下，HMR 的粒度是模块，因此如果在一个模块文件中创建几个类，其实里面的类型是不会被代理的。也就享受不到 react-hot-loader 带来的好处。 react-hot-loader 应用情况react-hot-loader beta 版本发布了很长时间，并未见到流行项目中用到，create-react-app，react-redux-starter-kit，dva， react-boilerplate。如果项目中用 redux 来管理 store，组件很少维护数据状态，确实这个特性并没那么大的吸引力。 相关文章： 探究Webpack中的HMR(hot module replacement) React Native 热加载（Hot Reload）原理简介 Webpack &amp; The Hot Module Replacement Webpack’s HMR &amp; React-Hot-Loader — The Missing Manual","link":"/2017/06/25/obsolete/HMR-%E8%B8%A9%E5%9D%91%E8%AE%B0/"},{"title":"Promise 被玩坏了","text":"收到产品同学反馈的一个 bug：在 iOS 上，进入首页之后很快滑动，再点击切换到第二个页面会一直处于loading状态，可以稳定复现。拿自己的手机试了几次果然可以复现。在模拟器上准备调试，打开控制台并未看到错误。于是猜想有异常没有处理，检查代码是否遗漏： 1234showLoading();fetch(url).then(() =&gt; {}, () =&gt; ([])).then(() =&gt; { hideLoading();}); 对这段逻辑还是不放心，手动在 fetch 之后的 onsuccess，onerror 中打印调试信息，发现两个方法都没有调用。因为 Safari 不支持 fetch，开始怀疑引用的 fetch 有问题，加上调试信息： 123456789101112self.fetch = function (input, init) { return new Promise(function (resolve, reject) { var xhr = new XMLHttpRequest(); // ...省略部分代码 xhr.onload = function() { // ... console.log('---------&gt;', input); resolve(new Response(body, options)) }; // ...省略部分代码 });}; 一切正常 resolve 响应内容，但是就是不处理之后的 then 方法。手动在控制台敲： 1new Promise(resovle =&gt; { resovle('haha'); }).then(data =&gt; { console.log(data); }, () =&gt; { console.log('wwwwwww'); }) 不打印任何结果。检查 Promise 确认是原生对象 12Promise// function Promise() { [native code] } 之前看过的 Promise 实现是基于 setTimeout 实现，再测 setTimeout 是否正常 123setTimeout(() =&gt; { console.log('it works');}, 10); 结果正常工作。既然是 Promise 有问题，就把原来的注释掉，换用 core-js 的实现，虽然 bug 照样复现，但好歹我们可以调试了。再次执行上面测试 Promise 的代码，发现 Promise 交给 mocrotask 执行时，压根未执行。关键代码： 12345678910111213141516171819202122232425262728293031323334353637var Observer = window.MutationObserver || window.WebKitMutationObserver;module.exports = function(){ var head, last, notify; var flush = function(){ var parent, fn; console.log('&lt;---- flush'); while(head){ fn = head.fn; head = head.next; try { fn(); } catch(e){ if(head)notify(); else last = undefined; throw e; } } last = undefined; if(parent)parent.enter(); }; var toggle = true , node = document.createTextNode(''); new Observer(flush).observe(node, {characterData: true}); // eslint-disable-line no-new notify = function(){ console.log('notify----&gt;'); node.data = toggle = !toggle; }; return function(fn){ var task = {fn: fn, next: undefined}; if(last)last.next = task; if(!head){ head = task; notify(); } last = task; };}; 调试发现不知名的原因导致 head 不为空，追加任务只能追加在队列后面。手动调 flush 一下就会发现之前所有的未执行的任务都完成了处理。现在的问题就停留在是什么原因导致的 head 不为空。按说上面的代码确保队列里的任务最终都被处理， notify 之后都会触发 flush （注意不是说每个 notify 都会触发 flush，可能多个 notify 触发一次 flush ）。添加调试信息发现，某些情况下根本没有触发 flush，导致任务阻塞，Promise 被玩坏。 查看了其它 Promise （bluebird 以及 ES-promise）的实现，如果支持 MutationObserver 都会使用 MutationObserver，不支持才会降级到 setTimeout 。有人提示 Vue 的异步队列也用到了这个方案。 如何复现这种情况，调试时排除了路由的原因，只要在加载的时候滚动就会出现不触发 flush 的情况。尝试做了一个 demo 但是未能复现，google 了一下也未搜到 MutationObserver 相关 bug，只有一个类似 issues 。 当然禁用 MutationObserver 可以绕过这个问题。 如何验证这个问题，iOS 10.2 上（低版本 setTimeout 没有这个问题），对请求比较多的页面，未加载完成时就疯狂操作，争取能够稳定复现。","link":"/2017/01/19/obsolete/Promise-%E8%A2%AB%E7%8E%A9%E5%9D%8F%E4%BA%86/"},{"title":"也谈 JS 模块","text":"跟大多数人一样，在最初接触JS时，没有考虑过模块的概念。当工作中前端做的事情越来越复杂，JS 越来越庞大，就必须好好考虑组织 JS 代码了。了解其它语言的人，可能会惊讶 JS 没有模块。当每个人都在做这件事情的时候，就不得不归咎到语言问题上了，或许在以后的规范中将加入对模块的支持。不过现在的我们还是有必要了解下怎么组织模块和懒加载的思想。 闭包和命名空间解决方案当项目越来越复杂时，在项目开发中必须保证变量不冲突，当然最该考虑的就是约定，大家遵守同一个命名规范。不过总会用到别人的代码，外部代码就不一定遵从同一个规范了，为了避免变量冲突，最简单的处理，我们可以把代码包在一个匿名函数中。就像这样： 1234567(function () { //变量 函数 var privateMember; function privateMethod () { //.... }})(); 如果我们的其它模块需要访问该文件中的变量或函数，我们可以这样： 123456789var m1 = (function ($) { var privateMember; function privateMethod () { } return { publicMethod: privateMethod };})(Jquery); 或者直接交给全局： 12345678910(function (global, $) { //变量 函数 var privateMember; function privateMethod () { //.... } global.m1 = { };})(global, Jquery); 将全局变量导入模块，作为局部变量解析速度更快。 使用模块模式，建立命名空间，假设项目名称：pandora；用户相关名称 pandora.user，可以这样约定命名规则。 12345678910111213141516171819202122232425262728293031//代码出处：Javascript 模式var pandora = pandora || {};pandora.namespace = function (ns_string) { var parts = ns_string.split('.'), parent = pandora, i; if (parts[0] === &quot;pandora&quot;) { parts = parts.slice(1); } for (i = 0; i &lt; parts.length; i += 1) { if (typeof parent[parts[i]] === 'undefined') { parent[parts[i]] = {}; } parent = parent[parts[i]]; } return parent;}//使用var moudle = pandora.namespace('utilities.array');pandora.utilities.array = (function () { var privateVariable = {}; function privateMethod () { //... } return { publicMethod: privateMethod };})(); 模块化编程上面提到的是命名冲突，还有文件依赖问题。我们引入 Node.js 支持的模块编程，将上面代码改为模块的方式： 12345678910111213141516//代码片段 003 math.jsdefine(function (require, exports) { //变量 函数 var privateMember; function privateMethod () { //.... } exports.publicMethod = privateMethod;});//在另一个模块中 net.jsdefine(function (require, exports) { var math = requeire('math'); math.publicMethod();}); Node.js 的模块系统就是这种方式，只不过不需要用户自己写define, node会对文件进行处理，Node.js 是服务端的应用当然可以这么写，而且读取文件依赖也是同步的。 同步模块实现如果我们要在浏览器端实现上面提到的模块功能，应该解决哪些问题呢？ define 应该实现功能： 定义一个模块，模块 id 按照脚本 url 获取， 同时包括一个 function require 应该实现功能：获取模块，返回的是模块中的 exports。 为避免重复，我们在全局变量 cacheMods 中保存加载的模块，并且保存 exports。每个模块应该是这么一个对象： 12345{ id: uri, factory: function(require, exports){}, exports: *** || {}} 根据上面的分析： define和require的功能伪代码大概是这样的 1234567891011function define (f) { var uri = getCurrentScriptUri(); //该uri是当前脚本路径 var mod = new Module(uri, f); mod.exports = factory(require, mod.exports, mod); cacheMods[uri] = mod;}function require (uri) { var mod = cacheMods[uri]; return mod.exports;} 还有一个问题，当调用require的时候该模块还没加载怎么办？ 先考虑如果是同步加载的话：我们可以这样子，加载该模块代码–简单的说就是插入一个script （src=’uri’）当这个脚本加载完成后，执行define就有了这个模块。 上面的require代码也需要调整一下： 123456789101112function require (uri) { var mod = Module.get(uri); return mod.exports;}Module.get: function (uri) { var mod = cacheMods[uri]; if (!mod) { var mod = new Moudule(uri); mod.load(); } return mod;} 好简单啊！不过load方法必须是同步的，必须检测到依赖的模块已加载完成后才能往下执行。 异步加载模块实现如果我们要采用异步加载的方式，就不能在require时 加载依赖脚本。 在代码片段003我们也能看出，在define的时候，require是不会执行的。我们可以在define的时候指明模块依赖，在define的时候就把所依赖模块下载下来。模块也多了一个属性deps，表明所依赖的模块; 问题是这样强制用户指明依赖，是不是不太好。那我们自己来，分析下factory.toString()中的require (‘’);语句 获取到deps。 这下就简单多了，不过还没完，所依赖的所有模块加载完成后，应该告知模块。并标示该模块的状态。003中的math模块加载完成后应该通知net模块。现在我们的模块应该定义成这样： 123456789{ id: uri, factory: function(require, exports){}, exports: *** || {}, deps: [], status: 0, //标识模块状态 _awaiting: [], //等待该模块的模块队列 _remain: deps.length //当前模块依赖的deps还有几个没加载} 1234567891011121314151617181920212223//模块定义也变得复杂了点function define (deps, factory) { var mod = cacheMods[uri] = new Module(uri, f, deps); mod._remain = deps.length; mod._awaiting = {}; for (var i = deps.length - 1; i &gt; 0; i--) { Module.get(deps[i]).load(); } //如果依赖的模块都已加载完成调用onload if (loadedAlldeps) { mod.onload(); }}function load () { var this; this.fetch();//在页面插入script 脚本加载之后又会调用define}function onload () { //检查 _awaiting 上阻塞的模块 并对激活的模块 调用相应的 onload 方法} 现在我们的模块应该差不多了。上面的 require 方法获取 mod 的 exports，如果多个地方调用 require(‘math’),也应该只有一个 exports，也就是每个模块的 factory 只执行那么一次。我们再稍微改下： 1234567function require (uri) { var mod = Module.get(uri); return mod.exec();}function exec () { return exports || mod.factory(require, mod.exports, mod);} 启动执行第一个模块，我们的入口代码 只需要 require('main') 就可以了。等等，貌似哪里不对。main 模块这个时候还没加载啊。我们可以定义一个依赖 main 模块的 mod，加载这个 12345var mod = new Module(['main']);mod.onload = function() { mod.exec();}mod.load(); 以上描述了 CMD 模块加载器 seajs 的大概实现。seajs 就不用介绍。在 seajs 中 define 是个全局的函数，而 require 是一局部的变量。在 define 的时候并不做加载操作。我们看下 seajs 整个的结构 12345678910111213141516171819202122232425262728function Module(uri, deps) { //没有 factory}Module.prototype.load = function () { //加载依赖模块 //没用下载的模块 调用fetch}Module.prototype.onload = function () { //模块加载完成后的回调，通知_awaiting上的模块，如果所有依赖也完成就调用它的onload}Module.prototype.fetch = function () { //下载脚本，脚本下载完成后回调 load}Module.prototype.exec = funcction () { //执行factory 得到exports function require(id) { return Module.get(require.resolve(id)).exec() }}Module.define = function (id, deps, factory) { parseDependencies();}Module.get = function (uri, deps) {}Module.use = function (ids, callback, uri) { //加载匿名模块} 当然还是有不少差别的，上面我们实现的是简化了很多的。seajs引入模块： 1seajs.use(&quot;examples/hello/1.0.0/main&quot;); 如果懒得提供匿名模块加载方式，我们也能用类似的方式来启动第一个模块： 12345var mod = seajs.cache['main'] = new seajs.Module('main' , ['examples/hello/1.0.0/main']);mod.callback = function() { seajs.cache[seajs.resolve('examples/hello/1.0.0/main')].exec();};mod.load(); 模块周边大概或许就是这样，再来谈下文件合并的问题。CMD规范中一个文件只有一个模块，我们不能直接将所有模块连接到一个文件中。在seajs中定义的匿名模块是根据脚本的uri生成id的。在合并的时候必须指定该uri为id。即将原来的模块定义改成。 123//代码片段 003 math.jsdefine('main.js', [], function (require, exports) {}); seajs文件的合并是用spm-js做的。 最后，提下 smash，d3 用到的一个文件拆分的处理方式。在 JS 中可以这么用 1import &quot;math&quot; 在运行前重新生成合并文件。主要是用 namespace 避免污染变量，这样的好处就是实际生成的 JS 还是原来的 JS 代码，但是在开发的时候各个文件很小，在开发的时候感觉更为清晰些（我们的一个项目中有用过这东东）。","link":"/2014/02/13/obsolete/about-js-module/"},{"title":"AngularJS中的懒加载【翻译】","text":"原文 Lazy Loading In AngularJS 当我们使用 AngularJS 构建一个包含许多路由/视图的大型应用的时候，我们希望在最初加载的时，最好不加载所有的 artefacts，像 controller、directive 之类。理想情况下，最初加载时只加载必需的模块。之后用户改变路由，加载尚未加载的所需模块。这样做的好处不仅会加快页面初次加载速度，而且会节约带宽。这篇文章就介绍了在 AngularJS 架构的应用中如何懒加载 artefacts。 为了实现懒加载 controller 和 directives，首先需要弄清两个问题： 在应用启动之后，如何注册这些 artefacts 什么时候加载这些脚本 第一个问题是因为在应用启动后，不能使用模块 API 注册 artefacts。换句话说，如果我们尝试在已经启动的应用中使用下面的方式注册一个 controller: 123angular.module('app').controller('SomeLazyController', function($scope) { $scope.key = '...';}); 当你使用 ng-controller 指令引用到这个 controller 时，将会出现下面的错误提示： Error: Argument ‘SomeLazyController’ is not a function, got undefined 这时，在一个已经启动的应用中唯一注册 artefacts 的方式不是使用模块 API，而是使用 Angular provider。Providers 往往用来创建和配置 artefacts 的实例。因此为了注册一个 provider，你应该使用 $controllerProvider。同样，使用 $compileProvider 来注册 directive，使用 $filterProvider 注册 filter，使用 $provider 来注册服务。注册 controller 和 directive 的代码大概是这样的： 12345678910111213// Registering a controller after app bootstrap$controllerProvider.register('SomeLazyController', function($scope) { $scope.key = '...';});// Registering a directive after app bootstrap$compileProvider.directive('SomeLazyDirective', function() { return { restrict: 'A', templateUrl: 'templates/some-lazy-directive.html' }})// etc provider 只有在模块配置的时候可用，因此你需要保存一个引用，这样子就可以注册一个 artefact。类似于下面的方式： 123456789101112(function() { var app = angular.module('app', []); app.config(function($routeProvider, $controllerProvider, $compileProvider, $filterProvider, $provide) { app.controllerProvider = $controllerProvider; app.compileProvider = $compileProvider; app.routeProvider = $routeProvider; app.filterProvider = $filterProvider; app.provide = $provide; // Register routes with the $routeProvider });})(); 你就可以用这种方式注册controller： 123angular.module('app').controllerProvider.resgister('SomeLazyController', function($scope) { $scope.key = '...';}); 还有另外一个问题，什么时候加载上面的脚本呢？在 route 的 resolve 属性中可以做到。 使用 $routeProvider 时，你可以指定一个可选的 key/factory 依赖表，这个会注入到 rotute controller 中，依赖表使用 resolve 指定如下： 12345$routeProvider.when('/about', { templateUrl:'views/about.html', controller:'AboutViewController', resolve:{ key: factory }}); 依赖表中的 key 作为依赖的 name，factory 要么是一个字符串，要么是一个函数。字符串作为服务的别名，函数使用返回值。如果函数返回的是 Promise，这个 Promise 在 route 开始渲染之前完成 resolved。这样我们就可以在依赖表中的函数中返回一个加载脚本的 Promise，保证在 route 开始渲染之前加载所依赖脚本。下面的例子中使用 $script 完成脚本加载： 123456789101112131415161718$routeProvider.when('/about', { templateUrl:'views/about.html', resolve:{ deps: function($q, $rootScope) { var deferred = $q.defer(); var dependencies = [ 'controllers/AboutViewController.js', 'directives/some-directive.js' ]; // Load the dependencies $script(dependencies, function() { // all dependencies have now been loaded by so resolve the promise $rootScope.$apply(function() { deferred.resolve(); }); }); return deferred.promise;}}}); 唯一需要注意的是promise的resolve很可能需要在AngularJS的环境中执行，像上面那样。可以使用$rootScope的$apply方法实现。如果不这样做的话，在页面完成加载时route不会开始渲染。 现在模块定义看起来像下面这样： 123456789101112131415161718192021222324252627282930313233(function() { var app = angular.module('app', []); app.config(function($routeProvider, $controllerProvider, $compileProvider, $filterProvider, $provide) { app.controllerProvider = $controllerProvider; app.compileProvider = $compileProvider; app.routeProvider = $routeProvider; app.filterProvider = $filterProvider; app.provide = $provide; // Register routes with the $routeProvider $routeProvider.when('/', { templateUrl:'views/home.html' }); $routeProvider.when('/about', { templateUrl:'views/about.html', resolve:{ deps: function($q, $rootScope) { var deferred = $q.defer(); var dependencies = [ 'controllers/AboutViewController.js', 'directives/some-directive.js' ]; $script(dependencies, function() { // all dependencies have now been loaded by $script.js so resolve the promise $rootScope.$apply(function() { deferred.resolve(); }); }); return deferred.promise; }}}); });})(); 最后，你可以用下面的方式启动应用： 1234// This file will be loaded from index.html$script(['appModule.js'], function() { angular.bootstrap(document, ['app'])}); 以上就是实现懒加载的大概步骤。总之，首先在定义你的 app 模块时，保存相关 provider 实例的引用。然后你应该使用它们注册你自己的 artefacts，之后在定义 route 时，使用resolve返回一个 promise，在其中加载所需的脚本，完成之后 resolve promise，不要忘了在 $rootScope.$apply 里面。最后，加载完主模块后，你应该手动启动应用。 翻译之外可以看下 angular-lazyload，一样的思路! 也有反对的声音：不过并不提倡这种 hack 的方式，因为不符合 Angular 的设计思想。 Angular 将配置和执行独立分开，在配置块之外禁用 providers。这样做可能会导致 bug 和不可预期的结果，因为 AngularJS 在 injdector 创建之后并不期望再去注册 controller 和 directive。当然懒加载是一个很好的想法，不过我们应该等待框架支持而不是用 hack 的方式实现。在 AngularJS 2.0 中将支持这一特性。 之前在介绍启动的时候提过Angular启动： 在使用 module 的 api 时，并木有直接注册 controller，而是丢在了 _invokeQueue中。启动的时候，从队列中取出，完成加载： 1$controllerProvidre.register('controllerName', function () {});","link":"/2014/11/01/obsolete/angular-lazy-load/"},{"title":"初看 avalon","text":"因为需要考虑兼容低版本浏览器，想选择一个更为合适的MVVM框架，需要调研下 avalon，从一个 angular 使用者角度看 avalon（有排斥心理）。 MVVM首先是 MVVM，MVVM 方式的实现中，如何监测数据发生变化是一个问题。angular 实际并没有监听数据变化，在使用 angular 时改变 $scope 上数据，能够立即看到效果，原因是你修改数据的地方都是 angular 提供的，在 controller 内部、$http 回调以及 $timeout 回调，在你修改数据之后，angular 调用了一次$digest，所以数据能够发生变化，这也是为什么不建议你用 $.ajax 、settimetout 的原因。另外，众所周知 angular 脏检测效率低下，假设一个数据发生变化，需要调用相关的处理函数。在处理函数执行完成之后有可能再次引起数据变化，所以就需要再次检测….在这方面 avalon 用 hack 的方式，直接监测数据变化。 avalon 在 vm 上加入 getter setter watch数据变化。对于复杂 model 对象 vm.options.colsdef 最里层的变化就需要你自己监测了。 123// avalon 形如以下方式：vm.options.$watch('colsdef', function () {})// 当然 如果你对 options 添加新的属性，就.... 如果浏览器都支持 Object.observe 的话，MVVM也没这么折腾了。 ms-duplex 等同于 ng-model avalon 加入 data-duplex-observe=&quot;false&quot; 来禁止双向同步。 虽然在 ms-duplex 2.0 引入 ms-duplex-string、 ms-duplex-number、 ms-duplex-boolean、 ms-duplex-checked 取代原来的 ms-duplex-text、ms-duplex-bool、 ms-duplex-radio。感觉这些东西不是经过良好设计加入进来，而是为了解决现成的问题引入进来。angular 中的 ng-model提供的 $parsers（view 经该 pipeline 解析数据） $formatter（model 数据经该 pipeline 格式化显示）更给力。当然ms-duplex 在内部实现上也有一个 pipe 做数据适配过滤的。 指令avalon 在设计时考虑兼容性，抛弃了自定义标签。所有的标签都是由框架提供，也是下面的一些短板。不够优雅之处： ms-click-1，ms-click-2，ms-click-3 表示可以为某一个元素绑定 N 个点击事件。或许一般人也不会这么来做，完全可以在一个事件绑定中处理完这些事情。但是 ms-class-1 ms-class-2 可能是经常用到的。angular的写法 ng-class=&quot;{strike: deleted, bold: important, red: error}&quot;。 指令存在优先级 这个是神马 虽说对用户透明。但是还是引入了 ms-if-loop。 组件：没有了自定义标签，在组件化上就弱了很多。 1234&lt;!-- avalon写法：跟现在的组件写法差不多 --&gt;&lt;div ms-widget=&quot;accordion, accordionId, accordionOpts&quot;&gt;&lt;/div&gt;&lt;!-- angular写法 --&gt;&lt;accordion options=&quot;&quot;&gt;&lt;/accordion&gt; 另外，不知道全局配置在 avalon 如何实现。比如对项目下所有的 pager 设置默认配置。 路由mm-router 虽说敢叫板 ng-router，但是跟 ui-router 比还是差很远。在权限验证和二级路由上，不知道怎么做的，还没深入使用。 12345678910// avalonavalon.router.get(&quot;/ddd/{dddID}/&quot;, callback)// angular ui-router$stateProvider.state('adnav', { url: '', controller: 'AdnavCtrl', templateUrl: 'business/adnav.html', resolve: // 权限验证}) 小结avalon 宣称是一个迷你易用的 MVVM 框架，从文档上看，avalon 主要精力都在MVVM方面。除了 MVVM，angular 的依赖注入也是很不错的特性，尤其是。自定义标签可以在组件化上做很多事情。路由不及 ui-router 强大。组件化在当前的开发中，可以说是一种共识，React的流行就能看出。emberJs 也在做，虽说跟angular一样强依赖于框架。","link":"/2015/04/23/obsolete/angularer-watch-avalon/"},{"title":"Angular启动","text":"本来有一篇很好的文章讲解启动流程，后来设为秘密的了，虽然有复制，但也不太好拿人家不愿共享的东西出来。在这就贴一下我的总结啦。 AngularJS加载完成后工作：12345678bindJQuery(); // 如果有JQuerypublishExternalAPI(angular); // 暴露对外APIjqLite(document).ready(function() { angularInit(document, bootstrap); //初始启动}); publishExternalAPI1234567891011121314151617publishExternalAPI: extendAngular() // 扩展bootstrap extend isArray... angularModule() // 定义angular.module module('ngLocale', []).provider('$locale', $localeProvider); //创建模块 // 实际结果是 _invokeQueue.push('$provider', 'provider', arguments); _invokeQueue是模块实例的一个属性，自己在控制台输出下一个模块所有的属性。 module('ng', ['ngLocale'], ['$provider', function ($provider){ $provider.provider({ $animate: $AnimateProvider, $browser: $BrowserProvider, $http: $HttpProvider, ...... }); }]); // 实际效果： _invokeQueue.push('$injector', 'invoke', arguments); // 为了批量添加，provider接收一个object module('app', []).controller('ctrl', function(){}); // 实际效果： _invokeQueue.push('$controllerProvider', 'registre', arguments); 假设我们自己定义了一个 app 模块。 123app.run(['$scope', '$http', function ($scope, $http) {}]);// 实际先添加在_runblocks上 angularInit 做的事情是，查找页面上的 ng-app，启动整个应用。doBootstrap 首先创建 injector，然后执行下面代码。 12345678injector.invoke(['$rootScope', '$rootElement', '$compile', '$injector', '$animate', function(scope, element, compile, injector, animate) { scope.$apply(function() { element.data('$injector', injector); compile(element)(scope); }); }]); 创建Injector的过程：123456789101112131415161718192021222324doBootstrap--&gt;createInjector(modules): //modules参数: ['ng', ['$provider', function () {$provide.value('$rootElement', element);}], 'App']; providerCache {$provider, $injector: providerInjector} providerInjector {get: 从providerCache中取} instanceCache instanceInjector loadModules: 递归加载Provider类，后加载的覆盖之前的。 loadModules(moduleFn.requires) invoke(_invokeQueue) { // 大致操作 ngLocale: $provider.provider('$locale', $localeProvider); } return _runBlocks.concat(reuqires._runBlocks); provider:每个应用注册的provider类全在providerCache中 providerCache['$httpProvider'] = $httpProvider; providerCache['$filterProvider'] = $filterProvider; ...... instanceInjector.invoke(['$rooScope', '$http', function ($rootScope, $http) {}]); instanceCache {$rootScope: ,$http: .....,Config} //获取$rootScope实例时，可能递归的创建了其他的实例。 每个应用只创建一个Injector，当然一个页面可以手动启动几个独立的app，好像还没人这么干。 从下面的代码中可以看到，一个app模块依赖的provider实例全在providerCache中，没错我说的就是xxProvider实例。 当我们定义一个provider时，我们知道它是包装过的一个函数 1function XXXProvider() {this._get = function () {}} 这个 providerCache 中保存的就是该函数的实例 instance 它有一个 _get 属性。而 instanceCache 中保存的是调用_get 返回的实例。 一个 Injector 的实现并不复杂，cache 存放 provider 和实例，invoke 方法将函数提供的 $inject 或者参数名称 name 映射为 cache[name]，然后调用函数。 将 Provider 类和 Provider 实例分开单独的 cache 和 injector是基于这样的考虑：Provider 类不应该注入一个实例，同样实例不应该注入一个 Provider 类。 内部实现一个 createInternalInjector 方法提取共同部分创建这两个 injector。如果在 instanceCache 中查找不到，providerInjector 又有对应的 Provider 类，则会从 providerInjector 获取到该 Provider 类实例化一个。 至于模块的 config 方法，当然是与 Provider 类有关，而 run 方法当然是与 Provider 实例有关。就是 12345//config(['xxProvider', function (xxProvider) {}]) 在Provider类注入完之后才应该调用，所以保存在_configBlocks 而非 _invokeQueue中providerInjector.invoke(['xxProvider', function (xxProvider) {}]);// run(function (){})instanceInjector.invoke(fn); 注册 directive 提供的的 directiveFactory 一般也不会被用到。注册的 Directive 有单独的 cache。$compile 的功能实现也不复杂，查找节点，以节点为参数调用对应 Directve 的 compile 方法。对每个节点都进行递归处理。","link":"/2014/10/31/obsolete/angularjs-bootstrap/"},{"title":"烦人的 Flow.js","text":"不管怎么说，Javascript 构建的项目越来越大且越来越复杂，弱类型这个短板显得越来越为致命。而 Flow.js 提供了一个向强类型过渡的方案。 挫挫的枚举类型 12345678type Color = 'red' | 'blue' | 'green';const Colors = Object.freeze({ Red: 'Red', Blue: 'Blue', Green: 'Green',})type Color = $Values&lt;typeof Colors&gt;; 莫名其妙的严格 12345678type Vehicle = { name?: string }const car: Vehicle = {}; // It workstype Vehicle = {| name?: string, color?: string |}const car: Vehicle = { color: '' }; // It workstype Vehicle = {| name?: string |}const car: Vehicle = {}; // Error 不严格的类型和里式替换 我们知道，在 flow.js 中，当我们定义一个类型时，type Vehicle = { name: string } 只是意味着这个对象至少有 name: string 这个属性，你可以这样 const bike: Vehicle = { name: 'bike', brand: 'phoenix' }。就是因为太不严格，所以我们会选择严格类型 type Vehicle = {| name: string |} 不过接下来就遇到麻烦了。 123456789type Vehicle = {| name: string,|}type Car = { ...Vehicle, wheel: number};const car: Car = { name: 'Toyota', wheel: 4 };const vehicle: Vehicle = (car: Vehicle); // Cannot cast `car` to `Vehicle` 三方库依赖问题 有次突然发现我们漏掉了 import * as React from &quot;react&quot;; 但是可以直接使用 React.Node，flow.js 竟然没有报错。当然，它只可能是默认的 Any 类型。很快就定位到在 flow-typed/npm/enzyme_v3.x.x.js 包含有： 1234import * as React from &quot;react&quot;;declare module &quot;react-redux&quot; { // Here will use React type} 看样子是，只要在声明文件中导入之后，类型就被污染了。被污染的还有：Dispatch, Store, React, ComponentType and ElementConfig 不难猜到越是流行的库越可能被污染。而这一切发生的时候，flow.js 没有任何警告信息或错误提示。Known issue: https://github.com/flow-typed/flow-typed/issues/1857 第三方库 时不时的第三方库导致 flow 错误，又不能 ignore 所有的 node_modules 文件，每次遇到新错误，都只能加载后面。甚至 node_modules/**/test/*.json 都有可能导致 flow 错误。 目前没有什么优雅的办法：https://github.com/facebook/flow/issues/869 12[ignore].*\\/node_modules\\/draft-js\\/lib\\/.*.js.flow.* 关于 $FlowFixMe 有时 flow 不够聪明，即使我们知道没有那个逻辑，当然有时我们可以绕过这些报错。个人认为用额外的逻辑来弥补 flow 的错误更不合适。$FlowFixMe 是很烦，但是至少不会引起困惑。","link":"/2019/06/17/obsolete/annoying-flowjs/"},{"title":"前端模板","text":"原始的不足在最开始接触前端的时候，经常写这样的代码，当然不止做前端的时候都写过这样的代码。 123456var content = &quot;&lt;table&gt;&lt;tr&gt;&lt;th&gt;name&lt;/th&gt;&lt;th&gt;score&lt;/th&gt;&lt;/tr&gt;&quot;;for (var i = Things.length; i &gt;= 0; i--) { content += &quot;&lt;tr&gt;&lt;td&gt;&quot; + Things[i].name + &quot;&lt;/td&gt;&lt;td&gt;&quot; + Things[i].score + &quot;&lt;/td&gt;&lt;/tr&gt;&quot;;}content += &quot;&lt;/table&gt;&quot;;document.getElementById('table').innerHTML = content; smells bad! 太复杂且不利于阅读。使用引擎之后，可能是这样：(模板格式是自己随便写的，jekyll 使用的 liquid 模板，在这个项目中经常看到下面格式模板) 12345&lt;table&gt; &lt;tr&gt;&lt;th&gt;name&lt;/th&gt;&lt;th&gt;score&lt;/th&gt;&lt;/tr&gt; {{ &quot;{{ for student in Things&quot; }} }} &lt;tr&gt;&lt;td&gt;{{ &quot;{% student.name&quot; }} %}&lt;/td&gt;&lt;td&gt;{{ &quot;{% student.score&quot; }} %}&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; {``{ 与博客模板冲突，后面换成 &lt;% 之前算是学过 jsp 吧，对这样的标记并不陌生，&lt;%= %&gt; &lt;% %&gt; 之间插入 java 代码，只不过 jsp 是编译成 servlet 的。 模板要达到的效果一个模板要达到的效果至少不能再让我用 + 连接字符串了。(之前的项目里就有一个 format 方法做这种事，用于将 html 模板中的 #{} 替换为变量值。之后模板最好支持分支逻辑和循环遍历，上面的 student.score 可能需要根据值以不同颜色显示。另外，经常是数据的展现形式与数据不完全相同，比如日期 date，输出的要求可能是(‘YYYY-MM-DD’)，还有经常出现的 bool 值，可能存的是 true false 或者 0 1 或者 yes no 显示的时候又要显示为中文 是 否等。如果支持一个适配函数的形式就比较方便。 标记选取 &lt;% %&gt;只是约定形式，个人喜好 &lt;%= %&gt;，&lt;% %&gt; 这种。上面文章也提到避免与后端标记冲突，模板标记转义。模板位置：现在比较常见的是放在 &lt;script&gt; 标签中。web components 组件规范中直接定义了一个 template 元素，不过并未广泛支持。 123&lt;script type=&quot;text/tmpl&quot; id=&quot;myTmpl&quot;&gt; 模板放这里......&lt;/script&gt; 关于性能：文章指出了性能是个伪命题，也给了理由。但还是需要了解下 artTemplate 性能高效原因：http://cdc.tencent.com/?p=5723(原文有链接) 预编译 各个模板简单说最终还是生成了一个 js 函数，预编译当然就是，，。 更快的字符串相加方式 很多人误以为数组 push 方法拼接字符串会比 += 快，要知道这仅仅是 IE6-8 的浏览器下。实测表明现代浏览器使用 += 会比数组 push 方法快，而在 v8 引擎中，使用 += 方式比数组拼接快 4.7 倍。 关于异常处理：就是模板函数执行错误时，artTemplate 能指定到行号。文中还给出了一个简单的 demo 方案。就是在生成的代码中每行插入 $liine = currentLine 这样的东西，感觉很老土，不知道 artTemplate 是不是采用的这种方式。 自己实现一个简单的模板解析jQuery 作者 john 开发的微型模板引擎 http://ejohn.org/blog/javascript-micro-templating/ 1234567891011121314151617181920212223242526272829303132333435// Simple JavaScript Templating// John Resig - http://ejohn.org/ - MIT Licensed(function(){ var cache = {}; this.tmpl = function tmpl(str, data){ // Figure out if we're getting a template, or if we need to // load the template - and be sure to cache the result. var fn = !/\\W/.test(str) ? cache[str] = cache[str] || tmpl(document.getElementById(str).innerHTML) : // Generate a reusable function that will serve as a template // generator (and which will be cached). new Function(&quot;obj&quot;, &quot;var p=[],print=function(){p.push.apply(p,arguments);};&quot; + // Introduce the data as local variables using with(){} &quot;with(obj){p.push('&quot; + // Convert the template into pure JavaScript str .replace(/[\\r\\t\\n]/g, &quot; &quot;) .split(&quot;&lt;%&quot;).join(&quot;\\t&quot;) .replace(/((^|%&gt;)[^\\t]*)'/g, &quot;$1\\r&quot;) .replace(/\\t=(.*?)%&gt;/g, &quot;',$1,'&quot;) .split(&quot;\\t&quot;).join(&quot;');&quot;) .split(&quot;%&gt;&quot;).join(&quot;p.push('&quot;) .split(&quot;\\r&quot;).join(&quot;\\\\'&quot;) + &quot;');}return p.join('');&quot;); // Provide some basic currying to the user return data ? fn( data ) : fn; };})(); 以下面的模板为例： 12345&lt;h3&gt; &lt;% if (typeof content === 'string') { %&gt; &lt;%= content %&gt; &lt;% } %&gt;&lt;/h3&gt; 解析之后生成的函数类似： 1234567891011function anonymous(obj/**/) { var p=[]; with(obj){ p.push(' &lt;h3&gt; '); if (typeof content === 'string') { p.push(' ', content ,' '); } p.push(' &lt;/h3&gt; '); } return p.join('');} 刚给出的模板引擎正则表达式那段不大容易看懂，作用就是将&lt;% %&gt;里面的内容直接作为字符串，其它的正文则是压入结果数组。我自己稍微改了下： 123456789101112131415var cache = {};this.tmpl = function (str, data) { var fn = !/\\W/.test(str) ? cache[str] = cache[str] || tmpl(document.getElementById(str).innerHTML) : new Function(&quot;obj&quot; , &quot;var p = []; with(obj){&quot; + format(str) + &quot;};return p.join('');&quot;); return data ? fn(data) : fn;};function format (str) { str = '%&gt;' + str + '&lt;%'; return str.replace(/[\\r\\n\\t]/g, ' ') .replace(/&lt;%=(.*?)%&gt;/g, &quot;',$1,'&quot;) .split(&quot;&lt;%&quot;).join(&quot;');&quot;) .split(&quot;%&gt;&quot;).join(&quot;p.push('&quot;);} 测试页面在：http://fiddle.jshell.net/fedeoo/6jwMy/2/show/ 一篇讲设计模板引擎的文章，不过本身没有讲如何设计。http://www.toobug.net/article/how_to_design_front_end_template_engine.html","link":"/2013/10/15/obsolete/front-template/"},{"title":"前端三俗之Promise","text":"这篇文章很好的介绍了Promise http://www.html5rocks.com/zh/tutorials/es6/promises/ 前端乱炖上有篇文章介绍了它的简易实现 http://www.html-js.com/article/1850 作为一个俗人，怎么会不用自己的逻辑再实现一遍呢。因为defer更为常见，就说说defer吧 123456789101112var d = new Defer(function (resolve, reject) { setTimeout(function () { resolve('start'); }, 1000);});d.then(function (x) { console.log('then --' + x) return 'filter';}).then (function (value) { console.log('then ----' + value); return '***';}); 完成上面的功能，分析一下我们需要实现的有这么几个方法：then resovle reject。then做的事情就push回调，resovle reject就是唤醒处理。 123456789101112131415161718192021222324252627function defer (func) { var observers = []; var state = 0; function then (onFulfill, onError) { observers.push([onFulfill, onError]); return this; } function exec (result) { while (observers.length) { var observer = observers.shift(); observer[state](result); } observers = null; } function resovle (x) { state = 0; exec(x); } function reject (x) { state = 1; exec(x); } func(resovle, reject); return { then: then }} 这个屌丝版defer还有几个问题： 调用then的时候可能已经执行过resovle了，需要重新执行一次。 没有考虑返回值是Promise的情况 值没有传给下一个then处理 处理一下这几个问题后： 1234567891011121314151617181920212223242526272829303132333435363738394041424344function defer (func) { var observers = []; var state = 0; function then (onFulfill, onError) { if (observers) { observers.push([onFulfill, onError]); } else { observers = []; observers.push([onFulfill, onError]); exec(); } return this; } function exec (result) { while (observers.length) { var observer = observers.shift(); if (typeof observer[state] !== 'function') { continue; } try { result = observer[state](result); if (result &amp;&amp; typeof (result.then) === 'function') {//返回的是defer return result.then(resolve, reject); } } catch (e) { state = 1; result = e; } } observers = null; } function resovle (x) { state = 0; exec(x); } function reject (x) { state = 1; exec(x); } func(resovle, reject); return { then: then };} 看上去虽说屌丝，不过好像功能还行，而且逻辑很简单。但是跟标准有点不一样：defer.then().then() 和 defer.then(); defer.then() 按照标准是执行结果是不同的。如果按照上面的代码逻辑，处理的结果却是一样的，因为 then 只是简单的返回了 this。这儿的 then 应该返回一个新的defer对象。大概是这样子: 12345678910111213141516171819202122232425function defer () { var observers = []; function then (onFulfil, onError) { var deferred = new defer(); function resolve (value) { var ret = onFulfil ? onFulfil(value) : value; deffrred.resolve(ret); } function reject (value) { var ret = onError ? onError(value) : value; deferred.reject(ret); } observers.push({resolve: resolve, reject: reject}); return deferred; } function resolve (value) { } function reject (value) { } return { then: then, resolve: resolve, reject: reject };} then 返回的是一个新 deferr 对象, 而 observers 保存的是包装过的函数，调用这个函数的时候会调用传的回调，并使用返回值调用新 deferred 的 resolve 方法。 再看下 resolve 和 reject，这次我们将所有的返回值都当成 derfer 来处理，然后调用 then 方法。不难理解下面这段代码： 12345678910111213141516171819function resolve (value) { result = isPromise(value) ? value : {then : function (resolve) {resolve(value);}}; while (observers.length) { var observer = observers.shift(); result.then(observer.resolve, observer.reject); } observers = null;}function reject (value) { result = isPromise(value) ? value : {then : function (resolve, reject) {reject(value);}}; while (observers.length) { var observer = observers.shift(); result.then(observer.resolve, observer.reject); } observers = null; resolve({then: function (resolve, reject) {reject(value);}});} 我们可能看到reject与resolve非常相像，可以完全用resolve替代，稍微改下： 123function reject (value) { resolve({then: function (resolve, reject) {reject(value);}});} 该文章只是描述实现defer的思想，代码不能直接用，只是希望大家能一看就明白这个思路。 全部代码见https://github.com/fedeoo/codebrick firefox_raw_promise.js是firefox下的实现,比较具有参考意义。","link":"/2014/03/29/obsolete/promise/"},{"title":"谈谈组件封装","text":"在前端开发中，我们往往会定义自己的组件，比如常见的日期选择器 datepicker，在其它页面上需要使用时再实例化一个组件。本文简单地聊下组件封装。首先会说下基于 jquery 的封装，之后会以 angular 为例，简单说下框架与组件的适配。最后说说 web components 标准。 jquery 类组件封装一个组件，往往需要提供的方法有 init： 负责构造组件 DOM 结构，之后会做一些事件绑定。这个接口往往还会接受一个options类的配置数据。 render： 根据状态数据渲染组件。往往是重绘模板。 onchange： 对外暴露一些事件。 setValue：改变内部数据接口，重绘最简单的方式就是再次 render。 destroy: 处理组件销毁工作，比如 unbind event。 然后像下面这样实例化一个组件（假设是 jquery widget）。 1$('ele').datepicker({...}); 不过，很多时候我们不想手动去实例化一个组件。我们觉得下面这种写法更符合 web 语义。 123&lt;div data-role=&quot;date-picker&quot;&gt;....&lt;/div&gt;&lt;date-picker onchange='****'&gt;&lt;/date-picker&gt; 如果我们使用一些配套的框架，往往也会支持这种写法（如 Bootstrap…）。在 DOM 加载完成之后，扫描所有节点，对支持组件的节点，调用对应的组件构造方法。如果在框架加载处理完成之后，自己手动插入组件节点，是不会生效的，往往还需要手动 init，销毁时的还要手动销毁。通常所有的组件还会继承自一个 BaseComponent，该 BaseComponent提供一些公共的方法。 angular.js 1.× 中的 directivedirective 是 angular 的三大特性(mvvm，依赖注入，directive)之一。 angular 在 do-bootstrap 之后，最后会编译根节点并 link 到 rootScope 上。 1compile(rootElement)(rootScope) 简化再简化版伪代码描述： 123456789101112131415161718192021222324function compile ($compileNodes) { return compileNodes($compileNodes);}function compileNodes ($compileNodes) { _.forEach($compileNodes, function (node) { var directives = collectDirectives(node); applyDirectivesToNode(directives, node); // 递归compile子元素 compileNodes(node.childNodes); });}function collectDirectives (node) { var directives = []; // 查找 nodeName attributesName className 中的directive 并加入directives ... return directives;}function applyDirectivesToNode (directives, node) { // 以node为参数调用所有diretives的compile方法 _.forEach(directives, function (directive) { directive.compile(node); });} link 与 compile 对应但又分开。考虑 ng-repeat 这样的 directive， 只需要一次 compile，而 link 次数就不确定了。还有很多需要考虑的，如 scope 层级、独立 scope、属性上的双向绑定…. react.js 对 react 不熟，就看看吧。 简单的组件 React 写法看上去没有什么不同。看上去好像也是提供一个模板，数据变化时重新渲染。代码中的标签写法是 jsx 语法，实际会处理成 reactElement。 12345678910var XXXComponent = React.creatClass({ render: function () { return ( &lt;div&gt; // 根据props state数据填充 .... &lt;/div&gt; ); }});React.render(&lt;XXXComponent&gt;, element); 每次都重新渲染的方式，有点太过简单暴力。虽说性能可能会有影响，但是开发者完全不需要关注数据变化是怎么改变组件（之前可能会介绍选择局部重绘）。react 引入的 virtual DOM 使得重绘非常高效（传说这样子），就更不用担心。 如果根据react的diff算法设计场景故意让重绘效率降低 表单验证这方面就不如 angular 的写法优雅。假如对一个 Input 添加一个新的验证，React 就必须用一个采用 Wrap 的方式，当然也可以重写一个 Input 支持 xxx 属性。 123456&lt;!-- react 写法 --&gt;&lt;XXXValidator&gt; &lt;Input/&gt;&lt;/XXXValidator&gt;&lt;!-- angular 写法 --&gt;&lt;input xxx-validator /&gt; web components上面讨论的做法不管怎样封装，实际 DOM 结构都会暴露在外，而且样式冲突防不胜防。而 web components 提供的 shadow DOM 做到了完全隔离组件。比如现在很常见的 range 组件，对外看起来就是只有一个元素，遍历时也获取不到 range 中的子元素，而且外部的样式也不会影响到组件。 1&lt;input type=&quot;range&quot;&gt; 考虑到确实有这样的场景，需要自定义组件样式，需要自定义组件内容，如 Dialog 这种。shadow DOM 提供了通透的那部分对外又是可见的。使用伪元素选择器又能改变组件样式。 前年有翻译规范的打算。后来读的时候不能完全明白，实在翻译不下去了… 小结整篇文章先说常规的组件封装。再谈语义化更为明确的 direactive，只说了框架与组件的适配，中间插入最近比较火的 react 组件，最后以 web components 标准结束。最后还想说下基础组件与业务组件，基础组件一般不会变化。做一个项目时，有时发现几个功能类似需求出现在几个地方，封装成一个组件。过段时间需求变更，其中一个地方需要加新功能…。这么来几次，一个业务组件很容易被玩坏。业务组件如何复用是一个麻烦的问题。","link":"/2015/05/14/obsolete/talk-about-component/"},{"title":"前端开发畅想","text":"整篇文章的思路是这样子的：重新思考前端开发过程，有不少让人觉得不痛快的地方。如果没有这些束缚的话，理想中的开发部署又是怎样的？理想有点远，眼下我们又能做些什么呢？ 现状与问题最近在重新思考前端构建与部署，不过上次写的文章根本没有讲清楚。没有讲清楚为什么要这么做。现在我们来重新审视下目前流行的前端技术栈与发布方式，以我熟悉的技术栈为例。 项目启动：一般团队都会有自己的脚手架，再不济也可以 copy 现有项目，迅速搭建开发环境。 开发与调试：集团内比较常见的或许是 React + Redux，集团外或许是 Vue 构建与部署：云构建发布到 CDN 上，修改后端维护的版本号。构建工具基于 webpack。 我们再来看看问题： 脚手架的问题之前也提到过，没有一个像样的更新工具，用 generator 生成之后只能手工升级。 开发中的问题很多，与技术栈关联也比较大。我觉得有个问题值得反思一下，现在很多的框架号称半小时搞定增删改查，为啥前端还是忙的没时间，甚至有些觉得开发速度不比 jQuery 来的快。 计算资源的浪费。以 React 应用为例，对于纯静态的内容，每次重新渲染这不是纯粹浪费么，不管在服务端还是客户端这些浪费都是不能容忍的。前后的资源消耗统一来看的话，怎么看都是服务端模板渲染更为节约。 带宽的浪费。每个项目都用到了那么多的公共库，webpack 一并打进去，每个应用每个版本都有大量的请求都是非必须的。而且集团很多的变更，都是一个版本的粗粒度的控制，每次版本变更，所有的资源又重新拉取一次。 构建的问题。webpack 取代 gulp，改变了我们管理资源的方式。webpack 很复杂，它的配置我们应该关心吗？我们把构建理解为一个黑盒，源文件作为输入，目标代码作为输出。我多希望我什么都不用关心，构建就能正常运行起来。 部署优化。pagespeed 已经提供了一个性能诊断，甚至可以给出一个优化方案，为什么我们还要自己上手来搞这些。 大胆思考上次也接着贺老的思路 YY 了一番，这儿再啰嗦一遍： 资源云化。所有可以公用的资源都在云上，可以说是公用 CDN 资源的高级形式。这个可以解决问题4。 几乎不需要配置，构建可以自动完成，并且它可以做些分析，还可以做些优化。 现成的部署和监控方案，根据请求响应不同内容。比如首屏的关键路径样式优化，预加载，这些由服务器自动优化。 上面的 2 和 3 听上去都有些智能的感觉。还有一个问题是：我们进行了太多重复不必要的运算，就像上面的问题3，每次重新运行 webpack 我都觉得是种浪费。 脚踏实地步子迈的太大，有点不切实际。我们先利用好手上可用的工具来解决性能和资源问题。不对现有技术体系做大的调整下，利用新的技术做些优化。 对上面的问题3，我们还有服务端渲染嘛，只是服务端渲染的机会成本略高。退一步我们可以考虑，数据直出，预渲染。create-react-app没有引入服务端渲染的特性，倒是提到了预渲染 React Snapshot 。这个库的思路是：服务端渲染太麻烦，不如生成静态页面。react-snapshot 提前渲染这些页面，重新生成一个静态页面。 增强构建。PWA 很诱人，无需大的改动，我们可以使用webpack 插件 可以帮助生成 servicework.js 文件。若有必要我们可以使用 http2-aggressive-splitting 分割文件来加快 HTTP2 网络下的资源加载速度。还有 preload-webpack-plugin 插件帮助做预加载的脚本。 后记追随框架与工具变革却看不清未来发展趋势，整理下最近的想法。","link":"/2017/06/26/obsolete/%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83/"},{"title":"移动端视频播放的另一种方式","text":"前几天做的一个移动端页面需要播放一段视频。嵌入页面video标签的方式肯定是不合适的。 因为页面需要的其实只是一个动画效果，对音频没有要求，所以就有机会选择其它的方式解决。最先想到的方案是 canvas-video，前不久 BM2 炫酷的微信广告就是用它实现的。原理很简单： 123let context = canvas.getContext('2d');// 在视频播放时，从video中取出图片，在画布上 drawImagecontext.drawImage(video, 0, 0, width, height); 整个库除了做播放控制之外，还值得提的就是绘制时使用 requestAnimationFrame 优化，这儿就不多说了。效果实现之后，在自己手机 iOS 上看下效果，video 可以自动播放，虽然不能循环，效果还行。找了台安卓机测试发现，效果出不来，因为移动环境下的流量保护禁用自动播放。 然后在 github 上发现一个 canvid 项目用来解决移动端视频播放，它的方案是：将视频中的关键帧抽取出来，合并为一张图片，使用 canvas 播放图片。这个方案的浏览器支持肯定是没有问题的，问题是效果和加载速度如何。准备图片倒是折腾了一会，给的两个工具安装后直接使用报错，需要根据报错提示安装依赖的工具（两次）。之后的实现就很简单了，视频播放效果还很流畅。注意播放帧的设置，从源视频抽取图片时，是每 5 帧抽取一张图，这儿设置每秒 13 帧，与视频的每秒65帧是同步的。 现在的问题就是合并之后的图片太大了。canvid 有提醒：图片大小在一些移动端存在大小上限。后来把原图片分为 4 张图进行合并，每张大小在 700k 左右（无法再做压缩，ImageMagick 合并图片时已经做了压缩）。该库会在所有图片加载完成之后进行播放，在这儿我们可以进行一些简单的优化，在第一张图片加载完成之后就可以播放动画，播放时可以继续加载后面的图片。 为什么这种方式优于 gif 呢？因为 gif 是无损压缩，jpg 是有损压缩，压缩比差距很大。 那如果我把图片有损压缩处理之后，再转成 gif 格式呢？效果还是不如 jpg，因为 jpg 可以对这些图片的共同部分进行压缩。 当然最好还是使用视频格式，因为视频在播放的时候无需全部加载完成，而且在之后的数据传输可以传入差异数据。 最后，放下效果页面","link":"/2016/05/07/obsolete/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E8%A7%86%E9%A2%91%E6%92%AD%E6%94%BE%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E6%96%B9%E5%BC%8F/"}],"tags":[{"name":"AWS","slug":"AWS","link":"/tags/AWS/"},{"name":"Client","slug":"Client","link":"/tags/Client/"},{"name":"Serviceless","slug":"Serviceless","link":"/tags/Serviceless/"},{"name":"Amazon connect","slug":"Amazon-connect","link":"/tags/Amazon-connect/"},{"name":"DataBase","slug":"DataBase","link":"/tags/DataBase/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"},{"name":"DynamoDB","slug":"DynamoDB","link":"/tags/DynamoDB/"},{"name":"读书笔记","slug":"读书笔记","link":"/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"后端架构","slug":"后端架构","link":"/tags/%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84/"},{"name":"工作生活","slug":"工作生活","link":"/tags/%E5%B7%A5%E4%BD%9C%E7%94%9F%E6%B4%BB/"},{"name":"CS","slug":"CS","link":"/tags/CS/"},{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"SQS Processor","slug":"SQS-Processor","link":"/tags/SQS-Processor/"},{"name":"架构","slug":"架构","link":"/tags/%E6%9E%B6%E6%9E%84/"},{"name":"经济学","slug":"经济学","link":"/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"name":"工作效率","slug":"工作效率","link":"/tags/%E5%B7%A5%E4%BD%9C%E6%95%88%E7%8E%87/"},{"name":"工程化","slug":"工程化","link":"/tags/%E5%B7%A5%E7%A8%8B%E5%8C%96/"},{"name":"解释器","slug":"解释器","link":"/tags/%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"name":"总结思考","slug":"总结思考","link":"/tags/%E6%80%BB%E7%BB%93%E6%80%9D%E8%80%83/"},{"name":"测试","slug":"测试","link":"/tags/%E6%B5%8B%E8%AF%95/"},{"name":"见识","slug":"见识","link":"/tags/%E8%A7%81%E8%AF%86/"},{"name":"个人管理","slug":"个人管理","link":"/tags/%E4%B8%AA%E4%BA%BA%E7%AE%A1%E7%90%86/"},{"name":"学习笔记","slug":"学习笔记","link":"/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"调试","slug":"调试","link":"/tags/%E8%B0%83%E8%AF%95/"},{"name":"监控","slug":"监控","link":"/tags/%E7%9B%91%E6%8E%A7/"},{"name":"编码","slug":"编码","link":"/tags/%E7%BC%96%E7%A0%81/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"webSocket","slug":"webSocket","link":"/tags/webSocket/"},{"name":"软实力","slug":"软实力","link":"/tags/%E8%BD%AF%E5%AE%9E%E5%8A%9B/"},{"name":"翻译","slug":"翻译","link":"/tags/%E7%BF%BB%E8%AF%91/"},{"name":"CDN共享","slug":"CDN共享","link":"/tags/CDN%E5%85%B1%E4%BA%AB/"},{"name":"框架","slug":"框架","link":"/tags/%E6%A1%86%E6%9E%B6/"},{"name":"AngularJs","slug":"AngularJs","link":"/tags/AngularJs/"},{"name":"业务流程","slug":"业务流程","link":"/tags/%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B/"},{"name":"秒杀","slug":"秒杀","link":"/tags/%E7%A7%92%E6%9D%80/"},{"name":"MobX","slug":"MobX","link":"/tags/MobX/"},{"name":"webpack","slug":"webpack","link":"/tags/webpack/"},{"name":"Promise","slug":"Promise","link":"/tags/Promise/"},{"name":"MutationObserver","slug":"MutationObserver","link":"/tags/MutationObserver/"},{"name":"Module","slug":"Module","link":"/tags/Module/"},{"name":"SeaJS","slug":"SeaJS","link":"/tags/SeaJS/"},{"name":"AngularJS lazyload","slug":"AngularJS-lazyload","link":"/tags/AngularJS-lazyload/"},{"name":"MVVM","slug":"MVVM","link":"/tags/MVVM/"},{"name":"AngularJS","slug":"AngularJS","link":"/tags/AngularJS/"},{"name":"编程语言","slug":"编程语言","link":"/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"Flow.js","slug":"Flow-js","link":"/tags/Flow-js/"},{"name":"模板","slug":"模板","link":"/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"组件","slug":"组件","link":"/tags/%E7%BB%84%E4%BB%B6/"},{"name":"移动端","slug":"移动端","link":"/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF/"}],"categories":[{"name":"实践总结","slug":"实践总结","link":"/categories/%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/"},{"name":"学习笔记","slug":"学习笔记","link":"/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"设计实现","slug":"设计实现","link":"/categories/%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0/"},{"name":"技术杂谈","slug":"技术杂谈","link":"/categories/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/"}],"pages":[{"title":"About","text":"Fedeoo, 工程师，工作前7年主要做前端开发，这两年开始做全栈开发。正值而立之年，惟愿多多读些书，更好的认知这个世界。 选鲨鱼图的原因是：鲨鱼永不停止游动，提醒自己学习永不止步。 毕业渣浪工作两年，之后西厂工作一段时间，现就职悉尼一家小厂。 博客主要是放一些技术收获，考虑放入更多的技术思考，因为单纯去讲一个技术点，谷歌很容易找到很有深度的文章，没必要做搬运工了。 用博客记录成长，坚持定期自省。只有在静下来思考时，才能好好的反省和检视自己。","link":"/about/index.html"}]}