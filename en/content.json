{"posts":[{"title":"AWS client getaddrinfo EMFILE issue","text":"Recently, we introduced AWS Cloud Map for service discovery, primarily to retrieve queue URLs. However, after deployment, we encountered intermittent errors weeks later, logged as getaddrinfo EMFILE events.ap-southeast-2.amazonaws.com. Not all requests triggered this error, indicating a selective issue. Upon inspection, it became apparent that we were facing a socket timeout problem, a known issue in our setup. The remedy was simple: reusing our existing agent. 1const keepAliveAgent = new HttpsAgent({ keepAlive: true, maxSockets: 500 }); Unpacking the IssueIn our codebase, each lambda utilizes a createService function to generate service instances for each request/run. Attempts to switch to a singleton instance were met with complications, prompting a rollback. The core problem emerged: with a new client instance created for each request, sockets held by the HTTP agent from each run might not be released, leading to hitting the socket limit before garbage collection could free them. During periods of low traffic, the issue remained dormant, with lambda invocations being infrequent. However, the potential for overload was evident, given that a lambda can manage up to 1024 outgoing connections, a threshold that could be gradually reached over time. A similar issue surfaced in other resource integrations. Investigation revealed that our OpenAPI-generated client was using a singleton instance: 1const keepAliveAgent = new HttpsAgent({ keepAlive: true, timeout: 15000 }); Key TakeawaysReflecting on this incident, we realized some fundamental truths. While such mistakes may seem basic, they underscore the complexities that emerge as systems evolve. Take away:keepAlive is not default in AWS SDK, you need to set it manually even it’s recommended to maintain persistent connections.In SDK v2, there’s a easy way to do that by setting AWS_NODEJS_CONNECTION_REUSE_ENABLED=1. But it seems removed in SDK v3.Interestingly, http.globalAgent in Node.js set it as default since v19.0.0 https://nodejs.org/en/blog/announcements/v19-release-announce/#http-s-1-1-keepalive-by-default","link":"/en/2024/05/16/AWS-client-getaddrinfo-EMFILE-issue/"},{"title":"AWS connect last agent call routing","text":"ChallengeIn a recent project, we were assigned the task to route calls to the last agent who interacted with the caller.This requirement may seem fundamental, but it proved to be more complex than anticipated,especially considering its integration into Salesforce via Salesforce Service Cloud Voice. SolutionThe proposed solution is relatively straightforward.We will employ a Lambda function to query recent voice calls and retrieve information about the previous queue and agent.If the agent is available and the customer wishes to continue the previous call, the call will be transferred to that same agent.However, should any errors occur during this process, the call will follow the normal flow. In summary: A Lambda function will retrieve details about the customer’s previous call, including the queue and agent. If the previous agent is available and the customer wants to resume the call, the new call will be transferred to that agent. If any issues arise during this process (such as agent unavailability or customer preference), the call will proceed through the regular call flow. ImplementationUpdate IVRFirst, invoke the Lambda to retrieve the last agent and queue, then set the agent to the working queue.Before transferring the call to the queue, we need to check if the agent is available and provide the customer with the option to continue the previous call or not.The IVR flow is depicted as follows: Lambda Get Last Voice CallNow, the only remaining task is to retrieve the last agent.Unfortunately, the SearchContactsCommand from aws client does not support searching for contacts by customer phone number, possibly due to privacy concerns.As a workaround, we can set a custom contact attribute (e.g., CustomerPhoneNumber) and mark it as searchable.Then, using the SearchContactsCommand, we can search for contacts by the searchable attributes.It’s important to note that contacts created before adding the searchable key will still not be searchable.Additionally, we can set a time range in this Lambda to search for contacts within the past 7 days. The code snippets are as follows: 1234567891011121314151617181920212223242526272829303132const input: SearchContactsRequest = { InstanceId, TimeRange: { Type: &quot;INITIATION_TIMESTAMP&quot;, StartTime: new Date(Date.now() - 7 * 24 * 3600 * 1000), // Since 7 days ago EndTime: new Date(), }, SearchCriteria: { Channels: [ &quot;VOICE&quot;, ], SearchableContactAttributes: { Criteria: [ { Key: &quot;CustomerPhoneNumber&quot;, Values: [ phoneNumber, ], }, ], MatchType: &quot;MATCH_ALL&quot;, }, }, MaxResults: 10, Sort: { FieldName: &quot;INITIATION_TIMESTAMP&quot;, Order: &quot;DESCENDING&quot;, },};const command = new SearchContactsCommand(input);const response = await client.send(command);const lastContact = (response.Contacts ?? []).filter((contact) =&gt; contact.AgentInfo?.Id)[0] ConclusionAt last year’s AWS conference, I observed a promising demonstration of Amazon Connect integrated with AI, highlighting its potential to proactively resolve customer issues.However, I was surprised to discover that implementing last-agent call routing, which I deemed fundamental, lacked an out-of-the-box solution in Amazon Connect.This realization prompted us to develop a custom solution to address this critical requirement. Reference post Last Agent and Last Queue Routing on Amazon Connect for Returning Callers","link":"/en/2024/05/21/AWS-connect-last-agent-call-routing/"},{"title":"DynamoDB the Advantages and Considerations of Single Table Design","text":"Developers familiar with Relational Database design often find themselves initially drawn to the familiar territory of normalization when designing data models for DynamoDB. This instinct typically leads them towards what’s known as multi-table design, where each entity or relationship resides in a separate table. On the other hand, DynamoDB’s schemaless nature encourages a different approach: single-table design, where all entities and relationships coexist within a single table. However, it’s worth noting that these two designs represent extremes on a spectrum, rather than strict boundaries. According to the official documentation, single-table design is often recommended. This article explores the advantages of single-table design based on practical experience. Our project predominantly utilizes single-table design, largely influenced by 《The DynamoDB Book》, authored by advocate Alex DeBrie. Despite our commitment to single-table design, we still manage more than a dozen tables, albeit with a focus on storing related entities together. Advantages of single table design1. Data Locality for Efficient QueriesSingle-table design allows for fetching multiple entity types in a single database call, reducing requests, improving performance, and cutting costs. However, realizing this advantage in practice requires deliberate structuring, as applications often overlook underlying implementation details. In the table below, user information and order information use the same Partition Key, in the same itemCollection, you can get user information and order information in one Query request. The problem in reality: realizing this advantage in practice requires deliberate structuring, as applications often overlook the underlying implementation details.In practice, we don’t care about the order of entity types in the same itemCollection, usually still keep 2 requests for different entities each in same itemCollection. 2. Reduces overall financial and latency costs of reads: A single query for two items totalling less than 4KB is 0.5 RCU eventually consistentTwo queries for two items totalling less than 4KB is 1 RCU eventually consistent (0.5 RCU each)The time to return two separate database calls will average higher than a single call The advantages of the bill are quite obvious: reducing the number of requests naturally reduces the cost of the bill, usually a single entity record will not be very large, and multiple entity records may not necessarily have 4KB.The advantage of latency is also easy to understand, a single request is usually faster than two separate requests. However, this advantage based on assumptions that you can save another request call, which is not always the case in practice. 3. Reduces the number of tables to manage: Permissions do not need to be maintained across multiple IAM roles or policiesCapacity management for the table is averaged across all entities, usually resulting in a more predictable consumption patternMonitoring requires fewer alarmsCustomer Managed Encryption Keys only need to be rotated on one table Permissions do not need to be maintained across multiple IAM roles or policies, which also means that the granularity of permissions is not so fine. In practice, we already feel that our permissions are too fine, and each Lambda has different role permissions.These advantages are real, single-table design is easier to manage, does not require more configuration, and does not require access permissions for each table in each Lambda. 4. Smooths traffic to the table: By aggregating multiple usage patterns to the same table, the overall usage tends to be smoother (the way a stock index’s performance tends to be smoother than any individual stock) which works better for achieving higher utilization with provisioned mode tablesNot much to say, the advantage is not obvious in practice Disadvantages Learning curve can be steep due to paradoxical design compared to relational databasesUnderstanding the fundamental principles of DynamoDB, grasping the single-table design concept isn’t overly complex. However, there are associated trade-offs to consider, and the learning curve isn’t particularly steep. In our experience, incorporating a new entity type into an existing table proves far simpler than introducing a new table altogether. Data requirements must be consistent across all entity types All changed data will be propagated to DynamoDB Streams even if only a subset of entities need to be processed. When using GraphQL, single table design will be more difficult to implement When using higher-level SDK clients like Java’s DynamoDBMapper or Enhanced Client, it can be more difficult to process results because items in the same response may be associated with different classes SummaryDynamoDB’s flexibility allows for a spectrum of design choices, with no strict delineation between single-table and multi-table approaches. Advocates typically recommend a per-service table, challenging developers to break free from the relational database mindset. Understanding DynamoDB’s core concepts is essential. From there, the choice between single-table and multi-table design should align with specific project requirements. For further exploration: Single-table vs. multi-table design in Amazon DynamoDB Single table design for DynamoDB: The reality The What, Why, and When of Single-Table Design with DynamoDB","link":"/en/2024/06/02/DynamoDB%20single%20table%20design/"},{"title":"How to prevent duplicate SQS messages","text":"ProblemIn our system, queue processors must implement idempotency to prevent the double-processing of messages. Duplicate messages may arise in the following scenarios: Scheduler and Message Producer: The scheduler or message producer may be triggered multiple times, occasionally rerunning due to timeouts. Queue Management: If a lambda instance times out while processing a message, another instance may retrieve the same message if the visibility timeout is not properly set. This can have terrible consequences. We aim to avoid sending duplicate emails or messages to our customers, not to mention inadvertently delivering duplicate gift cards. So a generic idempotency mechanism is required. SolutionThe idea is straightforward: we will use a DynamoDB / Redis cache to store the message ID and the processing status. When a message is received, we will check the record to see if it has been processed. If it has, we will skip the message. If not, we will process it and update the cache. Considering our serverless architecture, DynamoDB is selected. Basically, there are three cases: Message first time processed: process the message. Message is being processed or has been processed: discard the message. Message processing failed: reprocess the message.To handle this case, we need to add a lock timeout to the record. If the message is still in processing status after the lock timeout, we give it another chance to be processed. Implementation Create a DynamoDB table message-processor. It’s a normal table with a primary key messageId. Implement a service with this interface:12345678910111213141516171819202122interface IMessageProcessorService { /** * Here use DynamoDB message-processor table as the fact store to decide if a message has been seen before * @param messageId unique identifier for each message * @param lockTimeoutInSeconds how long to lock the message for processing. It gives another chance to reprocess the message if it fails. * @returns boolean: true indicates the lock is acquired and should continue the processing. * false indicates the message is already being processed or being processed by another instance. */ acquireProcessingLock(messageId: string, lockTimeoutInSeconds: number): Promise&lt;boolean&gt;; /** * Mark the message as processed, preventing it from being processed again * @param messageId */ markMessageProcessed(messageId: string): Promise&lt;void&gt;; /** * Remove record of failed message processing, allowing it to be processed again * @param messageId */ releaseProcessingLock(messageId: string): Promise&lt;void&gt;;} The code snippet below shows how to implement the acquireProcessingLock method:(Never mind, we’re using internal libraries to simplify the code) 1234567891011121314151617181920await this.store.replace( { _id: id, status: 'PROCESSING', timestamp: Date.now(), }, { condition: { $or: [ { _id: { $exists: false } }, // insert new record { $and: [ { timestamp: { $lt: Date.now() - lockTimeoutInSeconds * 1000 } }, { status: { $eq: 'PROCESSING' } }, ], }, ], }, },); At last, we enhance the existing message handler with the idempotency mechanism: 12345678910111213141516171819202122232425262728293031323334export const makeHandlerIdempotent = async &lt;T&gt;( handler: MessageHandler&lt;T&gt;, IdGenerator: (message: T) =&gt; string, { messageProcessorService, lockTimeoutInSeconds, logger, }: { logger: ILoggerService; messageProcessorService: IMessageProcessorService; lockTimeoutInSeconds: number; },): Promise&lt;MessageHandler&lt;T&gt;&gt; =&gt; { return async (message: T) =&gt; { const id = IdGenerator(message); const acquiredProcessingExclusiveLock = await messageProcessorService.acquireProcessingLock( id, lockTimeoutInSeconds, ); if (!acquiredProcessingExclusiveLock) { logger.info('processMessageIdempotent: message has already been processed', { message }); return; } try { const result = await handler(message); await messageProcessorService.markMessageProcessed(id); return result; } catch (error) { await messageProcessorService.releaseProcessingLock(id); throw error; } };}; ConclusionIt seems preventing duplicate messages in a distributed system is likely a common requirement.While implementing this idempotency mechanism, I found almost similar solution discussed How to prevent duplicate SQS Messages?.It was very helpful and offered clear explanations.","link":"/en/2024/05/11/How-to-prevent-duplicate-SQS-messages/"},{"title":"Auth0 lock issue","text":"This Friday when we’re demo case, we noticed that it took a long time to display content on the Home page. It happened occasionally and soon we found it happened only when we had login into ULP but not the HOME page. Particularly, if you remove the cookie flag auth0.is.authenticated and refresh the page, you can reproduce it. You might have to wait for more than 10s. Reproduce and address potential codeThe first step is to check the network activity. The weird thing was: we got token at 1s roughly and it took about 100ms but the first GraphQL request was started at 11s. At first, I guessed probably it was blocked by some requests as there were so many requests online. The good thing is we could reproduce it locally. It proved that my guess was wrong after removing all unimportant requests. Then I began to log some methods with performance.mark and performance.measure. Soon we find the issue was caused by getTokenSilently API. It took more than 5s even the Network showed it took only 100ms. Look into SDKHere’s what I found in that method: 1234567public async getTokenSilently() { options.scope = getUniqueScopes(this.DEFAULT_SCOPE, options.scope); await lock.acquireLock(GET_TOKEN_SILENTLY_LOCK_KEY, 5000); // 20 lines code lock.releaseLock(GET_TOKEN_SILENTLY_LOCK_KEY); return authResult.access_token;} We notice they’re using a lock which is a fresh thing in the FE. What if we get an exception, does it mean we have to wait for 5s? We might invoke this API a few times as we wanna always get a valid token. It’s why the GraphQL request began at 11s. Let’s take a look at the source code and see if they fixed it. Yeah, they were aware of that issue and had already fixed it. And the new one looks like: 12345try { await lock.acquireLock(GET_TOKEN_SILENTLY_LOCK_KEY, 5000);} finally { await lock.releaseLock(GET_TOKEN_SILENTLY_LOCK_KEY);} Let’s upgrade our package file and give it another try. 🎉After a few moments ⏰. What? It still took 5s. Maybe I didn’t update the right one, double-double-check: we did have the new package. There was something different: The first GraphQL began at 7s. Understand the Lock 🔐As we know, we don’t the lock API in FE(There’s an experimental API) They are using the browser-tabs-lock library. It’s used for preventing two tabs send request parallel. Basically, they’re using localStorage to implement the lock. Check if the Certain key is set in lcoalStorage, if not set it and acquire the lock successfully. Otherwise, listen to Storage’s event or wait until timeout. After adding some log statements, I found there was an item in localStorage after redirecting back. Auth page is unable to access it. Therefore, the only reason was we didn’t clean it before redirecting to the Auth page. However, before redirect to the Auth page, there’s nothing on the localStorage. Here’s related code. Have you noticed the problem? 123456789const isAuthenticated = await auth0FromHook.isAuthenticated();if (!isAuthenticated) { // Checked there was no item on lcoalStorage await auth0FromHook.loginWithRedirect({ appState: { targetUrl: window.location.href }, }); // Using location.assign}const token = await auth0FromHook.getTokenSilently();// ... There’s nothing special with loginWithRedirect API. The root cause is the script doesn’t stop after location change. The following code getTokenSilently is trying to acquire the lock but it doesn’t have a chance to release the lock as the location was changed. It’s hard to debug it because we can’t set a breakpoint or print any message after location change. Review of the bugWhen was the issue introduced In the beginning, we didn’t have this issue as the Auth0 was not using the lock. We upgraded the version to 1.5.0 on 13/11/2019. It was a minor change and we didn’t notice that. That means we had that issue since then. Why there was no alert from NewRelic This bug happens occasionally. It might just have a minor impact on the average time. The page load time fluctuates every day and only keep one week’s data. It might not be able to noticed in short term. Lesson from itRemember stop the script after location change.","link":"/en/2020/01/17/journeyman/Auth0-lock-issue/"},{"title":"Babel polyfill","text":"As a front developer, we should keep in mind what browsers are used by our customers. We say only modern browsers and IE 11 are supported, at least users won’t get an error on IE 11. Our config is : ['last 2 versions', 'ie &gt;= 11'] and here’s detailed browsers: last 2 versions. If you find your config is different from this one, please ensure your config is a superset of the above list. Mystery of BabelWith Babel, we could use new features of ES without worrying about compatibility. You must have got some error saying browser doesn’t support it even you’re using babel. Yeah, you might know we still have to take care of polyfill. Have you ever been confused about various babel packages? @babel/preset-env @babel/transform-runtime @babel/runtime @babel/polyfill To help you better understand those, Let’s do some experiment.Let’s say the source code looks like this: 123456789101112131415class A { method() {}}const arr = Array.from(['a']);const s = new Symbol();function* gen() { yield 3;}let array = [1, 2, 3, 4, 5, 6];array.includes(item =&gt; item &gt; 2);const promise = new Promise(); Please note: class arrow function let const are part of the syntax. Promise and Symbol and Array.from belong to global properties and static properties. [].includes is instance property. Preset-envFirstly, we just add preset-env and see what’s the outcode looks like 123[ &quot;@babel/env&quot;,] Outcode(output 1) ⬇️⬇️⬇️⬇️⬇️ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&quot;use strict&quot;;var _marked =/*#__PURE__*/regeneratorRuntime.mark(gen);function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError(&quot;Cannot call a class as a function&quot;); } }function _defineProperties(target, props) { for (var i = 0; i &lt; props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if (&quot;value&quot; in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } }function _createClass(Constructor, protoProps, staticProps) { if (protoProps) _defineProperties(Constructor.prototype, protoProps); if (staticProps) _defineProperties(Constructor, staticProps); return Constructor; }var A =/*#__PURE__*/function () { function A() { _classCallCheck(this, A); } _createClass(A, [{ key: &quot;method&quot;, value: function method() {} }]); return A;}();var arr = Array.from(['a']);var s = new Symbol();function gen() { return regeneratorRuntime.wrap(function gen$(_context) { while (1) { switch (_context.prev = _context.next) { case 0: _context.next = 2; return 3; case 2: case &quot;end&quot;: return _context.stop(); } } }, _marked);}var array = [1, 2, 3, 4, 5, 6];array.includes(function (item) { return item &gt; 2;});var set = new Promise(); As we can see, babel transforms new syntax to the old one for us. But didn’t change the static methods in the new feature. (Notice it’s using global regeneratorRuntime ) If we specify targets only for modern browsers like below: 12345678[ &quot;@babel/env&quot;, { targets: { browsers: ['Chrome 70'], }, }] The outcode is almost same with source code. It’s not surprising. Let’s move on. Now we set useBuiltIns as ‘usage’ 1234567891011[ '@babel/preset-env', { targets: { browsers: ['defaults'], }, useBuiltIns: 'usage', corejs: 3, // using useBuiltIns without declare corejs version will get warning. modules: false, },], We found babel imported a few files for us. 123456789import &quot;core-js/modules/es.symbol&quot;;import &quot;core-js/modules/es.symbol.description&quot;;import &quot;core-js/modules/es.array.from&quot;;import &quot;core-js/modules/es.array.includes&quot;;import &quot;core-js/modules/es.object.to-string&quot;;import &quot;core-js/modules/es.promise&quot;;import &quot;core-js/modules/es.string.iterator&quot;;import &quot;regenerator-runtime/runtime&quot;;// Below is same with output1 From now I set modules as false, so the outcode is using import rather than require. useBuiltIns has another option: ‘entry’. It won’t import polyfill for us. We still have to import polyfill by yourself but it will import specific files in terms of your target setting. 12import 'core-js/stable';import 'regenerator-runtime/runtime'; If in your entry files, you have the above code, it will transform to the below one. (It depends on your target browsers. Even you never use it in your code.) 123456import &quot;core-js/modules/es.symbol.description&quot;;import &quot;core-js/modules/es.symbol.async-iterator&quot;;import &quot;core-js/modules/es.array.flat&quot;;import &quot;core-js/modules/es.array.flat-map&quot;;....// very long Transform-runtimeLet’s disable useBuiltIns and move on. Add plugins to babel config file: &quot;plugins&quot;: [&quot;@babel/plugin-transform-runtime&quot;] The outcode: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import _regeneratorRuntime from &quot;@babel/runtime/regenerator&quot;;import _classCallCheck from &quot;@babel/runtime/helpers/classCallCheck&quot;;import _createClass from &quot;@babel/runtime/helpers/createClass&quot;;var _marked =/*#__PURE__*/_regeneratorRuntime.mark(gen);var A =/*#__PURE__*/function () { function A() { _classCallCheck(this, A); } _createClass(A, [{ key: &quot;method&quot;, value: function method() {} }]); return A;}();var arr = Array.from(['a']);var s = new Symbol();function gen() { return _regeneratorRuntime.wrap(function gen$(_context) { while (1) { switch (_context.prev = _context.next) { case 0: _context.next = 2; return 3; case 2: case &quot;end&quot;: return _context.stop(); } } }, _marked);}var array = [1, 2, 3, 4, 5, 6];array.includes(function (item) { return item &gt; 2;});var promise = new Promise(); This outcode is a bit different from the previous. As the documentation, by default, it set regenerator as true. Besides, it replaced inline helper with the module. But it didn’t import any polyfill files. Let’s try other params: [&quot;@babel/plugin-transform-runtime&quot;, {&quot;corejs&quot;: 3 }] The out code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import _Promise from &quot;@babel/runtime-corejs3/core-js-stable/promise&quot;;import _includesInstanceProperty from &quot;@babel/runtime-corejs3/core-js-stable/instance/includes&quot;;import _regeneratorRuntime from &quot;@babel/runtime-corejs3/regenerator&quot;;import _Symbol from &quot;@babel/runtime-corejs3/core-js-stable/symbol&quot;;import _Array$from from &quot;@babel/runtime-corejs3/core-js-stable/array/from&quot;;import _classCallCheck from &quot;@babel/runtime-corejs3/helpers/classCallCheck&quot;;import _createClass from &quot;@babel/runtime-corejs3/helpers/createClass&quot;;var _marked =/*#__PURE__*/_regeneratorRuntime.mark(gen);var A =/*#__PURE__*/function () { function A() { _classCallCheck(this, A); } _createClass(A, [{ key: &quot;method&quot;, value: function method() {} }]); return A;}();var arr = _Array$from(['a']);var s = new _Symbol();function gen() { return _regeneratorRuntime.wrap(function gen$(_context) { while (1) { switch (_context.prev = _context.next) { case 0: _context.next = 2; return 3; case 2: case &quot;end&quot;: return _context.stop(); } } }, _marked);}var array = [1, 2, 3, 4, 5, 6];_includesInstanceProperty(array).call(array, function (item) { return item &gt; 2;});var promise = new _Promise(); Here you see: it helped us handle new global properties even instance properties in a different way. It was using internal methods, which means it won’t pollute prototype or global namespace. It’s commonly used in the development of the third library. Also, you might have already noticed, we should ensure the packages used in outcode is accessible. @babel/runtime or core-js or regenerator-runtime/runtime (@babel/polyfill has been deprecated. use core-js/stable and regenerator-runtime/runtime directly. ) Which one we should chooseHere’s guide If we take a look at CRA create.js it’s using @babel/plugin-transform-runtime to save on codesize and regenerator polyfill. In @babel/preset-env set useBuiltIns: 'entry', it means we should import polyfill by ourselves. Here’s detailed documentation: https://create-react-app.dev/docs/supported-browsers-features#supported-browsers For us, maybe the best way is aligning our solution to CRA. For the micro front end, the best way is to import polyfill in the shell entry, others don’t need import twice. I know, even we import all polyfill files, it doesn’t make much difference. Anyway, we’re stepping into the right direction.","link":"/en/2019/08/25/journeyman/babel-polyfill-enigma/"},{"title":"A bug which should have been solved a week ago","text":"Recently, we have a DS ticket that said a user got banner error periodically on the home page. That means we got some BE errors on API requests. I checked NewRelic and nothing exception was found. I checked error logs on our server and only a few 500 errors. I can’t find further information about these errors. So I believe it was caused by an unstable network or it might be caused. I was not working on that task. Until yesterday we were going to solve a cache memory issue and I still not realized that was caused by cache memory. After I had submitted that PR of fixing cache memory, I decided to look that DS ticket again. I notice there’s some clue. I could find banner errors on fullstory and that means we did get some request errors. Then I checked request logs on Cloudflare and here are unsuccessful requests. We should notice that not all the unsuccessful requests matter because some of them are from scanners and attackers. If we look into the intensive bar, we notice that most of the errors are 5** error. We got this error probably because of the unavailable server. Let’s take a look at all 502 errors. Then I notice that the chart is highly correlated to our server up-down. So, That errors must be caused by the memory issue. Basically, we’re caching the requests to microservices. The problem is the cache instance is infinite by default. https://github.com/apollographql/apollo-server/issues/2252 Misunderstand about zero downtimeAs we might know when we’re deploying a new update. A new docker image will be created and we start up 2 new instances. Then the load balancer will refer to the new instances once they are healthy. Then it’s safe for us to delete the old instances. I didn’t take it seriously when I saw the containers were down and up. As I thought it should be looked after by AWS agents and we won’t have downtime. When I looked at the chart, I was further convinced. Look, before the old container is down, a new container is already up. It’s awesome! Unfortunately, we still got 502 errors. But Why? 🤔 The reason is we were not closing the old container deliberately. The old container dead of using out of memory. During that time(might be a few seconds), the request was assigned to that old container. Self ReviewI didn’t know that the max size is infinite in the beginning. I did find that memory was increasing a few weeks ago but I didn’t take it seriously as the misunderstanding I mentioned above. I could do better is to solve it immediately. Two things to help to debug: Talk to the people who report that bug; Check everything in that task. Some tips of troubleshootingThe most important thing to debug is to restore the error. If we could reproduce it, usually it’s easy to fix it. Basically, it’s hard to debug occasional errors. Once we know some user info, we could check fullstory and see the original errors. Once we know the accurate time, we could check all the logs during that period. The last critical thing from this lesson is prepare it beforehand. Before any alerts are triggered, we’d better understand what normal error we have. In the next few days, I get to watch the error logs of Cloudflare, Newrelic and ensure I understand every error.","link":"/en/2020/02/14/novice/cache-memory-issue/"}],"tags":[{"name":"AWS","slug":"AWS","link":"/en/tags/AWS/"},{"name":"Client","slug":"Client","link":"/en/tags/Client/"},{"name":"Serviceless","slug":"Serviceless","link":"/en/tags/Serviceless/"},{"name":"Amazon connect","slug":"Amazon-connect","link":"/en/tags/Amazon-connect/"},{"name":"DynamoDB","slug":"DynamoDB","link":"/en/tags/DynamoDB/"},{"name":"SQS Processor","slug":"SQS-Processor","link":"/en/tags/SQS-Processor/"},{"name":"FE","slug":"FE","link":"/en/tags/FE/"},{"name":"Auth0","slug":"Auth0","link":"/en/tags/Auth0/"},{"name":"Lock","slug":"Lock","link":"/en/tags/Lock/"},{"name":"babel","slug":"babel","link":"/en/tags/babel/"},{"name":"troubleshooting","slug":"troubleshooting","link":"/en/tags/troubleshooting/"},{"name":"apollo-data-source","slug":"apollo-data-source","link":"/en/tags/apollo-data-source/"}],"categories":[{"name":"Explorations","slug":"Explorations","link":"/en/categories/Explorations/"}],"pages":[{"title":"About","text":"Fedeoo, 工程师，工作前7年主要做前端开发，这两年开始做全栈开发。正值而立之年，惟愿多多读些书，更好的认知这个世界。 选鲨鱼图的原因是：鲨鱼永不停止游动，提醒自己学习永不止步。 毕业渣浪工作两年，之后西厂工作一段时间，现就职悉尼一家小厂。 博客主要是放一些技术收获，考虑放入更多的技术思考，因为单纯去讲一个技术点，谷歌很容易找到很有深度的文章，没必要做搬运工了。 用博客记录成长，坚持定期自省。只有在静下来思考时，才能好好的反省和检视自己。","link":"/en/about/index.html"}]}