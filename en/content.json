{"posts":[{"title":"AWS client getaddrinfo EMFILE issue","text":"Recently, we introduced AWS Cloud Map for service discovery, primarily to retrieve queue URLs. However, after deployment, we encountered intermittent errors weeks later, logged as getaddrinfo EMFILE events.ap-southeast-2.amazonaws.com. Not all requests triggered this error, indicating a selective issue. Upon inspection, it became apparent that we were facing a socket timeout problem, a known issue in our setup. The remedy was simple: reusing our existing agent. 1const keepAliveAgent = new HttpsAgent({ keepAlive: true, maxSockets: 500 }); Unpacking the IssueIn our codebase, each lambda utilizes a createService function to generate service instances for each request/run. Attempts to switch to a singleton instance were met with complications, prompting a rollback. The core problem emerged: with a new client instance created for each request, sockets held by the HTTP agent from each run might not be released, leading to hitting the socket limit before garbage collection could free them. During periods of low traffic, the issue remained dormant, with lambda invocations being infrequent. However, the potential for overload was evident, given that a lambda can manage up to 1024 outgoing connections, a threshold that could be gradually reached over time. A similar issue surfaced in other resource integrations. Investigation revealed that our OpenAPI-generated client was using a singleton instance: 1const keepAliveAgent = new HttpsAgent({ keepAlive: true, timeout: 15000 }); Key TakeawaysReflecting on this incident, we realized some fundamental truths. While such mistakes may seem basic, they underscore the complexities that emerge as systems evolve. Take away:keepAlive is not default in AWS SDK, you need to set it manually even itâ€™s recommended to maintain persistent connections.In SDK v2, thereâ€™s a easy way to do that by setting AWS_NODEJS_CONNECTION_REUSE_ENABLED=1. But it seems removed in SDK v3.Interestingly, http.globalAgent in Node.js set it as default since v19.0.0 https://nodejs.org/en/blog/announcements/v19-release-announce/#http-s-1-1-keepalive-by-default","link":"/en/2024/05/16/AWS-client-getaddrinfo-EMFILE-issue/"},{"title":"AWS connect last agent call routing","text":"ChallengeIn a recent project, we were assigned the task to route calls to the last agent who interacted with the caller.This requirement may seem fundamental, but it proved to be more complex than anticipated,especially considering its integration into Salesforce via Salesforce Service Cloud Voice. SolutionThe proposed solution is relatively straightforward.We will employ a Lambda function to query recent voice calls and retrieve information about the previous queue and agent.If the agent is available and the customer wishes to continue the previous call, the call will be transferred to that same agent.However, should any errors occur during this process, the call will follow the normal flow. In summary: A Lambda function will retrieve details about the customerâ€™s previous call, including the queue and agent. If the previous agent is available and the customer wants to resume the call, the new call will be transferred to that agent. If any issues arise during this process (such as agent unavailability or customer preference), the call will proceed through the regular call flow. ImplementationUpdate IVRFirst, invoke the Lambda to retrieve the last agent and queue, then set the agent to the working queue.Before transferring the call to the queue, we need to check if the agent is available and provide the customer with the option to continue the previous call or not.The IVR flow is depicted as follows: Lambda Get Last Voice CallNow, the only remaining task is to retrieve the last agent.Unfortunately, the SearchContactsCommand from aws client does not support searching for contacts by customer phone number, possibly due to privacy concerns.As a workaround, we can set a custom contact attribute (e.g., CustomerPhoneNumber) and mark it as searchable.Then, using the SearchContactsCommand, we can search for contacts by the searchable attributes.Itâ€™s important to note that contacts created before adding the searchable key will still not be searchable.Additionally, we can set a time range in this Lambda to search for contacts within the past 7 days. The code snippets are as follows: 1234567891011121314151617181920212223242526272829303132const input: SearchContactsRequest = { InstanceId, TimeRange: { Type: &quot;INITIATION_TIMESTAMP&quot;, StartTime: new Date(Date.now() - 7 * 24 * 3600 * 1000), // Since 7 days ago EndTime: new Date(), }, SearchCriteria: { Channels: [ &quot;VOICE&quot;, ], SearchableContactAttributes: { Criteria: [ { Key: &quot;CustomerPhoneNumber&quot;, Values: [ phoneNumber, ], }, ], MatchType: &quot;MATCH_ALL&quot;, }, }, MaxResults: 10, Sort: { FieldName: &quot;INITIATION_TIMESTAMP&quot;, Order: &quot;DESCENDING&quot;, },};const command = new SearchContactsCommand(input);const response = await client.send(command);const lastContact = (response.Contacts ?? []).filter((contact) =&gt; contact.AgentInfo?.Id)[0] ConclusionAt last yearâ€™s AWS conference, I observed a promising demonstration of Amazon Connect integrated with AI, highlighting its potential to proactively resolve customer issues.However, I was surprised to discover that implementing last-agent call routing, which I deemed fundamental, lacked an out-of-the-box solution in Amazon Connect.This realization prompted us to develop a custom solution to address this critical requirement. Reference post Last Agent and Last Queue Routing on Amazon Connect for Returning Callers","link":"/en/2024/05/21/AWS-connect-last-agent-call-routing/"},{"title":"DynamoDB the Advantages and Considerations of Single Table Design","text":"Developers familiar with Relational Database design often find themselves initially drawn to the familiar territory of normalization when designing data models for DynamoDB. This instinct typically leads them towards whatâ€™s known as multi-table design, where each entity or relationship resides in a separate table. On the other hand, DynamoDBâ€™s schemaless nature encourages a different approach: single-table design, where all entities and relationships coexist within a single table. However, itâ€™s worth noting that these two designs represent extremes on a spectrum, rather than strict boundaries. According to the official documentation, single-table design is often recommended. This article explores the advantages of single-table design based on practical experience. Our project predominantly utilizes single-table design, largely influenced by ã€ŠThe DynamoDB Bookã€‹, authored by advocate Alex DeBrie. Despite our commitment to single-table design, we still manage more than a dozen tables, albeit with a focus on storing related entities together. Advantages of single table design1. Data Locality for Efficient QueriesSingle-table design allows for fetching multiple entity types in a single database call, reducing requests, improving performance, and cutting costs. However, realizing this advantage in practice requires deliberate structuring, as applications often overlook underlying implementation details. In the table below, user information and order information use the same Partition Key, in the same itemCollection, you can get user information and order information in one Query request. The problem in reality: realizing this advantage in practice requires deliberate structuring, as applications often overlook the underlying implementation details.In practice, we donâ€™t care about the order of entity types in the same itemCollection, usually still keep 2 requests for different entities each in same itemCollection. 2. Reduces overall financial and latency costs of reads: A single query for two items totalling less than 4KB is 0.5 RCU eventually consistentTwo queries for two items totalling less than 4KB is 1 RCU eventually consistent (0.5 RCU each)The time to return two separate database calls will average higher than a single call The advantages of the bill are quite obvious: reducing the number of requests naturally reduces the cost of the bill, usually a single entity record will not be very large, and multiple entity records may not necessarily have 4KB.The advantage of latency is also easy to understand, a single request is usually faster than two separate requests. However, this advantage based on assumptions that you can save another request call, which is not always the case in practice. 3. Reduces the number of tables to manage: Permissions do not need to be maintained across multiple IAM roles or policiesCapacity management for the table is averaged across all entities, usually resulting in a more predictable consumption patternMonitoring requires fewer alarmsCustomer Managed Encryption Keys only need to be rotated on one table Permissions do not need to be maintained across multiple IAM roles or policies, which also means that the granularity of permissions is not so fine. In practice, we already feel that our permissions are too fine, and each Lambda has different role permissions.These advantages are real, single-table design is easier to manage, does not require more configuration, and does not require access permissions for each table in each Lambda. 4. Smooths traffic to the table: By aggregating multiple usage patterns to the same table, the overall usage tends to be smoother (the way a stock indexâ€™s performance tends to be smoother than any individual stock) which works better for achieving higher utilization with provisioned mode tablesNot much to say, the advantage is not obvious in practice Disadvantages Learning curve can be steep due to paradoxical design compared to relational databasesUnderstanding the fundamental principles of DynamoDB, grasping the single-table design concept isnâ€™t overly complex. However, there are associated trade-offs to consider, and the learning curve isnâ€™t particularly steep. In our experience, incorporating a new entity type into an existing table proves far simpler than introducing a new table altogether. Data requirements must be consistent across all entity types All changed data will be propagated to DynamoDB Streams even if only a subset of entities need to be processed. When using GraphQL, single table design will be more difficult to implement When using higher-level SDK clients like Javaâ€™s DynamoDBMapper or Enhanced Client, it can be more difficult to process results because items in the same response may be associated with different classes SummaryDynamoDBâ€™s flexibility allows for a spectrum of design choices, with no strict delineation between single-table and multi-table approaches. Advocates typically recommend a per-service table, challenging developers to break free from the relational database mindset. Understanding DynamoDBâ€™s core concepts is essential. From there, the choice between single-table and multi-table design should align with specific project requirements. For further exploration: Single-table vs. multi-table design in Amazon DynamoDB Single table design for DynamoDB: The reality The What, Why, and When of Single-Table Design with DynamoDB","link":"/en/2024/06/02/DynamoDB%20single%20table%20design/"},{"title":"How to prevent duplicate SQS messages","text":"ProblemIn our system, queue processors must implement idempotency to prevent the double-processing of messages. Duplicate messages may arise in the following scenarios: Scheduler and Message Producer: The scheduler or message producer may be triggered multiple times, occasionally rerunning due to timeouts. Queue Management: If a lambda instance times out while processing a message, another instance may retrieve the same message if the visibility timeout is not properly set. This can have terrible consequences. We aim to avoid sending duplicate emails or messages to our customers, not to mention inadvertently delivering duplicate gift cards. So a generic idempotency mechanism is required. SolutionThe idea is straightforward: we will use a DynamoDB / Redis cache to store the message ID and the processing status. When a message is received, we will check the record to see if it has been processed. If it has, we will skip the message. If not, we will process it and update the cache. Considering our serverless architecture, DynamoDB is selected. Basically, there are three cases: Message first time processed: process the message. Message is being processed or has been processed: discard the message. Message processing failed: reprocess the message.To handle this case, we need to add a lock timeout to the record. If the message is still in processing status after the lock timeout, we give it another chance to be processed. Implementation Create a DynamoDB table message-processor. Itâ€™s a normal table with a primary key messageId. Implement a service with this interface:12345678910111213141516171819202122interface IMessageProcessorService { /** * Here use DynamoDB message-processor table as the fact store to decide if a message has been seen before * @param messageId unique identifier for each message * @param lockTimeoutInSeconds how long to lock the message for processing. It gives another chance to reprocess the message if it fails. * @returns boolean: true indicates the lock is acquired and should continue the processing. * false indicates the message is already being processed or being processed by another instance. */ acquireProcessingLock(messageId: string, lockTimeoutInSeconds: number): Promise&lt;boolean&gt;; /** * Mark the message as processed, preventing it from being processed again * @param messageId */ markMessageProcessed(messageId: string): Promise&lt;void&gt;; /** * Remove record of failed message processing, allowing it to be processed again * @param messageId */ releaseProcessingLock(messageId: string): Promise&lt;void&gt;;} The code snippet below shows how to implement the acquireProcessingLock method:(Never mind, weâ€™re using internal libraries to simplify the code) 1234567891011121314151617181920await this.store.replace( { _id: id, status: 'PROCESSING', timestamp: Date.now(), }, { condition: { $or: [ { _id: { $exists: false } }, // insert new record { $and: [ { timestamp: { $lt: Date.now() - lockTimeoutInSeconds * 1000 } }, { status: { $eq: 'PROCESSING' } }, ], }, ], }, },); At last, we enhance the existing message handler with the idempotency mechanism: 12345678910111213141516171819202122232425262728293031323334export const makeHandlerIdempotent = async &lt;T&gt;( handler: MessageHandler&lt;T&gt;, IdGenerator: (message: T) =&gt; string, { messageProcessorService, lockTimeoutInSeconds, logger, }: { logger: ILoggerService; messageProcessorService: IMessageProcessorService; lockTimeoutInSeconds: number; },): Promise&lt;MessageHandler&lt;T&gt;&gt; =&gt; { return async (message: T) =&gt; { const id = IdGenerator(message); const acquiredProcessingExclusiveLock = await messageProcessorService.acquireProcessingLock( id, lockTimeoutInSeconds, ); if (!acquiredProcessingExclusiveLock) { logger.info('processMessageIdempotent: message has already been processed', { message }); return; } try { const result = await handler(message); await messageProcessorService.markMessageProcessed(id); return result; } catch (error) { await messageProcessorService.releaseProcessingLock(id); throw error; } };}; ConclusionIt seems preventing duplicate messages in a distributed system is likely a common requirement.While implementing this idempotency mechanism, I found almost similar solution discussed How to prevent duplicate SQS Messages?.It was very helpful and offered clear explanations.","link":"/en/2024/05/11/How-to-prevent-duplicate-SQS-messages/"},{"title":"Page Object Model Pattern","text":"Every team seems to have its own pattern for writing E2E tests.Joining a new team made me realise that Page Object Models (POM) are still surprisingly debated.Why do people argue about it so much? Why is there no standard way? The truth is: there is no universally agreed best practice for Page Objects in modern E2E testing. Hereâ€™s my perspective after working across Selenium, Cypress, and now Playwright. What Page Objects try to solveAt its core, POM tries to address two concerns: Encapsulation â€“ keep UI knowledge and selectors inside page objects Separation of Responsibilities â€“ tests describe behaviour, page objects describe UI interactions Selenium: Strict SeparationIn Selenium, I used to follow a very rigid pattern: page objects handled only actions and navigation, while assertions lived exclusively in the test files. 123456789SendMoneyPage .selectBSB() .fillBSBAccount(bsb, accountName, accountNumber) .tapCheck() // returns MatchResultPage .run((page) =&gt; { // Assertions ONLY in tests expect(page.getContact()).toEqual({}); }) .tapContinue(); // returns next page Rules: Page Objects contain no assertions Page Objects describe behaviour and navigation only Verification logic stays in tests This may look strange if youâ€™ve never used classic POM, but it follows the principles from Martin Fowlerâ€™s original article on Page Object Cypress: Function-Based helpersCypress takes the opposite stance. It discourages Page Objects completely and encourages simple reusable functions: 12345function fillBSBAccount(bsb: string, accountName: string, accountNumber: string) { cy.getByLabel('bsb').type(bsb); cy.getByLabel('accountName').type(accountName); cy.getByLabel('accountNumber').type(accountNumber);} Cypress philosophy:â€œJust write the story.â€Its fluent command chain makes POM feel unnecessary. Playwright: Pragmatic Page ObjetsPlaywright reintroduces Page Objects, but with a different philosophy. The key difference:Playwright has built-in auto-waiting, auto-retry, and strong assertions. These features fundamentally change how POM should be structured. The key realisationThe classic â€œSeparation of Responsibilitiesâ€ becomes less practical with Playwright. Example: checking if a button is visible. A strict POM approach: 1expect(page.isButtonVisible()).toBe(true) This is actually worse: isButtonVisible() doesnâ€™t retry Assertions on booleans donâ€™t retry You bypass Playwrightâ€™s reliability system The recommended approach: 1expect(page.buttonLocator).toBeVisible(); // Built-in auto-retry Playwrightâ€™s documentation emphasises:: Page objects simplify authoring by creating a higher-level API which suits your application and simplify maintenance by capturing element selectors in one place and create reusable code to avoid repetition. Soâ€¦ is â€œSeparation of Responsibilitiesâ€ still relevant?Not really. E2E tests are naturally narrative-driven. They read like stories: â€œLog in, click this, expect that.â€ Cypress embraces this.Playwright mostly embraces this too, with optional structure via Page Objects. Encapsulation still matters â€” grouping selectors and common actions improves readability and maintenance.But strict, academic POM rules? They matter far less today. Final thoughtThere is no universal â€œcorrectâ€ Page Object pattern anymore.Modern frameworks â€” especially Playwright â€” optimise for reliability, not structure. So instead of asking: â€œWhat is the right Page Object pattern?â€ A better question is: â€œWhy am I using this pattern, and is it helping my tests stay readable, maintainable, and reliable?â€ Thatâ€™s the actual purpose of Page Objects â€” everything else is just preference.","link":"/en/2025/10/16/page-object-model-pattern/"},{"title":"Auth0 lock issue","text":"This Friday when weâ€™re demo case, we noticed that it took a long time to display content on the Home page. It happened occasionally and soon we found it happened only when we had login into ULP but not the HOME page. Particularly, if you remove the cookie flag auth0.is.authenticated and refresh the page, you can reproduce it. You might have to wait for more than 10s. Reproduce and address potential codeThe first step is to check the network activity. The weird thing was: we got token at 1s roughly and it took about 100ms but the first GraphQL request was started at 11s. At first, I guessed probably it was blocked by some requests as there were so many requests online. The good thing is we could reproduce it locally. It proved that my guess was wrong after removing all unimportant requests. Then I began to log some methods with performance.mark and performance.measure. Soon we find the issue was caused by getTokenSilently API. It took more than 5s even the Network showed it took only 100ms. Look into SDKHereâ€™s what I found in that method: 1234567public async getTokenSilently() { options.scope = getUniqueScopes(this.DEFAULT_SCOPE, options.scope); await lock.acquireLock(GET_TOKEN_SILENTLY_LOCK_KEY, 5000); // 20 lines code lock.releaseLock(GET_TOKEN_SILENTLY_LOCK_KEY); return authResult.access_token;} We notice theyâ€™re using a lock which is a fresh thing in the FE. What if we get an exception, does it mean we have to wait for 5s? We might invoke this API a few times as we wanna always get a valid token. Itâ€™s why the GraphQL request began at 11s. Letâ€™s take a look at the source code and see if they fixed it. Yeah, they were aware of that issue and had already fixed it. And the new one looks like: 12345try { await lock.acquireLock(GET_TOKEN_SILENTLY_LOCK_KEY, 5000);} finally { await lock.releaseLock(GET_TOKEN_SILENTLY_LOCK_KEY);} Letâ€™s upgrade our package file and give it another try. ğŸ‰After a few moments â°. What? It still took 5s. Maybe I didnâ€™t update the right one, double-double-check: we did have the new package. There was something different: The first GraphQL began at 7s. Understand the Lock ğŸ”As we know, we donâ€™t the lock API in FE(Thereâ€™s an experimental API) They are using the browser-tabs-lock library. Itâ€™s used for preventing two tabs send request parallel. Basically, theyâ€™re using localStorage to implement the lock. Check if the Certain key is set in lcoalStorage, if not set it and acquire the lock successfully. Otherwise, listen to Storageâ€™s event or wait until timeout. After adding some log statements, I found there was an item in localStorage after redirecting back. Auth page is unable to access it. Therefore, the only reason was we didnâ€™t clean it before redirecting to the Auth page. However, before redirect to the Auth page, thereâ€™s nothing on the localStorage. Hereâ€™s related code. Have you noticed the problem? 123456789const isAuthenticated = await auth0FromHook.isAuthenticated();if (!isAuthenticated) { // Checked there was no item on lcoalStorage await auth0FromHook.loginWithRedirect({ appState: { targetUrl: window.location.href }, }); // Using location.assign}const token = await auth0FromHook.getTokenSilently();// ... Thereâ€™s nothing special with loginWithRedirect API. The root cause is the script doesnâ€™t stop after location change. The following code getTokenSilently is trying to acquire the lock but it doesnâ€™t have a chance to release the lock as the location was changed. Itâ€™s hard to debug it because we canâ€™t set a breakpoint or print any message after location change. Review of the bugWhen was the issue introduced In the beginning, we didnâ€™t have this issue as the Auth0 was not using the lock. We upgraded the version to 1.5.0 on 13/11/2019. It was a minor change and we didnâ€™t notice that. That means we had that issue since then. Why there was no alert from NewRelic This bug happens occasionally. It might just have a minor impact on the average time. The page load time fluctuates every day and only keep one weekâ€™s data. It might not be able to noticed in short term. Lesson from itRemember stop the script after location change.","link":"/en/2020/01/17/journeyman/Auth0-lock-issue/"},{"title":"Babel polyfill","text":"As a front developer, we should keep in mind what browsers are used by our customers. We say only modern browsers and IE 11 are supported, at least users wonâ€™t get an error on IE 11. Our config is : ['last 2 versions', 'ie &gt;= 11'] and hereâ€™s detailed browsers: last 2 versions. If you find your config is different from this one, please ensure your config is a superset of the above list. Mystery of BabelWith Babel, we could use new features of ES without worrying about compatibility. You must have got some error saying browser doesnâ€™t support it even youâ€™re using babel. Yeah, you might know we still have to take care of polyfill. Have you ever been confused about various babel packages? @babel/preset-env @babel/transform-runtime @babel/runtime @babel/polyfill To help you better understand those, Letâ€™s do some experiment.Letâ€™s say the source code looks like this: 123456789101112131415class A { method() {}}const arr = Array.from(['a']);const s = new Symbol();function* gen() { yield 3;}let array = [1, 2, 3, 4, 5, 6];array.includes(item =&gt; item &gt; 2);const promise = new Promise(); Please note: class arrow function let const are part of the syntax. Promise and Symbol and Array.from belong to global properties and static properties. [].includes is instance property. Preset-envFirstly, we just add preset-env and see whatâ€™s the outcode looks like 123[ &quot;@babel/env&quot;,] Outcode(output 1) â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&quot;use strict&quot;;var _marked =/*#__PURE__*/regeneratorRuntime.mark(gen);function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError(&quot;Cannot call a class as a function&quot;); } }function _defineProperties(target, props) { for (var i = 0; i &lt; props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if (&quot;value&quot; in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } }function _createClass(Constructor, protoProps, staticProps) { if (protoProps) _defineProperties(Constructor.prototype, protoProps); if (staticProps) _defineProperties(Constructor, staticProps); return Constructor; }var A =/*#__PURE__*/function () { function A() { _classCallCheck(this, A); } _createClass(A, [{ key: &quot;method&quot;, value: function method() {} }]); return A;}();var arr = Array.from(['a']);var s = new Symbol();function gen() { return regeneratorRuntime.wrap(function gen$(_context) { while (1) { switch (_context.prev = _context.next) { case 0: _context.next = 2; return 3; case 2: case &quot;end&quot;: return _context.stop(); } } }, _marked);}var array = [1, 2, 3, 4, 5, 6];array.includes(function (item) { return item &gt; 2;});var set = new Promise(); As we can see, babel transforms new syntax to the old one for us. But didnâ€™t change the static methods in the new feature. (Notice itâ€™s using global regeneratorRuntime ) If we specify targets only for modern browsers like below: 12345678[ &quot;@babel/env&quot;, { targets: { browsers: ['Chrome 70'], }, }] The outcode is almost same with source code. Itâ€™s not surprising. Letâ€™s move on. Now we set useBuiltIns as â€˜usageâ€™ 1234567891011[ '@babel/preset-env', { targets: { browsers: ['defaults'], }, useBuiltIns: 'usage', corejs: 3, // using useBuiltIns without declare corejs version will get warning. modules: false, },], We found babel imported a few files for us. 123456789import &quot;core-js/modules/es.symbol&quot;;import &quot;core-js/modules/es.symbol.description&quot;;import &quot;core-js/modules/es.array.from&quot;;import &quot;core-js/modules/es.array.includes&quot;;import &quot;core-js/modules/es.object.to-string&quot;;import &quot;core-js/modules/es.promise&quot;;import &quot;core-js/modules/es.string.iterator&quot;;import &quot;regenerator-runtime/runtime&quot;;// Below is same with output1 From now I set modules as false, so the outcode is using import rather than require. useBuiltIns has another option: â€˜entryâ€™. It wonâ€™t import polyfill for us. We still have to import polyfill by yourself but it will import specific files in terms of your target setting. 12import 'core-js/stable';import 'regenerator-runtime/runtime'; If in your entry files, you have the above code, it will transform to the below one. (It depends on your target browsers. Even you never use it in your code.) 123456import &quot;core-js/modules/es.symbol.description&quot;;import &quot;core-js/modules/es.symbol.async-iterator&quot;;import &quot;core-js/modules/es.array.flat&quot;;import &quot;core-js/modules/es.array.flat-map&quot;;....// very long Transform-runtimeLetâ€™s disable useBuiltIns and move on. Add plugins to babel config file: &quot;plugins&quot;: [&quot;@babel/plugin-transform-runtime&quot;] The outcode: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import _regeneratorRuntime from &quot;@babel/runtime/regenerator&quot;;import _classCallCheck from &quot;@babel/runtime/helpers/classCallCheck&quot;;import _createClass from &quot;@babel/runtime/helpers/createClass&quot;;var _marked =/*#__PURE__*/_regeneratorRuntime.mark(gen);var A =/*#__PURE__*/function () { function A() { _classCallCheck(this, A); } _createClass(A, [{ key: &quot;method&quot;, value: function method() {} }]); return A;}();var arr = Array.from(['a']);var s = new Symbol();function gen() { return _regeneratorRuntime.wrap(function gen$(_context) { while (1) { switch (_context.prev = _context.next) { case 0: _context.next = 2; return 3; case 2: case &quot;end&quot;: return _context.stop(); } } }, _marked);}var array = [1, 2, 3, 4, 5, 6];array.includes(function (item) { return item &gt; 2;});var promise = new Promise(); This outcode is a bit different from the previous. As the documentation, by default, it set regenerator as true. Besides, it replaced inline helper with the module. But it didnâ€™t import any polyfill files. Letâ€™s try other params: [&quot;@babel/plugin-transform-runtime&quot;, {&quot;corejs&quot;: 3 }] The out code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import _Promise from &quot;@babel/runtime-corejs3/core-js-stable/promise&quot;;import _includesInstanceProperty from &quot;@babel/runtime-corejs3/core-js-stable/instance/includes&quot;;import _regeneratorRuntime from &quot;@babel/runtime-corejs3/regenerator&quot;;import _Symbol from &quot;@babel/runtime-corejs3/core-js-stable/symbol&quot;;import _Array$from from &quot;@babel/runtime-corejs3/core-js-stable/array/from&quot;;import _classCallCheck from &quot;@babel/runtime-corejs3/helpers/classCallCheck&quot;;import _createClass from &quot;@babel/runtime-corejs3/helpers/createClass&quot;;var _marked =/*#__PURE__*/_regeneratorRuntime.mark(gen);var A =/*#__PURE__*/function () { function A() { _classCallCheck(this, A); } _createClass(A, [{ key: &quot;method&quot;, value: function method() {} }]); return A;}();var arr = _Array$from(['a']);var s = new _Symbol();function gen() { return _regeneratorRuntime.wrap(function gen$(_context) { while (1) { switch (_context.prev = _context.next) { case 0: _context.next = 2; return 3; case 2: case &quot;end&quot;: return _context.stop(); } } }, _marked);}var array = [1, 2, 3, 4, 5, 6];_includesInstanceProperty(array).call(array, function (item) { return item &gt; 2;});var promise = new _Promise(); Here you see: it helped us handle new global properties even instance properties in a different way. It was using internal methods, which means it wonâ€™t pollute prototype or global namespace. Itâ€™s commonly used in the development of the third library. Also, you might have already noticed, we should ensure the packages used in outcode is accessible. @babel/runtime or core-js or regenerator-runtime/runtime (@babel/polyfill has been deprecated. use core-js/stable and regenerator-runtime/runtime directly. ) Which one we should chooseHereâ€™s guide If we take a look at CRA create.js itâ€™s using @babel/plugin-transform-runtime to save on codesize and regenerator polyfill. In @babel/preset-env set useBuiltIns: 'entry', it means we should import polyfill by ourselves. Hereâ€™s detailed documentation: https://create-react-app.dev/docs/supported-browsers-features#supported-browsers For us, maybe the best way is aligning our solution to CRA. For the micro front end, the best way is to import polyfill in the shell entry, others donâ€™t need import twice. I know, even we import all polyfill files, it doesnâ€™t make much difference. Anyway, weâ€™re stepping into the right direction.","link":"/en/2019/08/25/journeyman/babel-polyfill-enigma/"},{"title":"A bug which should have been solved a week ago","text":"Recently, we have a DS ticket that said a user got banner error periodically on the home page. That means we got some BE errors on API requests. I checked NewRelic and nothing exception was found. I checked error logs on our server and only a few 500 errors. I canâ€™t find further information about these errors. So I believe it was caused by an unstable network or it might be caused. I was not working on that task. Until yesterday we were going to solve a cache memory issue and I still not realized that was caused by cache memory. After I had submitted that PR of fixing cache memory, I decided to look that DS ticket again. I notice thereâ€™s some clue. I could find banner errors on fullstory and that means we did get some request errors. Then I checked request logs on Cloudflare and here are unsuccessful requests. We should notice that not all the unsuccessful requests matter because some of them are from scanners and attackers. If we look into the intensive bar, we notice that most of the errors are 5** error. We got this error probably because of the unavailable server. Letâ€™s take a look at all 502 errors. Then I notice that the chart is highly correlated to our server up-down. So, That errors must be caused by the memory issue. Basically, weâ€™re caching the requests to microservices. The problem is the cache instance is infinite by default. https://github.com/apollographql/apollo-server/issues/2252 Misunderstand about zero downtimeAs we might know when weâ€™re deploying a new update. A new docker image will be created and we start up 2 new instances. Then the load balancer will refer to the new instances once they are healthy. Then itâ€™s safe for us to delete the old instances. I didnâ€™t take it seriously when I saw the containers were down and up. As I thought it should be looked after by AWS agents and we wonâ€™t have downtime. When I looked at the chart, I was further convinced. Look, before the old container is down, a new container is already up. Itâ€™s awesome! Unfortunately, we still got 502 errors. But Why? ğŸ¤” The reason is we were not closing the old container deliberately. The old container dead of using out of memory. During that time(might be a few seconds), the request was assigned to that old container. Self ReviewI didnâ€™t know that the max size is infinite in the beginning. I did find that memory was increasing a few weeks ago but I didnâ€™t take it seriously as the misunderstanding I mentioned above. I could do better is to solve it immediately. Two things to help to debug: Talk to the people who report that bug; Check everything in that task. Some tips of troubleshootingThe most important thing to debug is to restore the error. If we could reproduce it, usually itâ€™s easy to fix it. Basically, itâ€™s hard to debug occasional errors. Once we know some user info, we could check fullstory and see the original errors. Once we know the accurate time, we could check all the logs during that period. The last critical thing from this lesson is prepare it beforehand. Before any alerts are triggered, weâ€™d better understand what normal error we have. In the next few days, I get to watch the error logs of Cloudflare, Newrelic and ensure I understand every error.","link":"/en/2020/02/14/novice/cache-memory-issue/"}],"tags":[{"name":"AWS","slug":"AWS","link":"/en/tags/AWS/"},{"name":"Client","slug":"Client","link":"/en/tags/Client/"},{"name":"Serviceless","slug":"Serviceless","link":"/en/tags/Serviceless/"},{"name":"Amazon connect","slug":"Amazon-connect","link":"/en/tags/Amazon-connect/"},{"name":"DynamoDB","slug":"DynamoDB","link":"/en/tags/DynamoDB/"},{"name":"SQS Processor","slug":"SQS-Processor","link":"/en/tags/SQS-Processor/"},{"name":"E2E","slug":"E2E","link":"/en/tags/E2E/"},{"name":"FE","slug":"FE","link":"/en/tags/FE/"},{"name":"Auth0","slug":"Auth0","link":"/en/tags/Auth0/"},{"name":"Lock","slug":"Lock","link":"/en/tags/Lock/"},{"name":"babel","slug":"babel","link":"/en/tags/babel/"},{"name":"troubleshooting","slug":"troubleshooting","link":"/en/tags/troubleshooting/"},{"name":"apollo-data-source","slug":"apollo-data-source","link":"/en/tags/apollo-data-source/"}],"categories":[{"name":"Explorations","slug":"Explorations","link":"/en/categories/Explorations/"}],"pages":[{"title":"About","text":"Fedeoo, å·¥ç¨‹å¸ˆï¼Œå·¥ä½œå‰7å¹´ä¸»è¦åšå‰ç«¯å¼€å‘ï¼Œè¿™ä¸¤å¹´å¼€å§‹åšå…¨æ ˆå¼€å‘ã€‚æ­£å€¼è€Œç«‹ä¹‹å¹´ï¼ŒæƒŸæ„¿å¤šå¤šè¯»äº›ä¹¦ï¼Œæ›´å¥½çš„è®¤çŸ¥è¿™ä¸ªä¸–ç•Œã€‚ é€‰é²¨é±¼å›¾çš„åŸå› æ˜¯ï¼šé²¨é±¼æ°¸ä¸åœæ­¢æ¸¸åŠ¨ï¼Œæé†’è‡ªå·±å­¦ä¹ æ°¸ä¸æ­¢æ­¥ã€‚ æ¯•ä¸šæ¸£æµªå·¥ä½œä¸¤å¹´ï¼Œä¹‹åè¥¿å‚å·¥ä½œä¸€æ®µæ—¶é—´ï¼Œç°å°±èŒæ‚‰å°¼ä¸€å®¶å°å‚ã€‚ åšå®¢ä¸»è¦æ˜¯æ”¾ä¸€äº›æŠ€æœ¯æ”¶è·ï¼Œè€ƒè™‘æ”¾å…¥æ›´å¤šçš„æŠ€æœ¯æ€è€ƒï¼Œå› ä¸ºå•çº¯å»è®²ä¸€ä¸ªæŠ€æœ¯ç‚¹ï¼Œè°·æ­Œå¾ˆå®¹æ˜“æ‰¾åˆ°å¾ˆæœ‰æ·±åº¦çš„æ–‡ç« ï¼Œæ²¡å¿…è¦åšæ¬è¿å·¥äº†ã€‚ ç”¨åšå®¢è®°å½•æˆé•¿ï¼ŒåšæŒå®šæœŸè‡ªçœã€‚åªæœ‰åœ¨é™ä¸‹æ¥æ€è€ƒæ—¶ï¼Œæ‰èƒ½å¥½å¥½çš„åçœå’Œæ£€è§†è‡ªå·±ã€‚","link":"/en/about/index.html"}]}